{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "!pip install --force-reinstall -U ipywidgets\n",
    "!pip install --force-reinstall unsloth\n",
    "\n",
    "!pip3 install --force-reinstall torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ],
   "id": "77499415f6338f7d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "import unsloth\n",
   "id": "8f31c95c6739d77a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# import unsloth\n",
    "\n",
    "import torch\n",
    "\n",
    "print(f\"Версия PyTorch: {torch.__version__}\")\n",
    "print(f\"Доступна ли CUDA: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Версия CUDA для PyTorch: {torch.version.cuda}\")\n",
    "    print(f\"Имя GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\">>> CUDA недоступна. Установлена CPU-версия PyTorch или есть проблема с совместимостью.\")"
   ],
   "id": "5eb20d35ab155ba9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-16T18:35:26.561595Z",
     "start_time": "2025-06-16T18:34:40.146927Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "import unsloth\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "from typing import Optional, List, Dict, Any\n",
    "\n",
    "from transformers import AutoConfig, TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. DEFINITION OF THE CUSTOM MODEL WRAPPER\n",
    "# ==============================================================================\n",
    "class EmotionUnslothModel(nn.Module):\n",
    "    \"\"\"\n",
    "    An unsloth-optimized wrapper that includes a trainable vector projector.\n",
    "    This class takes a raw, fixed-size emotion vector, projects it to the\n",
    "    model's hidden dimension, and then injects it into the forward pass.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name_or_path: str,\n",
    "        raw_emotion_vector_size: int,\n",
    "        lora_rank: int = 16,\n",
    "        lora_alpha: int = 16,\n",
    "        use_4bit: bool = True,\n",
    "        max_seq_length: int = 2048,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes the EmotionUnslothModel with a vector projector.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Load unsloth model and tokenizer\n",
    "        self.model, self.tokenizer = FastLanguageModel.from_pretrained(\n",
    "            model_name=model_name_or_path,\n",
    "            max_seq_length=max_seq_length,\n",
    "            load_in_4bit=use_4bit,\n",
    "        )\n",
    "        model_hidden_size = self.model.config.hidden_size\n",
    "\n",
    "        # Define the vector projector\n",
    "        self.vector_projector = nn.Linear(\n",
    "            in_features=raw_emotion_vector_size,\n",
    "            out_features=model_hidden_size,\n",
    "            bias=False\n",
    "        )\n",
    "        self.vector_projector.to(self.model.dtype)\n",
    "\n",
    "        # Apply LoRA using unsloth's function\n",
    "        self.peft_model = FastLanguageModel.get_peft_model(\n",
    "            self.model,\n",
    "            r=lora_rank,\n",
    "            lora_alpha=lora_alpha,\n",
    "            lora_dropout=0,\n",
    "            bias=\"none\",\n",
    "            use_gradient_checkpointing=True,\n",
    "            random_state=42,\n",
    "            target_modules=[\n",
    "                \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                \"gate_proj\", \"up_proj\", \"down_proj\"\n",
    "            ],\n",
    "            # modules_to_save=[\"vector_projector\"], # <--- УДАЛИТЕ ЭТУ СТРОКУ\n",
    "        )\n",
    "\n",
    "        # --- ДОБАВЬТЕ ЭТОТ БЛОК ---\n",
    "        # Manually unfreeze the projector weights after creating the PEFT model.\n",
    "        # This makes them trainable without using the restricted 'modules_to_save'.\n",
    "        for param in self.vector_projector.parameters():\n",
    "            param.requires_grad = True\n",
    "        # ---------------------------\n",
    "\n",
    "        self.peft_model.print_trainable_parameters()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor,\n",
    "        attention_mask: torch.Tensor,\n",
    "        emotion_vector: torch.Tensor,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        **kwargs,\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Performs the forward pass with projection and injection.\n",
    "        \"\"\"\n",
    "        projected_vector = self.vector_projector(emotion_vector)\n",
    "        embedding_layer = self.peft_model.get_input_embeddings()\n",
    "        token_embeddings = embedding_layer(input_ids)\n",
    "        combined_embeddings = token_embeddings + projected_vector.unsqueeze(1)\n",
    "\n",
    "        model_outputs = self.peft_model(\n",
    "            inputs_embeds=combined_embeddings,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels,\n",
    "            return_dict=True\n",
    "        )\n",
    "        return model_outputs\n",
    "\n",
    "    def generate(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor,\n",
    "        attention_mask: torch.Tensor,\n",
    "        emotion_vector: torch.Tensor,\n",
    "        **generation_kwargs: Any\n",
    "    ) -> List[int]:\n",
    "        \"\"\"\n",
    "        Generates text conditioned on a prompt and an emotion vector.\n",
    "        \"\"\"\n",
    "        if \"pad_token_id\" not in generation_kwargs:\n",
    "            generation_kwargs[\"pad_token_id\"] = self.tokenizer.eos_token_id\n",
    "\n",
    "        return self.peft_model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            emotion_vector=emotion_vector,\n",
    "            **generation_kwargs\n",
    "        )\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. DEFINITION OF THE CUSTOM DATA COLLATOR\n",
    "# ==============================================================================\n",
    "\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "class DataCollatorForEmotionLM(DataCollatorForLanguageModeling):\n",
    "    \"\"\"\n",
    "    Custom data collator that handles tokenizing text and stacking emotion vectors.\n",
    "    \"\"\"\n",
    "    def __call__(\n",
    "        self,\n",
    "        features: List[Dict[str, Any]]\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Processes a list of features to create a batch.\n",
    "        \"\"\"\n",
    "        emotion_vectors = [feature.pop(\"emotion_vector\") for feature in features]\n",
    "        batch = super().__call__(features)\n",
    "        batch['emotion_vector'] = torch.stack(emotion_vectors)\n",
    "        return batch\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. MAIN TRAINING SCRIPT\n",
    "# ==============================================================================\n",
    "\n",
    "def main():\n",
    "    # --- Configuration ---\n",
    "    MODEL_NAME = \"unsloth/Qwen3-0.6B-unsloth-bnb-4bit\"\n",
    "    MAX_SEQ_LENGTH = 2048\n",
    "    RAW_EMOTION_VECTOR_SIZE = 12\n",
    "\n",
    "    # --- Model Initialization ---\n",
    "    print(\"Initializing the model...\")\n",
    "    emotion_model_wrapper = EmotionUnslothModel(\n",
    "        model_name_or_path=MODEL_NAME,\n",
    "        raw_emotion_vector_size=RAW_EMOTION_VECTOR_SIZE,\n",
    "        max_seq_length=MAX_SEQ_LENGTH\n",
    "    )\n",
    "\n",
    "    # --- Data Preparation ---\n",
    "    print(\"Preparing the dataset...\")\n",
    "    # For Llama-3 instruct model, we should use its specific chat template\n",
    "    prompt_template = \"\"\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "{}<|eot_id|>\"\"\"\n",
    "\n",
    "    raw_data = [\n",
    "        {\"text\": prompt_template.format(\"Write a happy poem about spring.\", \"The sunbeams dance, a joyful sight,\\nNew flowers bloom in colors bright.\"), \"emotion\": \"happy\"},\n",
    "        {\"text\": prompt_template.format(\"Describe a spooky, abandoned mansion.\", \"The old manor stood in chilling dread,\\nWhere silent ghosts and shadows tread.\"), \"emotion\": \"spooky\"},\n",
    "        {\"text\": prompt_template.format(\"Compose a short, joyful song about a river.\", \"The river flows, a happy tune,\\nBeneath the sunny afternoon.\"), \"emotion\": \"happy\"},\n",
    "        {\"text\": prompt_template.format(\"Tell a short, eerie tale about a forest at night.\", \"Deep in the woods, when moonlight fails,\\nA whisper rides on chilling gales.\"), \"emotion\": \"spooky\"},\n",
    "    ]\n",
    "    dataset = Dataset.from_list(raw_data)\n",
    "\n",
    "    # Create and map emotion vectors to the dataset\n",
    "    emotion_mapping = {\n",
    "        \"happy\": torch.from_numpy(np.random.rand(RAW_EMOTION_VECTOR_SIZE) * 0.1),\n",
    "        \"spooky\": torch.from_numpy(np.random.rand(RAW_EMOTION_VECTOR_SIZE) * -0.1)\n",
    "    }\n",
    "\n",
    "    def add_emotion_vector(example: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Adds the emotion vector to a dataset example.\"\"\"\n",
    "        example[\"emotion_vector\"] = emotion_mapping[example[\"emotion\"]].to(torch.float16)\n",
    "        return example\n",
    "\n",
    "    dataset = dataset.map(add_emotion_vector)\n",
    "\n",
    "    # --- Pre-processing and Tokenization ---\n",
    "    def preprocess_function(example: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Tokenizes the text and prepares labels.\"\"\"\n",
    "        tokenized_example = emotion_model_wrapper.tokenizer(\n",
    "            example[\"text\"],\n",
    "            truncation=True,\n",
    "            max_length=MAX_SEQ_LENGTH,\n",
    "            padding=False,\n",
    "            return_tensors=None,\n",
    "        )\n",
    "        tokenized_example[\"labels\"] = tokenized_example[\"input_ids\"][:]\n",
    "        return tokenized_example\n",
    "\n",
    "    tokenized_dataset = dataset.map(\n",
    "        preprocess_function,\n",
    "        batched=True,\n",
    "        remove_columns=dataset.column_names\n",
    "    )\n",
    "\n",
    "    # --- Trainer Setup ---\n",
    "    print(\"Setting up the trainer...\")\n",
    "    data_collator = DataCollatorForEmotionLM(\n",
    "        tokenizer=emotion_model_wrapper.tokenizer,\n",
    "        mlm=False\n",
    "    )\n",
    "\n",
    "    trainer = SFTTrainer(\n",
    "        model=emotion_model_wrapper.peft_model,\n",
    "        tokenizer=emotion_model_wrapper.tokenizer,\n",
    "        train_dataset=tokenized_dataset,\n",
    "        data_collator=data_collator,\n",
    "        max_seq_length=MAX_SEQ_LENGTH,\n",
    "        args=TrainingArguments(\n",
    "            per_device_train_batch_size=2,\n",
    "            gradient_accumulation_steps=4,\n",
    "            warmup_steps=5,\n",
    "            max_steps=50,  # Increase for real training\n",
    "            learning_rate=2e-4,\n",
    "            fp16=not torch.cuda.is_bf16_supported(),\n",
    "            bf16=torch.cuda.is_bf16_supported(),\n",
    "            logging_steps=1,\n",
    "            optim=\"adamw_8bit\",\n",
    "            weight_decay=0.01,\n",
    "            lr_scheduler_type=\"linear\",\n",
    "            seed=42,\n",
    "            output_dir=\"outputs\",\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # --- Start Training ---\n",
    "    print(\"Starting training...\")\n",
    "    trainer.train()\n",
    "    print(\"Training finished!\")\n",
    "\n",
    "    # --- Inference Example ---\n",
    "    print(\"\\n--- Running Inference Example ---\")\n",
    "\n",
    "    inference_prompt = \"Tell me about a sunny day.\"\n",
    "    inference_emotion = \"happy\"\n",
    "\n",
    "    # Get the corresponding emotion vector\n",
    "    emotion_vector_tensor = emotion_mapping[inference_emotion].unsqueeze(0).to(\"cuda\", dtype=torch.float16)\n",
    "\n",
    "    # Format the prompt using the chat template (only the user part)\n",
    "    formatted_prompt = prompt_template.format(inference_prompt, \"\")\n",
    "\n",
    "    inputs = emotion_model_wrapper.tokenizer(formatted_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    generated_ids = emotion_model_wrapper.generate(\n",
    "        input_ids=inputs.input_ids,\n",
    "        attention_mask=inputs.attention_mask,\n",
    "        emotion_vector=emotion_vector_tensor,\n",
    "        max_new_tokens=50,\n",
    "        use_cache=True,\n",
    "        do_sample=True,\n",
    "        top_p=0.9,\n",
    "        temperature=0.7,\n",
    "    )\n",
    "\n",
    "    generated_text = emotion_model_wrapper.tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "    print(\"\\n--- Generated Text ---\")\n",
    "    print(generated_text)\n",
    "\n",
    "\n",
    "\n",
    "main()"
   ],
   "id": "1d69364d2ad355a6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:\n",
      "    PyTorch 2.7.0+cu126 with CUDA 1206 (you have 2.7.1+cu118)\n",
      "    Python  3.12.10 (you have 3.12.10)\n",
      "  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\n",
      "  Memory-efficient attention, SwiGLU, sparse and more won't be available.\n",
      "  Set XFORMERS_MORE_DETAILS=1 for more details\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth Zoo will now patch everything to make training faster!\n",
      "Initializing the model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\my_projects\\Feelent.venv1\\Lib\\site-packages\\unsloth_zoo\\gradient_checkpointing.py:339: UserWarning: expandable_segments not supported on this platform (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\c10/cuda/CUDAAllocatorConfig.h:28.)\n",
      "  GPU_BUFFERS = tuple([torch.empty(2*256*2048, dtype = dtype, device = f\"{DEVICE_TYPE}:{i}\") for i in range(n_gpus)])\n",
      "HTTP Error 429 thrown while requesting HEAD https://huggingface.co/unsloth/Qwen3-0.6B-unsloth-bnb-4bit/resolve/main/config.json\n",
      "Retrying in 1s [Retry 1/5].\n",
      "HTTP Error 429 thrown while requesting HEAD https://huggingface.co/unsloth/Qwen3-0.6B-unsloth-bnb-4bit/resolve/main/config.json\n",
      "Retrying in 2s [Retry 2/5].\n",
      "HTTP Error 429 thrown while requesting HEAD https://huggingface.co/unsloth/Qwen3-0.6B-unsloth-bnb-4bit/resolve/main/config.json\n",
      "Retrying in 4s [Retry 3/5].\n",
      "HTTP Error 429 thrown while requesting HEAD https://huggingface.co/unsloth/Qwen3-0.6B-unsloth-bnb-4bit/resolve/main/config.json\n",
      "Retrying in 8s [Retry 4/5].\n",
      "HTTP Error 429 thrown while requesting HEAD https://huggingface.co/unsloth/Qwen3-0.6B-unsloth-bnb-4bit/resolve/main/config.json\n",
      "Retrying in 8s [Retry 5/5].\n",
      "HTTP Error 429 thrown while requesting HEAD https://huggingface.co/unsloth/Qwen3-0.6B-unsloth-bnb-4bit/resolve/main/config.json\n",
      "HTTP Error 429 thrown while requesting HEAD https://huggingface.co/unsloth/Qwen3-0.6B-unsloth-bnb-4bit/resolve/main/adapter_config.json\n",
      "Retrying in 1s [Retry 1/5].\n",
      "HTTP Error 429 thrown while requesting HEAD https://huggingface.co/unsloth/Qwen3-0.6B-unsloth-bnb-4bit/resolve/main/adapter_config.json\n",
      "Retrying in 2s [Retry 2/5].\n",
      "HTTP Error 429 thrown while requesting HEAD https://huggingface.co/unsloth/Qwen3-0.6B-unsloth-bnb-4bit/resolve/main/adapter_config.json\n",
      "Retrying in 4s [Retry 3/5].\n",
      "HTTP Error 429 thrown while requesting HEAD https://huggingface.co/unsloth/Qwen3-0.6B-unsloth-bnb-4bit/resolve/main/adapter_config.json\n",
      "Retrying in 8s [Retry 4/5].\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[1]\u001B[39m\u001B[32m, line 276\u001B[39m\n\u001B[32m    271\u001B[39m     \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m--- Generated Text ---\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m    272\u001B[39m     \u001B[38;5;28mprint\u001B[39m(generated_text)\n\u001B[32m--> \u001B[39m\u001B[32m276\u001B[39m \u001B[43mmain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[1]\u001B[39m\u001B[32m, line 155\u001B[39m, in \u001B[36mmain\u001B[39m\u001B[34m()\u001B[39m\n\u001B[32m    153\u001B[39m \u001B[38;5;66;03m# --- Model Initialization ---\u001B[39;00m\n\u001B[32m    154\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33mInitializing the model...\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m--> \u001B[39m\u001B[32m155\u001B[39m emotion_model_wrapper = \u001B[43mEmotionUnslothModel\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    156\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmodel_name_or_path\u001B[49m\u001B[43m=\u001B[49m\u001B[43mMODEL_NAME\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    157\u001B[39m \u001B[43m    \u001B[49m\u001B[43mraw_emotion_vector_size\u001B[49m\u001B[43m=\u001B[49m\u001B[43mRAW_EMOTION_VECTOR_SIZE\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    158\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmax_seq_length\u001B[49m\u001B[43m=\u001B[49m\u001B[43mMAX_SEQ_LENGTH\u001B[49m\n\u001B[32m    159\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    161\u001B[39m \u001B[38;5;66;03m# --- Data Preparation ---\u001B[39;00m\n\u001B[32m    162\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33mPreparing the dataset...\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[1]\u001B[39m\u001B[32m, line 37\u001B[39m, in \u001B[36mEmotionUnslothModel.__init__\u001B[39m\u001B[34m(self, model_name_or_path, raw_emotion_vector_size, lora_rank, lora_alpha, use_4bit, max_seq_length)\u001B[39m\n\u001B[32m     34\u001B[39m \u001B[38;5;28msuper\u001B[39m().\u001B[34m__init__\u001B[39m()\n\u001B[32m     36\u001B[39m \u001B[38;5;66;03m# Load unsloth model and tokenizer\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m37\u001B[39m \u001B[38;5;28mself\u001B[39m.model, \u001B[38;5;28mself\u001B[39m.tokenizer = \u001B[43mFastLanguageModel\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m     38\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmodel_name\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmodel_name_or_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     39\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmax_seq_length\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmax_seq_length\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     40\u001B[39m \u001B[43m    \u001B[49m\u001B[43mload_in_4bit\u001B[49m\u001B[43m=\u001B[49m\u001B[43muse_4bit\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     41\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     42\u001B[39m model_hidden_size = \u001B[38;5;28mself\u001B[39m.model.config.hidden_size\n\u001B[32m     44\u001B[39m \u001B[38;5;66;03m# Define the vector projector\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\my_projects\\Feelent.venv1\\Lib\\site-packages\\unsloth\\models\\loader.py:173\u001B[39m, in \u001B[36mFastLanguageModel.from_pretrained\u001B[39m\u001B[34m(model_name, max_seq_length, dtype, load_in_4bit, load_in_8bit, full_finetuning, token, device_map, rope_scaling, fix_tokenizer, trust_remote_code, use_gradient_checkpointing, resize_model_vocab, revision, use_exact_model_name, fast_inference, gpu_memory_utilization, float8_kv_cache, random_state, max_lora_rank, disable_log_stats, *args, **kwargs)\u001B[39m\n\u001B[32m    171\u001B[39m     is_model = \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[32m    172\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m173\u001B[39m     peft_config = \u001B[43mPeftConfig\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    174\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmodel_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    175\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtoken\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[43mtoken\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    176\u001B[39m \u001B[43m        \u001B[49m\u001B[43mrevision\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[43mrevision\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    177\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtrust_remote_code\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrust_remote_code\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    178\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    179\u001B[39m     is_peft = \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[32m    180\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m error:\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\my_projects\\Feelent.venv1\\Lib\\site-packages\\peft\\config.py:198\u001B[39m, in \u001B[36mPeftConfigMixin.from_pretrained\u001B[39m\u001B[34m(cls, pretrained_model_name_or_path, subfolder, **kwargs)\u001B[39m\n\u001B[32m    196\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    197\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m198\u001B[39m         config_file = \u001B[43mhf_hub_download\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    199\u001B[39m \u001B[43m            \u001B[49m\u001B[43mpretrained_model_name_or_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mCONFIG_NAME\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msubfolder\u001B[49m\u001B[43m=\u001B[49m\u001B[43msubfolder\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mhf_hub_download_kwargs\u001B[49m\n\u001B[32m    200\u001B[39m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    201\u001B[39m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m exc:\n\u001B[32m    202\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mCan\u001B[39m\u001B[33m'\u001B[39m\u001B[33mt find \u001B[39m\u001B[33m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mCONFIG_NAME\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m'\u001B[39m\u001B[33m at \u001B[39m\u001B[33m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpretrained_model_name_or_path\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m'\u001B[39m\u001B[33m\"\u001B[39m) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mexc\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\my_projects\\Feelent.venv1\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001B[39m, in \u001B[36mvalidate_hf_hub_args.<locals>._inner_fn\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    111\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m check_use_auth_token:\n\u001B[32m    112\u001B[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001B[34m__name__\u001B[39m, has_token=has_token, kwargs=kwargs)\n\u001B[32m--> \u001B[39m\u001B[32m114\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\my_projects\\Feelent.venv1\\Lib\\site-packages\\huggingface_hub\\file_download.py:1008\u001B[39m, in \u001B[36mhf_hub_download\u001B[39m\u001B[34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001B[39m\n\u001B[32m    988\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m _hf_hub_download_to_local_dir(\n\u001B[32m    989\u001B[39m         \u001B[38;5;66;03m# Destination\u001B[39;00m\n\u001B[32m    990\u001B[39m         local_dir=local_dir,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1005\u001B[39m         local_files_only=local_files_only,\n\u001B[32m   1006\u001B[39m     )\n\u001B[32m   1007\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1008\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_hf_hub_download_to_cache_dir\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1009\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m# Destination\u001B[39;49;00m\n\u001B[32m   1010\u001B[39m \u001B[43m        \u001B[49m\u001B[43mcache_dir\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcache_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1011\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m# File info\u001B[39;49;00m\n\u001B[32m   1012\u001B[39m \u001B[43m        \u001B[49m\u001B[43mrepo_id\u001B[49m\u001B[43m=\u001B[49m\u001B[43mrepo_id\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1013\u001B[39m \u001B[43m        \u001B[49m\u001B[43mfilename\u001B[49m\u001B[43m=\u001B[49m\u001B[43mfilename\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1014\u001B[39m \u001B[43m        \u001B[49m\u001B[43mrepo_type\u001B[49m\u001B[43m=\u001B[49m\u001B[43mrepo_type\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1015\u001B[39m \u001B[43m        \u001B[49m\u001B[43mrevision\u001B[49m\u001B[43m=\u001B[49m\u001B[43mrevision\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1016\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m# HTTP info\u001B[39;49;00m\n\u001B[32m   1017\u001B[39m \u001B[43m        \u001B[49m\u001B[43mendpoint\u001B[49m\u001B[43m=\u001B[49m\u001B[43mendpoint\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1018\u001B[39m \u001B[43m        \u001B[49m\u001B[43metag_timeout\u001B[49m\u001B[43m=\u001B[49m\u001B[43metag_timeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1019\u001B[39m \u001B[43m        \u001B[49m\u001B[43mheaders\u001B[49m\u001B[43m=\u001B[49m\u001B[43mhf_headers\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1020\u001B[39m \u001B[43m        \u001B[49m\u001B[43mproxies\u001B[49m\u001B[43m=\u001B[49m\u001B[43mproxies\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1021\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtoken\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtoken\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1022\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m# Additional options\u001B[39;49;00m\n\u001B[32m   1023\u001B[39m \u001B[43m        \u001B[49m\u001B[43mlocal_files_only\u001B[49m\u001B[43m=\u001B[49m\u001B[43mlocal_files_only\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1024\u001B[39m \u001B[43m        \u001B[49m\u001B[43mforce_download\u001B[49m\u001B[43m=\u001B[49m\u001B[43mforce_download\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1025\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\my_projects\\Feelent.venv1\\Lib\\site-packages\\huggingface_hub\\file_download.py:1071\u001B[39m, in \u001B[36m_hf_hub_download_to_cache_dir\u001B[39m\u001B[34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001B[39m\n\u001B[32m   1067\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m pointer_path\n\u001B[32m   1069\u001B[39m \u001B[38;5;66;03m# Try to get metadata (etag, commit_hash, url, size) from the server.\u001B[39;00m\n\u001B[32m   1070\u001B[39m \u001B[38;5;66;03m# If we can't, a HEAD request error is returned.\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1071\u001B[39m (url_to_download, etag, commit_hash, expected_size, xet_file_data, head_call_error) = \u001B[43m_get_metadata_or_catch_error\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1072\u001B[39m \u001B[43m    \u001B[49m\u001B[43mrepo_id\u001B[49m\u001B[43m=\u001B[49m\u001B[43mrepo_id\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1073\u001B[39m \u001B[43m    \u001B[49m\u001B[43mfilename\u001B[49m\u001B[43m=\u001B[49m\u001B[43mfilename\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1074\u001B[39m \u001B[43m    \u001B[49m\u001B[43mrepo_type\u001B[49m\u001B[43m=\u001B[49m\u001B[43mrepo_type\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1075\u001B[39m \u001B[43m    \u001B[49m\u001B[43mrevision\u001B[49m\u001B[43m=\u001B[49m\u001B[43mrevision\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1076\u001B[39m \u001B[43m    \u001B[49m\u001B[43mendpoint\u001B[49m\u001B[43m=\u001B[49m\u001B[43mendpoint\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1077\u001B[39m \u001B[43m    \u001B[49m\u001B[43mproxies\u001B[49m\u001B[43m=\u001B[49m\u001B[43mproxies\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1078\u001B[39m \u001B[43m    \u001B[49m\u001B[43metag_timeout\u001B[49m\u001B[43m=\u001B[49m\u001B[43metag_timeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1079\u001B[39m \u001B[43m    \u001B[49m\u001B[43mheaders\u001B[49m\u001B[43m=\u001B[49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1080\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtoken\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtoken\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1081\u001B[39m \u001B[43m    \u001B[49m\u001B[43mlocal_files_only\u001B[49m\u001B[43m=\u001B[49m\u001B[43mlocal_files_only\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1082\u001B[39m \u001B[43m    \u001B[49m\u001B[43mstorage_folder\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstorage_folder\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1083\u001B[39m \u001B[43m    \u001B[49m\u001B[43mrelative_filename\u001B[49m\u001B[43m=\u001B[49m\u001B[43mrelative_filename\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1084\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1086\u001B[39m \u001B[38;5;66;03m# etag can be None for several reasons:\u001B[39;00m\n\u001B[32m   1087\u001B[39m \u001B[38;5;66;03m# 1. we passed local_files_only.\u001B[39;00m\n\u001B[32m   1088\u001B[39m \u001B[38;5;66;03m# 2. we don't have a connection\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m   1094\u001B[39m \u001B[38;5;66;03m# If the specified revision is a commit hash, look inside \"snapshots\".\u001B[39;00m\n\u001B[32m   1095\u001B[39m \u001B[38;5;66;03m# If the specified revision is a branch or tag, look inside \"refs\".\u001B[39;00m\n\u001B[32m   1096\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m head_call_error \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m   1097\u001B[39m     \u001B[38;5;66;03m# Couldn't make a HEAD call => let's try to find a local file\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\my_projects\\Feelent.venv1\\Lib\\site-packages\\huggingface_hub\\file_download.py:1533\u001B[39m, in \u001B[36m_get_metadata_or_catch_error\u001B[39m\u001B[34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001B[39m\n\u001B[32m   1531\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m   1532\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1533\u001B[39m         metadata = \u001B[43mget_hf_file_metadata\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1534\u001B[39m \u001B[43m            \u001B[49m\u001B[43murl\u001B[49m\u001B[43m=\u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mproxies\u001B[49m\u001B[43m=\u001B[49m\u001B[43mproxies\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m=\u001B[49m\u001B[43metag_timeout\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mheaders\u001B[49m\u001B[43m=\u001B[49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtoken\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtoken\u001B[49m\n\u001B[32m   1535\u001B[39m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1536\u001B[39m     \u001B[38;5;28;01mexcept\u001B[39;00m EntryNotFoundError \u001B[38;5;28;01mas\u001B[39;00m http_error:\n\u001B[32m   1537\u001B[39m         \u001B[38;5;28;01mif\u001B[39;00m storage_folder \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m relative_filename \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m   1538\u001B[39m             \u001B[38;5;66;03m# Cache the non-existence of the file\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\my_projects\\Feelent.venv1\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001B[39m, in \u001B[36mvalidate_hf_hub_args.<locals>._inner_fn\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    111\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m check_use_auth_token:\n\u001B[32m    112\u001B[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001B[34m__name__\u001B[39m, has_token=has_token, kwargs=kwargs)\n\u001B[32m--> \u001B[39m\u001B[32m114\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\my_projects\\Feelent.venv1\\Lib\\site-packages\\huggingface_hub\\file_download.py:1450\u001B[39m, in \u001B[36mget_hf_file_metadata\u001B[39m\u001B[34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers)\u001B[39m\n\u001B[32m   1447\u001B[39m hf_headers[\u001B[33m\"\u001B[39m\u001B[33mAccept-Encoding\u001B[39m\u001B[33m\"\u001B[39m] = \u001B[33m\"\u001B[39m\u001B[33midentity\u001B[39m\u001B[33m\"\u001B[39m  \u001B[38;5;66;03m# prevent any compression => we want to know the real size of the file\u001B[39;00m\n\u001B[32m   1449\u001B[39m \u001B[38;5;66;03m# Retrieve metadata\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1450\u001B[39m r = \u001B[43m_request_wrapper\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1451\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmethod\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mHEAD\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m   1452\u001B[39m \u001B[43m    \u001B[49m\u001B[43murl\u001B[49m\u001B[43m=\u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1453\u001B[39m \u001B[43m    \u001B[49m\u001B[43mheaders\u001B[49m\u001B[43m=\u001B[49m\u001B[43mhf_headers\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1454\u001B[39m \u001B[43m    \u001B[49m\u001B[43mallow_redirects\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m   1455\u001B[39m \u001B[43m    \u001B[49m\u001B[43mfollow_relative_redirects\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m   1456\u001B[39m \u001B[43m    \u001B[49m\u001B[43mproxies\u001B[49m\u001B[43m=\u001B[49m\u001B[43mproxies\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1457\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1458\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1459\u001B[39m hf_raise_for_status(r)\n\u001B[32m   1461\u001B[39m \u001B[38;5;66;03m# Return\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\my_projects\\Feelent.venv1\\Lib\\site-packages\\huggingface_hub\\file_download.py:286\u001B[39m, in \u001B[36m_request_wrapper\u001B[39m\u001B[34m(method, url, follow_relative_redirects, **params)\u001B[39m\n\u001B[32m    284\u001B[39m \u001B[38;5;66;03m# Recursively follow relative redirects\u001B[39;00m\n\u001B[32m    285\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m follow_relative_redirects:\n\u001B[32m--> \u001B[39m\u001B[32m286\u001B[39m     response = \u001B[43m_request_wrapper\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    287\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmethod\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    288\u001B[39m \u001B[43m        \u001B[49m\u001B[43murl\u001B[49m\u001B[43m=\u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    289\u001B[39m \u001B[43m        \u001B[49m\u001B[43mfollow_relative_redirects\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    290\u001B[39m \u001B[43m        \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    291\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    293\u001B[39m     \u001B[38;5;66;03m# If redirection, we redirect only relative paths.\u001B[39;00m\n\u001B[32m    294\u001B[39m     \u001B[38;5;66;03m# This is useful in case of a renamed repository.\u001B[39;00m\n\u001B[32m    295\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[32m300\u001B[39m <= response.status_code <= \u001B[32m399\u001B[39m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\my_projects\\Feelent.venv1\\Lib\\site-packages\\huggingface_hub\\file_download.py:309\u001B[39m, in \u001B[36m_request_wrapper\u001B[39m\u001B[34m(method, url, follow_relative_redirects, **params)\u001B[39m\n\u001B[32m    306\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m response\n\u001B[32m    308\u001B[39m \u001B[38;5;66;03m# Perform request and return if status_code is not in the retry list.\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m309\u001B[39m response = \u001B[43mhttp_backoff\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmethod\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43murl\u001B[49m\u001B[43m=\u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretry_on_exceptions\u001B[49m\u001B[43m=\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretry_on_status_codes\u001B[49m\u001B[43m=\u001B[49m\u001B[43m(\u001B[49m\u001B[32;43m429\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    310\u001B[39m hf_raise_for_status(response)\n\u001B[32m    311\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m response\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\my_projects\\Feelent.venv1\\Lib\\site-packages\\huggingface_hub\\utils\\_http.py:333\u001B[39m, in \u001B[36mhttp_backoff\u001B[39m\u001B[34m(method, url, max_retries, base_wait_time, max_wait_time, retry_on_exceptions, retry_on_status_codes, **kwargs)\u001B[39m\n\u001B[32m    331\u001B[39m \u001B[38;5;66;03m# Sleep for X seconds\u001B[39;00m\n\u001B[32m    332\u001B[39m logger.warning(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mRetrying in \u001B[39m\u001B[38;5;132;01m{\u001B[39;00msleep_time\u001B[38;5;132;01m}\u001B[39;00m\u001B[33ms [Retry \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnb_tries\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmax_retries\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m].\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m--> \u001B[39m\u001B[32m333\u001B[39m \u001B[43mtime\u001B[49m\u001B[43m.\u001B[49m\u001B[43msleep\u001B[49m\u001B[43m(\u001B[49m\u001B[43msleep_time\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    335\u001B[39m \u001B[38;5;66;03m# Update sleep time for next retry\u001B[39;00m\n\u001B[32m    336\u001B[39m sleep_time = \u001B[38;5;28mmin\u001B[39m(max_wait_time, sleep_time * \u001B[32m2\u001B[39m)\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "from triton.compiler.compiler import AttrsDescriptor",
   "id": "cb4e062a542f9c08",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
