{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "!pip install --force-reinstall -U ipywidgets\n",
    "!pip install --force-reinstall unsloth\n",
    "\n",
    "!pip3 install --force-reinstall torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ],
   "id": "77499415f6338f7d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "import unsloth\n",
   "id": "8f31c95c6739d77a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# import unsloth\n",
    "\n",
    "import torch\n",
    "\n",
    "print(f\"Версия PyTorch: {torch.__version__}\")\n",
    "print(f\"Доступна ли CUDA: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Версия CUDA для PyTorch: {torch.version.cuda}\")\n",
    "    print(f\"Имя GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\">>> CUDA недоступна. Установлена CPU-версия PyTorch или есть проблема с совместимостью.\")"
   ],
   "id": "5eb20d35ab155ba9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T08:11:21.735439Z",
     "start_time": "2025-06-18T08:11:15.946292Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ast\n",
    "from datasets import Dataset\n",
    "from typing import Optional, List, Dict, Any\n",
    "\n",
    "import unsloth\n",
    "from unsloth import FastLanguageModel\n",
    "from transformers import AutoConfig, TrainingArguments, DataCollatorForLanguageModeling\n",
    "from trl import SFTTrainer\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. DEFINITION OF THE CUSTOM MODEL WRAPPER\n",
    "# (This class remains unchanged)\n",
    "# ==============================================================================\n",
    "class EmotionUnslothModel(nn.Module):\n",
    "    \"\"\"\n",
    "    An unsloth-optimized wrapper that includes a trainable vector projector.\n",
    "    This class takes a raw, fixed-size emotion vector, projects it to the\n",
    "    model's hidden dimension, and then injects it into the forward pass.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name_or_path: str,\n",
    "        raw_emotion_vector_size: int,\n",
    "        lora_rank: int = 16,\n",
    "        lora_alpha: int = 16,\n",
    "        use_4bit: bool = True,\n",
    "        max_seq_length: int = 2048,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes the EmotionUnslothModel with a vector projector.\n",
    "\n",
    "        Args:\n",
    "            model_name_or_path (str): The name or path of the base model.\n",
    "            raw_emotion_vector_size (int): The dimension of the input emotion vector.\n",
    "            lora_rank (int): The rank for LoRA decomposition.\n",
    "            lora_alpha (int): The alpha parameter for LoRA.\n",
    "            use_4bit (bool): Whether to load the model in 4-bit.\n",
    "            max_seq_length (int): The maximum sequence length for the model.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.model, self.tokenizer = FastLanguageModel.from_pretrained(\n",
    "            model_name=model_name_or_path,\n",
    "            max_seq_length=max_seq_length,\n",
    "            load_in_4bit=use_4bit,\n",
    "            cache_dir=\"./model_cache\",\n",
    "        )\n",
    "        model_hidden_size = self.model.config.hidden_size\n",
    "        self.vector_projector = nn.Linear(\n",
    "            in_features=raw_emotion_vector_size,\n",
    "            out_features=model_hidden_size,\n",
    "            bias=False\n",
    "        )\n",
    "        self.vector_projector.to(\"cuda\", self.model.dtype)\n",
    "        self.peft_model = FastLanguageModel.get_peft_model(\n",
    "            self.model,\n",
    "            r=lora_rank,\n",
    "            lora_alpha=lora_alpha,\n",
    "            lora_dropout=0,\n",
    "            bias=\"none\",\n",
    "            use_gradient_checkpointing=True,\n",
    "            random_state=42,\n",
    "            target_modules=[\n",
    "                \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                \"gate_proj\", \"up_proj\", \"down_proj\"\n",
    "            ],\n",
    "        )\n",
    "        for param in self.vector_projector.parameters():\n",
    "            param.requires_grad = True\n",
    "        self.peft_model.print_trainable_parameters()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor,\n",
    "        attention_mask: torch.Tensor,\n",
    "        emotion_vector: torch.Tensor,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        **kwargs,\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Performs the forward pass with projection and injection.\n",
    "        \"\"\"\n",
    "        projected_vector = self.vector_projector(emotion_vector)\n",
    "        embedding_layer = self.peft_model.get_input_embeddings()\n",
    "        token_embeddings = embedding_layer(input_ids)\n",
    "        combined_embeddings = token_embeddings + projected_vector.unsqueeze(1)\n",
    "        model_outputs = self.peft_model(\n",
    "            inputs_embeds=combined_embeddings,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels,\n",
    "            return_dict=True\n",
    "        )\n",
    "        return model_outputs\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. DEFINITION OF THE CUSTOM DATA COLLATOR\n",
    "# (This class remains unchanged)\n",
    "# ==============================================================================\n",
    "class DataCollatorForEmotionLM(DataCollatorForLanguageModeling):\n",
    "    \"\"\"\n",
    "    Custom data collator that handles tokenizing text and stacking emotion vectors.\n",
    "    \"\"\"\n",
    "    def __call__(\n",
    "        self,\n",
    "        features: List[Dict[str, Any]]\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Processes a list of features to create a batch.\n",
    "        \"\"\"\n",
    "        emotion_vectors = [feature.pop(\"emotion_vector\") for feature in features]\n",
    "        batch = super().__call__(features)\n",
    "        batch['emotion_vector'] = torch.stack(emotion_vectors)\n",
    "        return batch\n",
    "\n",
    "\n",
    "def process_and_tokenize_example(\n",
    "    example: Dict[str, Any]\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Processes a single dataset example by formatting text, parsing the\n",
    "    emotion vector, and tokenizing the text.\n",
    "    \"\"\"\n",
    "    thinking = \"\\n\".join(ast.literal_eval(example[\"thinking\"]))\n",
    "    assistant_output = f\"<thinking>{thinking}</thinking>\\n{example['response']}\"\n",
    "    formatted_text = prompt_template.format(example[\"prompt\"], assistant_output)\n",
    "    vector_as_list = ast.literal_eval(example[\"feelings_vector\"])\n",
    "    emotion_vector = torch.tensor(vector_as_list, dtype=model_dtype)\n",
    "    tokenized_example = tokenizer(\n",
    "        formatted_text,\n",
    "        truncation=True,\n",
    "        max_length=MAX_SEQ_LENGTH,\n",
    "        padding=False,\n",
    "        return_tensors=None,\n",
    "    )\n",
    "    tokenized_example[\"labels\"] = tokenized_example[\"input_ids\"][:]\n",
    "    tokenized_example[\"emotion_vector\"] = emotion_vector\n",
    "    return tokenized_example"
   ],
   "id": "1d69364d2ad355a6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:\n",
      "    PyTorch 2.4.1+cu124 with CUDA 1204 (you have 2.6.0+cu118)\n",
      "    Python  3.12.5 (you have 3.12.10)\n",
      "  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\n",
      "  Memory-efficient attention, SwiGLU, sparse and more won't be available.\n",
      "  Set XFORMERS_MORE_DETAILS=1 for more details\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Failed to patch Gemma3ForConditionalGeneration.\n",
      "🦥 Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T08:11:31.298342Z",
     "start_time": "2025-06-18T08:11:21.836120Z"
    }
   },
   "cell_type": "code",
   "source": [
    "MODEL_NAME = \"unsloth/Qwen3-0.6B-unsloth-bnb-4bit\"\n",
    "MAX_SEQ_LENGTH = 2048\n",
    "RAW_EMOTION_VECTOR_SIZE = 16\n",
    "\n",
    "# --- Model Initialization ---\n",
    "print(\"Initializing the model...\")\n",
    "emotion_model_wrapper = EmotionUnslothModel(\n",
    "    model_name_or_path=MODEL_NAME,\n",
    "    raw_emotion_vector_size=RAW_EMOTION_VECTOR_SIZE,\n",
    "    max_seq_length=MAX_SEQ_LENGTH\n",
    ")\n",
    "model_dtype = emotion_model_wrapper.model.dtype\n",
    "tokenizer = emotion_model_wrapper.tokenizer"
   ],
   "id": "befcfdc5dbb79660",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing the model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\my_projects\\llm_vision\\.venv\\Lib\\site-packages\\unsloth_zoo\\gradient_checkpointing.py:330: UserWarning: expandable_segments not supported on this platform (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\c10/cuda/CUDAAllocatorConfig.h:28.)\n",
      "  GPU_BUFFERS = tuple([torch.empty(2*256*2048, dtype = dtype, device = f\"cuda:{i}\") for i in range(n_gpus)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.3.19: Fast Qwen3 patching. Transformers: 4.51.3.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 3090. Num GPUs = 1. Max memory: 24.0 GB. Platform: Windows.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu118. CUDA: 8.6. CUDA Toolkit: 11.8. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = None. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: Making `model.base_model.model.model` require gradients\n",
      "trainable params: 10,092,544 || all params: 606,142,464 || trainable%: 1.6650\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T08:11:31.590195Z",
     "start_time": "2025-06-18T08:11:31.327138Z"
    }
   },
   "cell_type": "code",
   "source": [
    "DATA_FILE_PATH = \"sft_one_emotion_thinking.csv\"\n",
    "\n",
    "# --- Data Preparation ---\n",
    "print(f\"Preparing the dataset from '{DATA_FILE_PATH}'...\")\n",
    "prompt_template = \"<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n{}<|im_end|>\\n<|im_start|>assistant\\n{}<|im_end|>\"\n",
    "raw_dataset = Dataset.from_csv(DATA_FILE_PATH)\n",
    "\n",
    "def process_and_tokenize_example(\n",
    "    example: Dict[str, Any]\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Processes a single dataset example by formatting text, parsing the\n",
    "    emotion vector, and tokenizing the text.\n",
    "    \"\"\"\n",
    "    thinking = \"\\n\".join(ast.literal_eval(example[\"thinking\"]))\n",
    "    assistant_output = f\"<thinking>{thinking}</thinking>\\n{example['response']}\"\n",
    "    formatted_text = prompt_template.format(example[\"prompt\"], assistant_output)\n",
    "    vector_as_list = ast.literal_eval(example[\"feelings_vector\"])\n",
    "    emotion_vector = torch.tensor(vector_as_list, dtype=model_dtype)\n",
    "    tokenized_example = tokenizer(\n",
    "        formatted_text,\n",
    "        truncation=True,\n",
    "        max_length=MAX_SEQ_LENGTH,\n",
    "        padding=False,\n",
    "        return_tensors=None,\n",
    "    )\n",
    "    tokenized_example[\"labels\"] = tokenized_example[\"input_ids\"][:]\n",
    "    tokenized_example[\"emotion_vector\"] = emotion_vector\n",
    "    return tokenized_example\n",
    "\n",
    "tokenized_dataset = raw_dataset.map(\n",
    "    process_and_tokenize_example,\n",
    "    remove_columns=raw_dataset.column_names\n",
    ")\n"
   ],
   "id": "6a7ae65bd832fd75",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing the dataset from 'sft_one_emotion_thinking.csv'...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/600 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b2f7af8731284bc99c29215838bb3a29"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T08:12:50.653977Z",
     "start_time": "2025-06-18T08:12:44.858010Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "# --- Inference Example ---\n",
    "print(\"\\n--- Пример генерации текста (инференс) ---\")\n",
    "\n",
    "inference_example = raw_dataset[0]\n",
    "inference_prompt = inference_example[\"prompt\"]\n",
    "inference_vector_str = inference_example[\"feelings_vector\"]\n",
    "inference_vector_list = ast.literal_eval(inference_vector_str)\n",
    "\n",
    "print(f\"Prompt: '{inference_prompt}'\")\n",
    "print(f\"Using feelings_vector: {inference_vector_list}...\")\n",
    "\n",
    "formatted_prompt_for_inference = prompt_template.format(inference_prompt, \"\")\n",
    "inputs = tokenizer(formatted_prompt_for_inference, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "emotion_vector_tensor = torch.tensor([inference_vector_list], dtype=model_dtype).to(\"cuda\")\n",
    "projected_vector = emotion_model_wrapper.vector_projector(emotion_vector_tensor)\n",
    "embedding_layer = emotion_model_wrapper.peft_model.get_input_embeddings()\n",
    "token_embeddings = embedding_layer(inputs.input_ids)\n",
    "combined_embeddings = token_embeddings + projected_vector.unsqueeze(1)\n",
    "\n",
    "# --- CORRECTED CALL TO .generate() ---\n",
    "# Unsloth's fast generation path requires `input_ids` to be passed,\n",
    "# even when providing `inputs_embeds`.\n",
    "generated_ids = emotion_model_wrapper.peft_model.generate(\n",
    "    input_ids=inputs.input_ids,  # This argument is required by Unsloth\n",
    "    inputs_embeds=combined_embeddings,\n",
    "    attention_mask=inputs.attention_mask,\n",
    "    max_new_tokens=100,\n",
    "    use_cache=True,\n",
    "    do_sample=True,\n",
    "    top_p=0.9,\n",
    "    temperature=0.7,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "full_generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=False)\n",
    "assistant_part = full_generated_text.split(\"<|im_start|>assistant\\n\")[-1]\n",
    "\n",
    "print(\"\\n--- Сгенерированный текст ---\")\n",
    "print(assistant_part)\n"
   ],
   "id": "da77ff376eea3826",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Пример генерации текста (инференс) ---\n",
      "Prompt: 'How are you feeling at this moment?'\n",
      "Using feelings_vector: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.77, 0.0, 0.8, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]...\n",
      "\n",
      "--- Сгенерированный текст ---\n",
      "<|im_end|>ícuenciaincincincincincincincincincincincincincincincincincincincincincincincincincincincincincincincincincincincincincincincincincincincincincincincincincincincincincincincincincincincincincincincincincincincincincincincincincincincincincincincincincincincincincincincincincincincincincincincincincinc\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T08:13:05.515540Z",
     "start_time": "2025-06-18T08:13:05.512724Z"
    }
   },
   "cell_type": "code",
   "source": "inference_example",
   "id": "556d410b65a1356c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': 'How are you feeling at this moment?',\n",
       " 'thinking': '[\"They\\'re asking how I am.\", \\'I feel this incredible buzz, a thrumming energy deep inside me.\\', \"It\\'s like waiting for the curtain to rise on the best show ever.\", \"My mind is racing with all the wonderful possibilities. It\\'s so close!\", \\'I should convey this excitement, this feeling that something great is just around the corner.\\']',\n",
       " 'response': \"I'm on the edge of my seat! Something amazing is about to happen, I can feel it.\",\n",
       " 'feelings_vector': '[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.77, 0.0, 0.8, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T08:20:50.161683Z",
     "start_time": "2025-06-18T08:13:30.846735Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ==============================================================================\n",
    "# 3. MAIN TRAINING SCRIPT\n",
    "# ==============================================================================\n",
    "\n",
    "# --- Trainer Setup ---\n",
    "print(\"Setting up the trainer...\")\n",
    "data_collator = DataCollatorForEmotionLM(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=emotion_model_wrapper.peft_model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    data_collator=data_collator,\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    args=TrainingArguments(\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=4,\n",
    "        warmup_ratio=0.1,\n",
    "        num_train_epochs=5,\n",
    "        learning_rate=2e-4,\n",
    "        fp16=not torch.cuda.is_bf16_supported(),\n",
    "        bf16=torch.cuda.is_bf16_supported(),\n",
    "        logging_strategy=\"steps\",\n",
    "        logging_steps=10,\n",
    "        optim=\"adamw_8bit\",\n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        seed=42,\n",
    "        output_dir=\"outputs\",\n",
    "        save_strategy=\"epoch\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "# --- Start Training ---\n",
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "print(\"Training finished!\")\n",
    "\n",
    "# --- Inference Example ---\n",
    "print(\"\\n--- Пример генерации текста (инференс) ---\")\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Prompt: '{inference_prompt}'\")\n",
    "print(f\"Using feelings_vector: {inference_vector_list}...\")\n",
    "\n",
    "\n",
    "formatted_prompt_for_inference = prompt_template.format(inference_prompt, \"\")\n",
    "inputs = tokenizer(formatted_prompt_for_inference, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "emotion_vector_tensor = torch.tensor([inference_vector_list], dtype=model_dtype).to(\"cuda\")\n",
    "projected_vector = emotion_model_wrapper.vector_projector(emotion_vector_tensor)\n",
    "embedding_layer = emotion_model_wrapper.peft_model.get_input_embeddings()\n",
    "token_embeddings = embedding_layer(inputs.input_ids)\n",
    "combined_embeddings = token_embeddings + projected_vector.unsqueeze(1)\n",
    "\n",
    "# --- CORRECTED CALL TO .generate() ---\n",
    "# Unsloth's fast generation path requires `input_ids` to be passed,\n",
    "# even when providing `inputs_embeds`.\n",
    "generated_ids = emotion_model_wrapper.peft_model.generate(\n",
    "    input_ids=inputs.input_ids,  # This argument is required by Unsloth\n",
    "    inputs_embeds=combined_embeddings,\n",
    "    attention_mask=inputs.attention_mask,\n",
    "    max_new_tokens=100,\n",
    "    use_cache=True,\n",
    "    do_sample=True,\n",
    "    top_p=0.9,\n",
    "    temperature=0.7,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "full_generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "assistant_part = full_generated_text.split(\"<|im_start|>assistant\\n\")[-1]\n",
    "\n",
    "print(\"\\n--- Сгенерированный текст ---\")\n",
    "print(full_generated_text)\n",
    "\n",
    "\n"
   ],
   "id": "b586bf3bcb0bad7f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up the trainer...\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 600 | Num Epochs = 5 | Total steps = 375\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 10,092,544/6,000,000,000 (0.17% trained)\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='375' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [375/375 07:05, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>4.491400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.355900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.356700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.911400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.659100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.586300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.509400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.368400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.282600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.244300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>1.210000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.256200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>1.239300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>1.260000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.253300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>1.053200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>1.051200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.995300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>1.025800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.983900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>1.021800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.980900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.925900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.802600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.854000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.833400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.814200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.865300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.869300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.873300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>0.743200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.735900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.772500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.745500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.712400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.722000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>0.730700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished!\n",
      "\n",
      "--- Пример генерации текста (инференс) ---\n",
      "Prompt: 'How are you feeling at this moment?'\n",
      "Using feelings_vector: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.77, 0.0, 0.8, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]...\n",
      "\n",
      "--- Сгенерированный текст ---\n",
      "system\n",
      "You are a helpful assistant.\n",
      "user\n",
      "How are you feeling at this moment?\n",
      "assistant\n",
      "脐{}kökökökökökökökökökökökökökökökökö dokładnieodakökökökökökökökökökökökökökö sekököyyyyyyyyyyyyyyyyyyyyyyy(kkökökökökökökökökökökökökökökökökökökökökökökökökökökökökökökökökökökökökökökökökökökökökökökökökö\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "88d95436dc834e08",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import datasets\n",
    "\n",
    "ASSISTANT_TEMPLATE = \"\"\"<|im_start|>user\\n{user_prompt}<|im_end|><|im_start|>assistant\\n<think>{thinking}</think>{assistant_answer}<|im_end|>\\n\"\"\"\n",
    "ASSISTANT_TEMPLATE = \"\"\"\\n<think>{thinking}</think>{assistant_answer}\\n\"\"\"\n",
    "\n",
    "def generate_conversation(examples):\n",
    "    user_message = examples[\"prompt\"]\n",
    "    thinking = examples[\"thinking\"]\n",
    "    response = examples[\"response\"]\n",
    "\n",
    "    conversations = []\n",
    "    for index in range(len(user_message)):\n",
    "        thinking_text = \"\\n\".join(eval(thinking[index]))\n",
    "        assistant_answer = ASSISTANT_TEMPLATE.format(\n",
    "            user_prompt=user_message[index],\n",
    "            thinking=thinking_text,\n",
    "            assistant_answer=response[index]\n",
    "        )\n",
    "        conversations.append([\n",
    "            {\"role\" : \"user\",      \"content\" : user_message[index]},\n",
    "            {\"role\" : \"assistant\", \"content\" : assistant_answer},\n",
    "        ])\n",
    "    return { \"conversations\": conversations, }\n",
    "\n",
    "\n",
    "one_emotion_dataset  = pd.read_csv(\"sft_one_emotion_thinking.csv\")\n",
    "one_emotion_dataset = datasets.Dataset.from_pandas(one_emotion_dataset)\n",
    "one_emotion_dataset = one_emotion_dataset.map(\n",
    "    generate_conversation,\n",
    "    remove_columns=one_emotion_dataset.column_names,\n",
    "    batched=True,\n",
    "    batch_size=1000,\n",
    ")"
   ],
   "id": "a77570839bfa90c0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "one_emotion_dataset  = pd.read_csv(\"sft_one_emotion_thinking.csv\")\n",
    "one_emotion_dataset"
   ],
   "id": "ce2603cf4dcf6acf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "one_emotion_dataset",
   "id": "cb4e062a542f9c08",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
