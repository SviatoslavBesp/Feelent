{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "!pip install --force-reinstall -U ipywidgets\n",
    "!pip install --force-reinstall unsloth\n",
    "\n",
    "!pip3 install --force-reinstall torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ],
   "id": "77499415f6338f7d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "import unsloth\n",
   "id": "8f31c95c6739d77a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# import unsloth\n",
    "\n",
    "import torch\n",
    "\n",
    "print(f\"Версия PyTorch: {torch.__version__}\")\n",
    "print(f\"Доступна ли CUDA: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Версия CUDA для PyTorch: {torch.version.cuda}\")\n",
    "    print(f\"Имя GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\">>> CUDA недоступна. Установлена CPU-версия PyTorch или есть проблема с совместимостью.\")"
   ],
   "id": "5eb20d35ab155ba9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-17T07:25:06.537668Z",
     "start_time": "2025-06-17T07:24:11.048762Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "import unsloth\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "from typing import Optional, List, Dict, Any\n",
    "\n",
    "from transformers import AutoConfig, TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. DEFINITION OF THE CUSTOM MODEL WRAPPER\n",
    "# ==============================================================================\n",
    "class EmotionUnslothModel(nn.Module):\n",
    "    \"\"\"\n",
    "    An unsloth-optimized wrapper that includes a trainable vector projector.\n",
    "    This class takes a raw, fixed-size emotion vector, projects it to the\n",
    "    model's hidden dimension, and then injects it into the forward pass.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name_or_path: str,\n",
    "        raw_emotion_vector_size: int,\n",
    "        lora_rank: int = 16,\n",
    "        lora_alpha: int = 16,\n",
    "        use_4bit: bool = True,\n",
    "        max_seq_length: int = 2048,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes the EmotionUnslothModel with a vector projector.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Load unsloth model and tokenizer\n",
    "        self.model, self.tokenizer = FastLanguageModel.from_pretrained(\n",
    "            model_name=model_name_or_path,\n",
    "            max_seq_length=max_seq_length,\n",
    "            load_in_4bit=use_4bit,\n",
    "            cache_dir=\"./model_cache\",\n",
    "        )\n",
    "        model_hidden_size = self.model.config.hidden_size\n",
    "\n",
    "        # Define the vector projector\n",
    "        self.vector_projector = nn.Linear(\n",
    "            in_features=raw_emotion_vector_size,\n",
    "            out_features=model_hidden_size,\n",
    "            bias=False\n",
    "        )\n",
    "        self.vector_projector.to(\"cuda\",self.model.dtype)\n",
    "\n",
    "        # Apply LoRA using unsloth's function\n",
    "        self.peft_model = FastLanguageModel.get_peft_model(\n",
    "            self.model,\n",
    "            r=lora_rank,\n",
    "            lora_alpha=lora_alpha,\n",
    "            lora_dropout=0,\n",
    "            bias=\"none\",\n",
    "            use_gradient_checkpointing=True,\n",
    "            random_state=42,\n",
    "            target_modules=[\n",
    "                \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                \"gate_proj\", \"up_proj\", \"down_proj\"\n",
    "            ],\n",
    "            # modules_to_save=[\"vector_projector\"], # <--- УДАЛИТЕ ЭТУ СТРОКУ\n",
    "        )\n",
    "\n",
    "        # --- ДОБАВЬТЕ ЭТОТ БЛОК ---\n",
    "        # Manually unfreeze the projector weights after creating the PEFT model.\n",
    "        # This makes them trainable without using the restricted 'modules_to_save'.\n",
    "        for param in self.vector_projector.parameters():\n",
    "            param.requires_grad = True\n",
    "        # ---------------------------\n",
    "\n",
    "        self.peft_model.print_trainable_parameters()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor,\n",
    "        attention_mask: torch.Tensor,\n",
    "        emotion_vector: torch.Tensor,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        **kwargs,\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Performs the forward pass with projection and injection.\n",
    "        \"\"\"\n",
    "        projected_vector = self.vector_projector(emotion_vector)\n",
    "        embedding_layer = self.peft_model.get_input_embeddings()\n",
    "        token_embeddings = embedding_layer(input_ids)\n",
    "        combined_embeddings = token_embeddings + projected_vector.unsqueeze(1)\n",
    "\n",
    "        model_outputs = self.peft_model(\n",
    "            inputs_embeds=combined_embeddings,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels,\n",
    "            return_dict=True\n",
    "        )\n",
    "        return model_outputs\n",
    "\n",
    "    def generate(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor,\n",
    "        attention_mask: torch.Tensor,\n",
    "        emotion_vector: torch.Tensor,\n",
    "        **generation_kwargs: Any\n",
    "    ) -> List[int]:\n",
    "        \"\"\"\n",
    "        Generates text conditioned on a prompt and an emotion vector.\n",
    "        \"\"\"\n",
    "        if \"pad_token_id\" not in generation_kwargs:\n",
    "            generation_kwargs[\"pad_token_id\"] = self.tokenizer.eos_token_id\n",
    "\n",
    "        return self.peft_model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            emotion_vector=emotion_vector,\n",
    "            **generation_kwargs\n",
    "        )\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. DEFINITION OF THE CUSTOM DATA COLLATOR\n",
    "# ==============================================================================\n",
    "\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "class DataCollatorForEmotionLM(DataCollatorForLanguageModeling):\n",
    "    \"\"\"\n",
    "    Custom data collator that handles tokenizing text and stacking emotion vectors.\n",
    "    \"\"\"\n",
    "    def __call__(\n",
    "        self,\n",
    "        features: List[Dict[str, Any]]\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Processes a list of features to create a batch.\n",
    "        \"\"\"\n",
    "        emotion_vectors = [feature.pop(\"emotion_vector\") for feature in features]\n",
    "        batch = super().__call__(features)\n",
    "        batch['emotion_vector'] = torch.stack(emotion_vectors)\n",
    "        return batch\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. MAIN TRAINING SCRIPT\n",
    "# ==============================================================================\n",
    "\n",
    "def main():\n",
    "    # --- Configuration ---\n",
    "    MODEL_NAME = \"unsloth/Qwen3-0.6B-unsloth-bnb-4bit\"\n",
    "    MAX_SEQ_LENGTH = 2048\n",
    "    RAW_EMOTION_VECTOR_SIZE = 12\n",
    "\n",
    "    # --- Model Initialization ---\n",
    "    print(\"Initializing the model...\")\n",
    "    emotion_model_wrapper = EmotionUnslothModel(\n",
    "        model_name_or_path=MODEL_NAME,\n",
    "        raw_emotion_vector_size=RAW_EMOTION_VECTOR_SIZE,\n",
    "        max_seq_length=MAX_SEQ_LENGTH\n",
    "    )\n",
    "    model_dtype = emotion_model_wrapper.model.dtype\n",
    "\n",
    "    # --- Data Preparation ---\n",
    "    print(\"Preparing the dataset...\")\n",
    "    # For Llama-3 instruct model, we should use its specific chat template\n",
    "    prompt_template = \"\"\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "{}<|eot_id|>\"\"\"\n",
    "\n",
    "    raw_data = [\n",
    "        {\"text\": prompt_template.format(\"Write a happy poem about spring.\", \"The sunbeams dance, a joyful sight,\\nNew flowers bloom in colors bright.\"), \"emotion\": \"happy\"},\n",
    "        {\"text\": prompt_template.format(\"Describe a spooky, abandoned mansion.\", \"The old manor stood in chilling dread,\\nWhere silent ghosts and shadows tread.\"), \"emotion\": \"spooky\"},\n",
    "        {\"text\": prompt_template.format(\"Compose a short, joyful song about a river.\", \"The river flows, a happy tune,\\nBeneath the sunny afternoon.\"), \"emotion\": \"happy\"},\n",
    "        {\"text\": prompt_template.format(\"Tell a short, eerie tale about a forest at night.\", \"Deep in the woods, when moonlight fails,\\nA whisper rides on chilling gales.\"), \"emotion\": \"spooky\"},\n",
    "    ]\n",
    "    dataset = Dataset.from_list(raw_data)\n",
    "\n",
    "    # Create and map emotion vectors to the dataset\n",
    "    emotion_mapping = {\n",
    "        \"happy\": torch.from_numpy(np.random.rand(RAW_EMOTION_VECTOR_SIZE) * 0.1),\n",
    "        \"spooky\": torch.from_numpy(np.random.rand(RAW_EMOTION_VECTOR_SIZE) * -0.1)\n",
    "    }\n",
    "\n",
    "    # CORRECTED: Define the function inside main() to access model_dtype\n",
    "    def add_emotion_vector(example: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Adds the emotion vector to a dataset example with the correct dtype on CPU.\"\"\"\n",
    "        em_vector = emotion_mapping[example[\"emotion\"]]\n",
    "        # The tensor should be created with the model's dtype, but remain on the CPU.\n",
    "        example[\"emotion_vector\"] = em_vector.to(dtype=model_dtype)\n",
    "        return example\n",
    "\n",
    "    # CORRECTED: Call .map() with only the function argument\n",
    "    dataset = dataset.map(add_emotion_vector)\n",
    "\n",
    "    # --- Pre-processing and Tokenization ---\n",
    "    def preprocess_function(example: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Tokenizes the text and prepares labels.\"\"\"\n",
    "        tokenized_example = emotion_model_wrapper.tokenizer(\n",
    "            example[\"text\"],\n",
    "            truncation=True,\n",
    "            max_length=MAX_SEQ_LENGTH,\n",
    "            padding=False,\n",
    "            return_tensors=None,\n",
    "        )\n",
    "        tokenized_example[\"labels\"] = tokenized_example[\"input_ids\"][:]\n",
    "        return tokenized_example\n",
    "\n",
    "    tokenized_dataset = dataset.map(\n",
    "        preprocess_function,\n",
    "        batched=True,\n",
    "        remove_columns=dataset.column_names\n",
    "    )\n",
    "\n",
    "    # --- Trainer Setup ---\n",
    "    print(\"Setting up the trainer...\")\n",
    "    data_collator = DataCollatorForEmotionLM(\n",
    "        tokenizer=emotion_model_wrapper.tokenizer,\n",
    "        mlm=False\n",
    "    )\n",
    "\n",
    "    trainer = SFTTrainer(\n",
    "        model=emotion_model_wrapper.peft_model,\n",
    "        tokenizer=emotion_model_wrapper.tokenizer,\n",
    "        train_dataset=tokenized_dataset,\n",
    "        data_collator=data_collator,\n",
    "        max_seq_length=MAX_SEQ_LENGTH,\n",
    "        args=TrainingArguments(\n",
    "            per_device_train_batch_size=2,\n",
    "            gradient_accumulation_steps=4,\n",
    "            warmup_steps=5,\n",
    "            max_steps=50,  # Increase for real training\n",
    "            learning_rate=2e-4,\n",
    "            fp16=not torch.cuda.is_bf16_supported(),\n",
    "            bf16=torch.cuda.is_bf16_supported(),\n",
    "            logging_steps=1,\n",
    "            optim=\"adamw_8bit\",\n",
    "            weight_decay=0.01,\n",
    "            lr_scheduler_type=\"linear\",\n",
    "            seed=42,\n",
    "            output_dir=\"outputs\",\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # --- Start Training ---\n",
    "    print(\"Starting training...\")\n",
    "    trainer.train()\n",
    "    print(\"Training finished!\")\n",
    "\n",
    "      # --- Inference Example ---\n",
    "\n",
    "    print(\"\\n--- Пример генерации текста (инференс) ---\")\n",
    "\n",
    "    inference_prompt = \"Tell me about a sunny day.\"\n",
    "    inference_emotion = \"happy\"\n",
    "\n",
    "\n",
    "    formatted_prompt = prompt_template.format(inference_prompt, \"\")\n",
    "    inputs = emotion_model_wrapper.tokenizer(formatted_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    # Manually prepare `inputs_embeds` for inference\n",
    "    # 1. Get the emotion vector and project it\n",
    "    emotion_vector_tensor = emotion_mapping[inference_emotion].unsqueeze(0).to(\"cuda\",dtype=model_dtype)\n",
    "    projected_vector = emotion_model_wrapper.vector_projector(emotion_vector_tensor)\n",
    "\n",
    "    # 2. Get the token embeddings from the input_ids\n",
    "    embedding_layer = emotion_model_wrapper.peft_model.get_input_embeddings()\n",
    "    token_embeddings = embedding_layer(inputs.input_ids)\n",
    "\n",
    "    # 3. Combine them to create final inputs_embeds\n",
    "    combined_embeddings = token_embeddings + projected_vector.unsqueeze(1)\n",
    "\n",
    "    # Call generate with `inputs_embeds` instead of `input_ids`.\n",
    "    generated_ids = emotion_model_wrapper.peft_model.generate(\n",
    "        input_ids=inputs.input_ids, # <--- ВОЗВРАЩАЕМ ЭТОТ АРГУМЕНТ\n",
    "        inputs_embeds=combined_embeddings,\n",
    "        attention_mask=inputs.attention_mask,\n",
    "        max_new_tokens=50,\n",
    "        use_cache=True,\n",
    "        do_sample=True,\n",
    "        top_p=0.9,\n",
    "        temperature=0.7,\n",
    "        pad_token_id=emotion_model_wrapper.tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "    generated_text = emotion_model_wrapper.tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "    print(\"\\n--- Сгенерированный текст ---\")\n",
    "    print(generated_text)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run the main function to start the training and inference process\n",
    "    main()"
   ],
   "id": "1d69364d2ad355a6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing the model...\n",
      "==((====))==  Unsloth 2025.3.19: Fast Qwen3 patching. Transformers: 4.51.3.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 3090. Num GPUs = 1. Max memory: 24.0 GB. Platform: Windows.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu118. CUDA: 8.6. CUDA Toolkit: 11.8. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = None. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: Making `model.base_model.model.model` require gradients\n",
      "trainable params: 10,092,544 || all params: 606,142,464 || trainable%: 1.6650\n",
      "Preparing the dataset...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/4 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b405363a7cfe45cf8b65da85bf41ceb6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/4 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "48a12bb8b6fd494cb5fcea4d8b27be0a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up the trainer...\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 4 | Num Epochs = 50 | Total steps = 50\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 10,092,544/6,000,000,000 (0.17% trained)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 00:26, Epoch 50/50]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.520700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.521100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.458600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.239900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.822400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.330800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.839300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.482400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.260000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.084200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.937500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.806900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.681900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.572700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.469100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.379600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.297300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.233700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.165800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.117200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.090100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.074100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.053400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.042600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.036500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.032700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.030000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.027300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.025800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.026700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.024800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.024600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.023800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.023600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.024000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.023100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.023000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.022100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.023000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.021500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.023500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.022000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.022100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.022300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.021800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.023300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.022000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.021900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.021400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.022200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished!\n",
      "\n",
      "--- Пример генерации текста (инференс) ---\n",
      "\n",
      "--- Сгенерированный текст ---\n",
      "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Tell me about a sunny day.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "<|eot_id|>|...|...|...|...|...|...|...|...|...|...|...|...|...|...|...|...|...|...|...|...|...|...|...|...|...\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "from triton.compiler.compiler import AttrsDescriptor",
   "id": "cb4e062a542f9c08",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
