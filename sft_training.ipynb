{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T08:19:38.225707Z",
     "start_time": "2025-06-18T08:12:46.415718Z"
    }
   },
   "cell_type": "code",
   "source": [
    "!pip install --force-reinstall -U ipywidgets\n",
    "!pip install --force-reinstall unsloth\n",
    "\n",
    "!pip3 install --force-reinstall torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ],
   "id": "77499415f6338f7d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ipywidgets\r\n",
      "  Using cached ipywidgets-8.1.7-py3-none-any.whl.metadata (2.4 kB)\r\n",
      "Collecting comm>=0.1.3 (from ipywidgets)\r\n",
      "  Using cached comm-0.2.2-py3-none-any.whl.metadata (3.7 kB)\r\n",
      "Collecting ipython>=6.1.0 (from ipywidgets)\r\n",
      "  Using cached ipython-9.3.0-py3-none-any.whl.metadata (4.4 kB)\r\n",
      "Collecting traitlets>=4.3.1 (from ipywidgets)\r\n",
      "  Using cached traitlets-5.14.3-py3-none-any.whl.metadata (10 kB)\r\n",
      "Collecting widgetsnbextension~=4.0.14 (from ipywidgets)\r\n",
      "  Using cached widgetsnbextension-4.0.14-py3-none-any.whl.metadata (1.6 kB)\r\n",
      "Collecting jupyterlab_widgets~=3.0.15 (from ipywidgets)\r\n",
      "  Using cached jupyterlab_widgets-3.0.15-py3-none-any.whl.metadata (20 kB)\r\n",
      "Collecting decorator (from ipython>=6.1.0->ipywidgets)\r\n",
      "  Using cached decorator-5.2.1-py3-none-any.whl.metadata (3.9 kB)\r\n",
      "Collecting ipython-pygments-lexers (from ipython>=6.1.0->ipywidgets)\r\n",
      "  Using cached ipython_pygments_lexers-1.1.1-py3-none-any.whl.metadata (1.1 kB)\r\n",
      "Collecting jedi>=0.16 (from ipython>=6.1.0->ipywidgets)\r\n",
      "  Using cached jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\r\n",
      "Collecting matplotlib-inline (from ipython>=6.1.0->ipywidgets)\r\n",
      "  Using cached matplotlib_inline-0.1.7-py3-none-any.whl.metadata (3.9 kB)\r\n",
      "Collecting pexpect>4.3 (from ipython>=6.1.0->ipywidgets)\r\n",
      "  Using cached pexpect-4.9.0-py2.py3-none-any.whl.metadata (2.5 kB)\r\n",
      "Collecting prompt_toolkit<3.1.0,>=3.0.41 (from ipython>=6.1.0->ipywidgets)\r\n",
      "  Using cached prompt_toolkit-3.0.51-py3-none-any.whl.metadata (6.4 kB)\r\n",
      "Collecting pygments>=2.4.0 (from ipython>=6.1.0->ipywidgets)\r\n",
      "  Using cached pygments-2.19.1-py3-none-any.whl.metadata (2.5 kB)\r\n",
      "Collecting stack_data (from ipython>=6.1.0->ipywidgets)\r\n",
      "  Using cached stack_data-0.6.3-py3-none-any.whl.metadata (18 kB)\r\n",
      "Collecting parso<0.9.0,>=0.8.4 (from jedi>=0.16->ipython>=6.1.0->ipywidgets)\r\n",
      "  Using cached parso-0.8.4-py2.py3-none-any.whl.metadata (7.7 kB)\r\n",
      "Collecting ptyprocess>=0.5 (from pexpect>4.3->ipython>=6.1.0->ipywidgets)\r\n",
      "  Using cached ptyprocess-0.7.0-py2.py3-none-any.whl.metadata (1.3 kB)\r\n",
      "Collecting wcwidth (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets)\r\n",
      "  Using cached wcwidth-0.2.13-py2.py3-none-any.whl.metadata (14 kB)\r\n",
      "Collecting executing>=1.2.0 (from stack_data->ipython>=6.1.0->ipywidgets)\r\n",
      "  Using cached executing-2.2.0-py2.py3-none-any.whl.metadata (8.9 kB)\r\n",
      "Collecting asttokens>=2.1.0 (from stack_data->ipython>=6.1.0->ipywidgets)\r\n",
      "  Using cached asttokens-3.0.0-py3-none-any.whl.metadata (4.7 kB)\r\n",
      "Collecting pure-eval (from stack_data->ipython>=6.1.0->ipywidgets)\r\n",
      "  Using cached pure_eval-0.2.3-py3-none-any.whl.metadata (6.3 kB)\r\n",
      "Using cached ipywidgets-8.1.7-py3-none-any.whl (139 kB)\r\n",
      "Using cached comm-0.2.2-py3-none-any.whl (7.2 kB)\r\n",
      "Using cached ipython-9.3.0-py3-none-any.whl (605 kB)\r\n",
      "Using cached jupyterlab_widgets-3.0.15-py3-none-any.whl (216 kB)\r\n",
      "Using cached traitlets-5.14.3-py3-none-any.whl (85 kB)\r\n",
      "Using cached widgetsnbextension-4.0.14-py3-none-any.whl (2.2 MB)\r\n",
      "Using cached jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\r\n",
      "Using cached pexpect-4.9.0-py2.py3-none-any.whl (63 kB)\r\n",
      "Using cached prompt_toolkit-3.0.51-py3-none-any.whl (387 kB)\r\n",
      "Using cached pygments-2.19.1-py3-none-any.whl (1.2 MB)\r\n",
      "Using cached decorator-5.2.1-py3-none-any.whl (9.2 kB)\r\n",
      "Using cached ipython_pygments_lexers-1.1.1-py3-none-any.whl (8.1 kB)\r\n",
      "Using cached matplotlib_inline-0.1.7-py3-none-any.whl (9.9 kB)\r\n",
      "Using cached stack_data-0.6.3-py3-none-any.whl (24 kB)\r\n",
      "Using cached asttokens-3.0.0-py3-none-any.whl (26 kB)\r\n",
      "Using cached executing-2.2.0-py2.py3-none-any.whl (26 kB)\r\n",
      "Using cached parso-0.8.4-py2.py3-none-any.whl (103 kB)\r\n",
      "Using cached ptyprocess-0.7.0-py2.py3-none-any.whl (13 kB)\r\n",
      "Using cached pure_eval-0.2.3-py3-none-any.whl (11 kB)\r\n",
      "Using cached wcwidth-0.2.13-py2.py3-none-any.whl (34 kB)\r\n",
      "Installing collected packages: wcwidth, pure-eval, ptyprocess, widgetsnbextension, traitlets, pygments, prompt_toolkit, pexpect, parso, jupyterlab_widgets, executing, decorator, asttokens, stack_data, matplotlib-inline, jedi, ipython-pygments-lexers, comm, ipython, ipywidgets\r\n",
      "  Attempting uninstall: wcwidth\r\n",
      "    Found existing installation: wcwidth 0.2.13\r\n",
      "    Uninstalling wcwidth-0.2.13:\r\n",
      "      Successfully uninstalled wcwidth-0.2.13\r\n",
      "  Attempting uninstall: pure-eval\r\n",
      "    Found existing installation: pure_eval 0.2.3\r\n",
      "    Uninstalling pure_eval-0.2.3:\r\n",
      "      Successfully uninstalled pure_eval-0.2.3\r\n",
      "  Attempting uninstall: ptyprocess\r\n",
      "    Found existing installation: ptyprocess 0.7.0\r\n",
      "    Uninstalling ptyprocess-0.7.0:\r\n",
      "      Successfully uninstalled ptyprocess-0.7.0\r\n",
      "  Attempting uninstall: widgetsnbextension\r\n",
      "    Found existing installation: widgetsnbextension 4.0.14\r\n",
      "    Uninstalling widgetsnbextension-4.0.14:\r\n",
      "      Successfully uninstalled widgetsnbextension-4.0.14\r\n",
      "  Attempting uninstall: traitlets\r\n",
      "    Found existing installation: traitlets 5.14.3\r\n",
      "    Uninstalling traitlets-5.14.3:\r\n",
      "      Successfully uninstalled traitlets-5.14.3\r\n",
      "  Attempting uninstall: pygments\r\n",
      "    Found existing installation: Pygments 2.19.1\r\n",
      "    Uninstalling Pygments-2.19.1:\r\n",
      "      Successfully uninstalled Pygments-2.19.1\r\n",
      "  Attempting uninstall: prompt_toolkit\r\n",
      "    Found existing installation: prompt_toolkit 3.0.51\r\n",
      "    Uninstalling prompt_toolkit-3.0.51:\r\n",
      "      Successfully uninstalled prompt_toolkit-3.0.51\r\n",
      "  Attempting uninstall: pexpect\r\n",
      "    Found existing installation: pexpect 4.9.0\r\n",
      "    Uninstalling pexpect-4.9.0:\r\n",
      "      Successfully uninstalled pexpect-4.9.0\r\n",
      "  Attempting uninstall: parso\r\n",
      "    Found existing installation: parso 0.8.4\r\n",
      "    Uninstalling parso-0.8.4:\r\n",
      "      Successfully uninstalled parso-0.8.4\r\n",
      "  Attempting uninstall: jupyterlab_widgets\r\n",
      "    Found existing installation: jupyterlab_widgets 3.0.15\r\n",
      "    Uninstalling jupyterlab_widgets-3.0.15:\r\n",
      "      Successfully uninstalled jupyterlab_widgets-3.0.15\r\n",
      "  Attempting uninstall: executing\r\n",
      "    Found existing installation: executing 2.2.0\r\n",
      "    Uninstalling executing-2.2.0:\r\n",
      "      Successfully uninstalled executing-2.2.0\r\n",
      "  Attempting uninstall: decorator\r\n",
      "    Found existing installation: decorator 5.2.1\r\n",
      "    Uninstalling decorator-5.2.1:\r\n",
      "      Successfully uninstalled decorator-5.2.1\r\n",
      "  Attempting uninstall: asttokens\r\n",
      "    Found existing installation: asttokens 3.0.0\r\n",
      "    Uninstalling asttokens-3.0.0:\r\n",
      "      Successfully uninstalled asttokens-3.0.0\r\n",
      "  Attempting uninstall: stack_data\r\n",
      "    Found existing installation: stack-data 0.6.3\r\n",
      "    Uninstalling stack-data-0.6.3:\r\n",
      "      Successfully uninstalled stack-data-0.6.3\r\n",
      "  Attempting uninstall: matplotlib-inline\r\n",
      "    Found existing installation: matplotlib-inline 0.1.7\r\n",
      "    Uninstalling matplotlib-inline-0.1.7:\r\n",
      "      Successfully uninstalled matplotlib-inline-0.1.7\r\n",
      "  Attempting uninstall: jedi\r\n",
      "    Found existing installation: jedi 0.19.2\r\n",
      "    Uninstalling jedi-0.19.2:\r\n",
      "      Successfully uninstalled jedi-0.19.2\r\n",
      "  Attempting uninstall: ipython-pygments-lexers\r\n",
      "    Found existing installation: ipython_pygments_lexers 1.1.1\r\n",
      "    Uninstalling ipython_pygments_lexers-1.1.1:\r\n",
      "      Successfully uninstalled ipython_pygments_lexers-1.1.1\r\n",
      "  Attempting uninstall: comm\r\n",
      "    Found existing installation: comm 0.2.2\r\n",
      "    Uninstalling comm-0.2.2:\r\n",
      "      Successfully uninstalled comm-0.2.2\r\n",
      "  Attempting uninstall: ipython\r\n",
      "    Found existing installation: ipython 9.3.0\r\n",
      "    Uninstalling ipython-9.3.0:\r\n",
      "      Successfully uninstalled ipython-9.3.0\r\n",
      "  Attempting uninstall: ipywidgets\r\n",
      "    Found existing installation: ipywidgets 8.1.7\r\n",
      "    Uninstalling ipywidgets-8.1.7:\r\n",
      "      Successfully uninstalled ipywidgets-8.1.7\r\n",
      "Successfully installed asttokens-3.0.0 comm-0.2.2 decorator-5.2.1 executing-2.2.0 ipython-9.3.0 ipython-pygments-lexers-1.1.1 ipywidgets-8.1.7 jedi-0.19.2 jupyterlab_widgets-3.0.15 matplotlib-inline-0.1.7 parso-0.8.4 pexpect-4.9.0 prompt_toolkit-3.0.51 ptyprocess-0.7.0 pure-eval-0.2.3 pygments-2.19.1 stack_data-0.6.3 traitlets-5.14.3 wcwidth-0.2.13 widgetsnbextension-4.0.14\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m25.0.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m25.1.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n",
      "Collecting unsloth\r\n",
      "  Downloading unsloth-2025.6.2-py3-none-any.whl.metadata (47 kB)\r\n",
      "Collecting unsloth_zoo>=2025.6.1 (from unsloth)\r\n",
      "  Using cached unsloth_zoo-2025.6.1-py3-none-any.whl.metadata (8.1 kB)\r\n",
      "Collecting torch<=2.7.0,>=2.4.0 (from unsloth)\r\n",
      "  Using cached torch-2.7.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (29 kB)\r\n",
      "Collecting xformers>=0.0.27.post2 (from unsloth)\r\n",
      "  Using cached xformers-0.0.30-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (1.2 kB)\r\n",
      "Collecting bitsandbytes (from unsloth)\r\n",
      "  Using cached bitsandbytes-0.46.0-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\r\n",
      "Collecting triton>=3.0.0 (from unsloth)\r\n",
      "  Using cached triton-3.3.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting packaging (from unsloth)\r\n",
      "  Using cached packaging-25.0-py3-none-any.whl.metadata (3.3 kB)\r\n",
      "Collecting tyro (from unsloth)\r\n",
      "  Using cached tyro-0.9.24-py3-none-any.whl.metadata (11 kB)\r\n",
      "Collecting transformers!=4.47.0,!=4.52.0,!=4.52.1,!=4.52.2,>=4.51.3 (from unsloth)\r\n",
      "  Using cached transformers-4.52.4-py3-none-any.whl.metadata (38 kB)\r\n",
      "Collecting datasets>=3.4.1 (from unsloth)\r\n",
      "  Using cached datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\r\n",
      "Collecting sentencepiece>=0.2.0 (from unsloth)\r\n",
      "  Using cached sentencepiece-0.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\r\n",
      "Collecting tqdm (from unsloth)\r\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\r\n",
      "Collecting psutil (from unsloth)\r\n",
      "  Using cached psutil-7.0.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (22 kB)\r\n",
      "Collecting wheel>=0.42.0 (from unsloth)\r\n",
      "  Using cached wheel-0.45.1-py3-none-any.whl.metadata (2.3 kB)\r\n",
      "Collecting numpy (from unsloth)\r\n",
      "  Using cached numpy-2.3.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (62 kB)\r\n",
      "Collecting accelerate>=0.34.1 (from unsloth)\r\n",
      "  Using cached accelerate-1.7.0-py3-none-any.whl.metadata (19 kB)\r\n",
      "Collecting trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9 (from unsloth)\r\n",
      "  Downloading trl-0.18.2-py3-none-any.whl.metadata (11 kB)\r\n",
      "Collecting peft!=0.11.0,>=0.7.1 (from unsloth)\r\n",
      "  Using cached peft-0.15.2-py3-none-any.whl.metadata (13 kB)\r\n",
      "Collecting protobuf (from unsloth)\r\n",
      "  Downloading protobuf-6.31.1-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)\r\n",
      "Collecting huggingface_hub (from unsloth)\r\n",
      "  Downloading huggingface_hub-0.33.0-py3-none-any.whl.metadata (14 kB)\r\n",
      "Collecting hf_transfer (from unsloth)\r\n",
      "  Using cached hf_transfer-0.1.9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.7 kB)\r\n",
      "Collecting diffusers (from unsloth)\r\n",
      "  Using cached diffusers-0.33.1-py3-none-any.whl.metadata (19 kB)\r\n",
      "Collecting torchvision (from unsloth)\r\n",
      "  Using cached torchvision-0.22.1-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\r\n",
      "Collecting pyyaml (from accelerate>=0.34.1->unsloth)\r\n",
      "  Using cached PyYAML-6.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\r\n",
      "Collecting safetensors>=0.4.3 (from accelerate>=0.34.1->unsloth)\r\n",
      "  Using cached safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\r\n",
      "Collecting filelock (from datasets>=3.4.1->unsloth)\r\n",
      "  Using cached filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\r\n",
      "Collecting pyarrow>=15.0.0 (from datasets>=3.4.1->unsloth)\r\n",
      "  Using cached pyarrow-20.0.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\r\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets>=3.4.1->unsloth)\r\n",
      "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\r\n",
      "Collecting pandas (from datasets>=3.4.1->unsloth)\r\n",
      "  Using cached pandas-2.3.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (91 kB)\r\n",
      "Collecting requests>=2.32.2 (from datasets>=3.4.1->unsloth)\r\n",
      "  Using cached requests-2.32.4-py3-none-any.whl.metadata (4.9 kB)\r\n",
      "Collecting xxhash (from datasets>=3.4.1->unsloth)\r\n",
      "  Using cached xxhash-3.5.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\r\n",
      "Collecting multiprocess<0.70.17 (from datasets>=3.4.1->unsloth)\r\n",
      "  Using cached multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\r\n",
      "Collecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1->unsloth)\r\n",
      "  Using cached fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\r\n",
      "Collecting typing-extensions>=3.7.4.3 (from huggingface_hub->unsloth)\r\n",
      "  Using cached typing_extensions-4.14.0-py3-none-any.whl.metadata (3.0 kB)\r\n",
      "Collecting hf-xet<2.0.0,>=1.1.2 (from huggingface_hub->unsloth)\r\n",
      "  Downloading hf_xet-1.1.4-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (879 bytes)\r\n",
      "Collecting setuptools (from torch<=2.7.0,>=2.4.0->unsloth)\r\n",
      "  Using cached setuptools-80.9.0-py3-none-any.whl.metadata (6.6 kB)\r\n",
      "Collecting sympy>=1.13.3 (from torch<=2.7.0,>=2.4.0->unsloth)\r\n",
      "  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\r\n",
      "Collecting networkx (from torch<=2.7.0,>=2.4.0->unsloth)\r\n",
      "  Using cached networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\r\n",
      "Collecting jinja2 (from torch<=2.7.0,>=2.4.0->unsloth)\r\n",
      "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\r\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.6.77 (from torch<=2.7.0,>=2.4.0->unsloth)\r\n",
      "  Using cached nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-cuda-runtime-cu12==12.6.77 (from torch<=2.7.0,>=2.4.0->unsloth)\r\n",
      "  Using cached nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-cuda-cupti-cu12==12.6.80 (from torch<=2.7.0,>=2.4.0->unsloth)\r\n",
      "  Using cached nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\r\n",
      "Collecting nvidia-cudnn-cu12==9.5.1.17 (from torch<=2.7.0,>=2.4.0->unsloth)\r\n",
      "  Using cached nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl.metadata (1.6 kB)\r\n",
      "Collecting nvidia-cublas-cu12==12.6.4.1 (from torch<=2.7.0,>=2.4.0->unsloth)\r\n",
      "  Using cached nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-cufft-cu12==11.3.0.4 (from torch<=2.7.0,>=2.4.0->unsloth)\r\n",
      "  Using cached nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-curand-cu12==10.3.7.77 (from torch<=2.7.0,>=2.4.0->unsloth)\r\n",
      "  Using cached nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-cusolver-cu12==11.7.1.2 (from torch<=2.7.0,>=2.4.0->unsloth)\r\n",
      "  Using cached nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\r\n",
      "Collecting nvidia-cusparse-cu12==12.5.4.2 (from torch<=2.7.0,>=2.4.0->unsloth)\r\n",
      "  Using cached nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\r\n",
      "Collecting nvidia-cusparselt-cu12==0.6.3 (from torch<=2.7.0,>=2.4.0->unsloth)\r\n",
      "  Using cached nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\r\n",
      "Collecting nvidia-nccl-cu12==2.26.2 (from torch<=2.7.0,>=2.4.0->unsloth)\r\n",
      "  Using cached nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\r\n",
      "Collecting nvidia-nvtx-cu12==12.6.77 (from torch<=2.7.0,>=2.4.0->unsloth)\r\n",
      "  Using cached nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\r\n",
      "Collecting nvidia-nvjitlink-cu12==12.6.85 (from torch<=2.7.0,>=2.4.0->unsloth)\r\n",
      "  Using cached nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-cufile-cu12==1.11.1.6 (from torch<=2.7.0,>=2.4.0->unsloth)\r\n",
      "  Using cached nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting triton>=3.0.0 (from unsloth)\r\n",
      "  Using cached triton-3.3.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting regex!=2019.12.17 (from transformers!=4.47.0,!=4.52.0,!=4.52.1,!=4.52.2,>=4.51.3->unsloth)\r\n",
      "  Using cached regex-2024.11.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\r\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers!=4.47.0,!=4.52.0,!=4.52.1,!=4.52.2,>=4.51.3->unsloth)\r\n",
      "  Using cached tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\r\n",
      "Collecting protobuf (from unsloth)\r\n",
      "  Using cached protobuf-3.20.3-py2.py3-none-any.whl.metadata (720 bytes)\r\n",
      "Collecting cut_cross_entropy (from unsloth_zoo>=2025.6.1->unsloth)\r\n",
      "  Using cached cut_cross_entropy-25.1.1-py3-none-any.whl.metadata (9.3 kB)\r\n",
      "Collecting pillow (from unsloth_zoo>=2025.6.1->unsloth)\r\n",
      "  Using cached pillow-11.2.1-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (8.9 kB)\r\n",
      "Collecting msgspec (from unsloth_zoo>=2025.6.1->unsloth)\r\n",
      "  Using cached msgspec-0.19.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\r\n",
      "Collecting importlib-metadata (from diffusers->unsloth)\r\n",
      "  Using cached importlib_metadata-8.7.0-py3-none-any.whl.metadata (4.8 kB)\r\n",
      "INFO: pip is looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\r\n",
      "Collecting torchvision (from unsloth)\r\n",
      "  Using cached torchvision-0.22.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\r\n",
      "Collecting docstring-parser>=0.15 (from tyro->unsloth)\r\n",
      "  Using cached docstring_parser-0.16-py3-none-any.whl.metadata (3.0 kB)\r\n",
      "Collecting rich>=11.1.0 (from tyro->unsloth)\r\n",
      "  Using cached rich-14.0.0-py3-none-any.whl.metadata (18 kB)\r\n",
      "Collecting shtab>=1.5.6 (from tyro->unsloth)\r\n",
      "  Using cached shtab-1.7.2-py3-none-any.whl.metadata (7.4 kB)\r\n",
      "Collecting typeguard>=4.0.0 (from tyro->unsloth)\r\n",
      "  Using cached typeguard-4.4.3-py3-none-any.whl.metadata (3.4 kB)\r\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1->unsloth)\r\n",
      "  Downloading aiohttp-3.12.13-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.6 kB)\r\n",
      "Collecting charset_normalizer<4,>=2 (from requests>=2.32.2->datasets>=3.4.1->unsloth)\r\n",
      "  Using cached charset_normalizer-3.4.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (35 kB)\r\n",
      "Collecting idna<4,>=2.5 (from requests>=2.32.2->datasets>=3.4.1->unsloth)\r\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\r\n",
      "Collecting urllib3<3,>=1.21.1 (from requests>=2.32.2->datasets>=3.4.1->unsloth)\r\n",
      "  Using cached urllib3-2.4.0-py3-none-any.whl.metadata (6.5 kB)\r\n",
      "Collecting certifi>=2017.4.17 (from requests>=2.32.2->datasets>=3.4.1->unsloth)\r\n",
      "  Downloading certifi-2025.6.15-py3-none-any.whl.metadata (2.4 kB)\r\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=11.1.0->tyro->unsloth)\r\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\r\n",
      "Collecting pygments<3.0.0,>=2.13.0 (from rich>=11.1.0->tyro->unsloth)\r\n",
      "  Using cached pygments-2.19.1-py3-none-any.whl.metadata (2.5 kB)\r\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch<=2.7.0,>=2.4.0->unsloth)\r\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\r\n",
      "Collecting zipp>=3.20 (from importlib-metadata->diffusers->unsloth)\r\n",
      "  Using cached zipp-3.23.0-py3-none-any.whl.metadata (3.6 kB)\r\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch<=2.7.0,>=2.4.0->unsloth)\r\n",
      "  Using cached MarkupSafe-3.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\r\n",
      "Collecting python-dateutil>=2.8.2 (from pandas->datasets>=3.4.1->unsloth)\r\n",
      "  Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\r\n",
      "Collecting pytz>=2020.1 (from pandas->datasets>=3.4.1->unsloth)\r\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\r\n",
      "Collecting tzdata>=2022.7 (from pandas->datasets>=3.4.1->unsloth)\r\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\r\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1->unsloth)\r\n",
      "  Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\r\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1->unsloth)\r\n",
      "  Using cached aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\r\n",
      "Collecting attrs>=17.3.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1->unsloth)\r\n",
      "  Using cached attrs-25.3.0-py3-none-any.whl.metadata (10 kB)\r\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1->unsloth)\r\n",
      "  Downloading frozenlist-1.7.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\r\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1->unsloth)\r\n",
      "  Downloading multidict-6.5.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\r\n",
      "Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1->unsloth)\r\n",
      "  Downloading propcache-0.3.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\r\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1->unsloth)\r\n",
      "  Downloading yarl-1.20.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (73 kB)\r\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro->unsloth)\r\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\r\n",
      "Collecting six>=1.5 (from python-dateutil>=2.8.2->pandas->datasets>=3.4.1->unsloth)\r\n",
      "  Using cached six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)\r\n",
      "Downloading unsloth-2025.6.2-py3-none-any.whl (276 kB)\r\n",
      "Using cached accelerate-1.7.0-py3-none-any.whl (362 kB)\r\n",
      "Using cached datasets-3.6.0-py3-none-any.whl (491 kB)\r\n",
      "Downloading huggingface_hub-0.33.0-py3-none-any.whl (514 kB)\r\n",
      "Using cached numpy-2.3.0-cp312-cp312-manylinux_2_28_x86_64.whl (16.6 MB)\r\n",
      "Using cached packaging-25.0-py3-none-any.whl (66 kB)\r\n",
      "Using cached peft-0.15.2-py3-none-any.whl (411 kB)\r\n",
      "Using cached sentencepiece-0.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\r\n",
      "Using cached torch-2.7.0-cp312-cp312-manylinux_2_28_x86_64.whl (865.0 MB)\r\n",
      "Using cached triton-3.3.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (156.5 MB)\r\n",
      "Using cached nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (393.1 MB)\r\n",
      "Using cached nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.9 MB)\r\n",
      "Using cached nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl (23.7 MB)\r\n",
      "Using cached nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (897 kB)\r\n",
      "Using cached nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl (571.0 MB)\r\n",
      "Using cached nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (200.2 MB)\r\n",
      "Using cached nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.1 MB)\r\n",
      "Using cached nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (56.3 MB)\r\n",
      "Using cached nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (158.2 MB)\r\n",
      "Using cached nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (216.6 MB)\r\n",
      "Using cached nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl (156.8 MB)\r\n",
      "Using cached nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (201.3 MB)\r\n",
      "Using cached nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (19.7 MB)\r\n",
      "Using cached nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\r\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\r\n",
      "Using cached transformers-4.52.4-py3-none-any.whl (10.5 MB)\r\n",
      "Downloading trl-0.18.2-py3-none-any.whl (366 kB)\r\n",
      "Using cached unsloth_zoo-2025.6.1-py3-none-any.whl (147 kB)\r\n",
      "Using cached protobuf-3.20.3-py2.py3-none-any.whl (162 kB)\r\n",
      "Using cached wheel-0.45.1-py3-none-any.whl (72 kB)\r\n",
      "Using cached xformers-0.0.30-cp312-cp312-manylinux_2_28_x86_64.whl (31.5 MB)\r\n",
      "Using cached bitsandbytes-0.46.0-py3-none-manylinux_2_24_x86_64.whl (67.0 MB)\r\n",
      "Using cached diffusers-0.33.1-py3-none-any.whl (3.6 MB)\r\n",
      "Using cached hf_transfer-0.1.9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\r\n",
      "Using cached psutil-7.0.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (277 kB)\r\n",
      "Using cached torchvision-0.22.0-cp312-cp312-manylinux_2_28_x86_64.whl (7.4 MB)\r\n",
      "Using cached tyro-0.9.24-py3-none-any.whl (128 kB)\r\n",
      "Using cached dill-0.3.8-py3-none-any.whl (116 kB)\r\n",
      "Using cached docstring_parser-0.16-py3-none-any.whl (36 kB)\r\n",
      "Using cached fsspec-2025.3.0-py3-none-any.whl (193 kB)\r\n",
      "Downloading hf_xet-1.1.4-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m3.1/3.1 MB\u001B[0m \u001B[31m12.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hUsing cached multiprocess-0.70.16-py312-none-any.whl (146 kB)\r\n",
      "Using cached pillow-11.2.1-cp312-cp312-manylinux_2_28_x86_64.whl (4.6 MB)\r\n",
      "Using cached pyarrow-20.0.0-cp312-cp312-manylinux_2_28_x86_64.whl (42.3 MB)\r\n",
      "Using cached PyYAML-6.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (767 kB)\r\n",
      "Using cached regex-2024.11.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (796 kB)\r\n",
      "Using cached requests-2.32.4-py3-none-any.whl (64 kB)\r\n",
      "Using cached rich-14.0.0-py3-none-any.whl (243 kB)\r\n",
      "Using cached safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\r\n",
      "Using cached setuptools-80.9.0-py3-none-any.whl (1.2 MB)\r\n",
      "Using cached shtab-1.7.2-py3-none-any.whl (14 kB)\r\n",
      "Using cached sympy-1.14.0-py3-none-any.whl (6.3 MB)\r\n",
      "Using cached tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\r\n",
      "Using cached typeguard-4.4.3-py3-none-any.whl (34 kB)\r\n",
      "Using cached typing_extensions-4.14.0-py3-none-any.whl (43 kB)\r\n",
      "Using cached cut_cross_entropy-25.1.1-py3-none-any.whl (22 kB)\r\n",
      "Using cached filelock-3.18.0-py3-none-any.whl (16 kB)\r\n",
      "Using cached importlib_metadata-8.7.0-py3-none-any.whl (27 kB)\r\n",
      "Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\r\n",
      "Using cached msgspec-0.19.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (213 kB)\r\n",
      "Using cached networkx-3.5-py3-none-any.whl (2.0 MB)\r\n",
      "Using cached pandas-2.3.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.0 MB)\r\n",
      "Using cached xxhash-3.5.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\r\n",
      "Downloading aiohttp-3.12.13-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.7/1.7 MB\u001B[0m \u001B[31m3.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0mta \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hDownloading certifi-2025.6.15-py3-none-any.whl (157 kB)\r\n",
      "Using cached charset_normalizer-3.4.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (148 kB)\r\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\r\n",
      "Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\r\n",
      "Using cached MarkupSafe-3.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23 kB)\r\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\r\n",
      "Using cached pygments-2.19.1-py3-none-any.whl (1.2 MB)\r\n",
      "Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\r\n",
      "Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\r\n",
      "Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\r\n",
      "Using cached urllib3-2.4.0-py3-none-any.whl (128 kB)\r\n",
      "Using cached zipp-3.23.0-py3-none-any.whl (10 kB)\r\n",
      "Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\r\n",
      "Using cached aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\r\n",
      "Using cached attrs-25.3.0-py3-none-any.whl (63 kB)\r\n",
      "Downloading frozenlist-1.7.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (241 kB)\r\n",
      "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\r\n",
      "Downloading multidict-6.5.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (237 kB)\r\n",
      "Downloading propcache-0.3.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (224 kB)\r\n",
      "Using cached six-1.17.0-py2.py3-none-any.whl (11 kB)\r\n",
      "Downloading yarl-1.20.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (355 kB)\r\n",
      "Installing collected packages: sentencepiece, pytz, nvidia-cusparselt-cu12, mpmath, zipp, xxhash, wheel, urllib3, tzdata, typing-extensions, tqdm, sympy, six, shtab, setuptools, safetensors, regex, pyyaml, pygments, pyarrow, psutil, protobuf, propcache, pillow, packaging, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, multidict, msgspec, mdurl, MarkupSafe, idna, hf-xet, hf_transfer, fsspec, frozenlist, filelock, docstring-parser, dill, charset_normalizer, certifi, attrs, aiohappyeyeballs, yarl, typeguard, triton, requests, python-dateutil, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, multiprocess, markdown-it-py, jinja2, importlib-metadata, aiosignal, rich, pandas, nvidia-cusolver-cu12, huggingface_hub, aiohttp, tyro, torch, tokenizers, diffusers, xformers, transformers, torchvision, datasets, cut_cross_entropy, bitsandbytes, accelerate, trl, peft, unsloth_zoo, unsloth\r\n",
      "  Attempting uninstall: sentencepiece\r\n",
      "    Found existing installation: sentencepiece 0.2.0\r\n",
      "    Uninstalling sentencepiece-0.2.0:\r\n",
      "      Successfully uninstalled sentencepiece-0.2.0\r\n",
      "  Attempting uninstall: pytz\r\n",
      "    Found existing installation: pytz 2025.2\r\n",
      "    Uninstalling pytz-2025.2:\r\n",
      "      Successfully uninstalled pytz-2025.2\r\n",
      "  Attempting uninstall: nvidia-cusparselt-cu12\r\n",
      "    Found existing installation: nvidia-cusparselt-cu12 0.6.3\r\n",
      "    Uninstalling nvidia-cusparselt-cu12-0.6.3:\r\n",
      "      Successfully uninstalled nvidia-cusparselt-cu12-0.6.3\r\n",
      "  Attempting uninstall: mpmath\r\n",
      "    Found existing installation: mpmath 1.3.0\r\n",
      "    Uninstalling mpmath-1.3.0:\r\n",
      "      Successfully uninstalled mpmath-1.3.0\r\n",
      "  Attempting uninstall: zipp\r\n",
      "    Found existing installation: zipp 3.23.0\r\n",
      "    Uninstalling zipp-3.23.0:\r\n",
      "      Successfully uninstalled zipp-3.23.0\r\n",
      "  Attempting uninstall: xxhash\r\n",
      "    Found existing installation: xxhash 3.5.0\r\n",
      "    Uninstalling xxhash-3.5.0:\r\n",
      "      Successfully uninstalled xxhash-3.5.0\r\n",
      "  Attempting uninstall: wheel\r\n",
      "    Found existing installation: wheel 0.45.1\r\n",
      "    Uninstalling wheel-0.45.1:\r\n",
      "      Successfully uninstalled wheel-0.45.1\r\n",
      "  Attempting uninstall: urllib3\r\n",
      "    Found existing installation: urllib3 2.4.0\r\n",
      "    Uninstalling urllib3-2.4.0:\r\n",
      "      Successfully uninstalled urllib3-2.4.0\r\n",
      "  Attempting uninstall: tzdata\r\n",
      "    Found existing installation: tzdata 2025.2\r\n",
      "    Uninstalling tzdata-2025.2:\r\n",
      "      Successfully uninstalled tzdata-2025.2\r\n",
      "  Attempting uninstall: typing-extensions\r\n",
      "    Found existing installation: typing_extensions 4.14.0\r\n",
      "    Uninstalling typing_extensions-4.14.0:\r\n",
      "      Successfully uninstalled typing_extensions-4.14.0\r\n",
      "  Attempting uninstall: tqdm\r\n",
      "    Found existing installation: tqdm 4.67.1\r\n",
      "    Uninstalling tqdm-4.67.1:\r\n",
      "      Successfully uninstalled tqdm-4.67.1\r\n",
      "  Attempting uninstall: sympy\r\n",
      "    Found existing installation: sympy 1.14.0\r\n",
      "    Uninstalling sympy-1.14.0:\r\n",
      "      Successfully uninstalled sympy-1.14.0\r\n",
      "  Attempting uninstall: six\r\n",
      "    Found existing installation: six 1.17.0\r\n",
      "    Uninstalling six-1.17.0:\r\n",
      "      Successfully uninstalled six-1.17.0\r\n",
      "  Attempting uninstall: shtab\r\n",
      "    Found existing installation: shtab 1.7.2\r\n",
      "    Uninstalling shtab-1.7.2:\r\n",
      "      Successfully uninstalled shtab-1.7.2\r\n",
      "  Attempting uninstall: setuptools\r\n",
      "    Found existing installation: setuptools 80.9.0\r\n",
      "    Uninstalling setuptools-80.9.0:\r\n",
      "      Successfully uninstalled setuptools-80.9.0\r\n",
      "  Attempting uninstall: safetensors\r\n",
      "    Found existing installation: safetensors 0.5.3\r\n",
      "    Uninstalling safetensors-0.5.3:\r\n",
      "      Successfully uninstalled safetensors-0.5.3\r\n",
      "  Attempting uninstall: regex\r\n",
      "    Found existing installation: regex 2024.11.6\r\n",
      "    Uninstalling regex-2024.11.6:\r\n",
      "      Successfully uninstalled regex-2024.11.6\r\n",
      "  Attempting uninstall: pyyaml\r\n",
      "    Found existing installation: PyYAML 6.0.2\r\n",
      "    Uninstalling PyYAML-6.0.2:\r\n",
      "      Successfully uninstalled PyYAML-6.0.2\r\n",
      "  Attempting uninstall: pygments\r\n",
      "    Found existing installation: Pygments 2.19.1\r\n",
      "    Uninstalling Pygments-2.19.1:\r\n",
      "      Successfully uninstalled Pygments-2.19.1\r\n",
      "  Attempting uninstall: pyarrow\r\n",
      "    Found existing installation: pyarrow 20.0.0\r\n",
      "    Uninstalling pyarrow-20.0.0:\r\n",
      "      Successfully uninstalled pyarrow-20.0.0\r\n",
      "  Attempting uninstall: psutil\r\n",
      "    Found existing installation: psutil 7.0.0\r\n",
      "    Uninstalling psutil-7.0.0:\r\n",
      "      Successfully uninstalled psutil-7.0.0\r\n",
      "  Attempting uninstall: protobuf\r\n",
      "    Found existing installation: protobuf 3.20.3\r\n",
      "    Uninstalling protobuf-3.20.3:\r\n",
      "      Successfully uninstalled protobuf-3.20.3\r\n",
      "  Attempting uninstall: propcache\r\n",
      "    Found existing installation: propcache 0.3.1\r\n",
      "    Uninstalling propcache-0.3.1:\r\n",
      "      Successfully uninstalled propcache-0.3.1\r\n",
      "  Attempting uninstall: pillow\r\n",
      "    Found existing installation: pillow 11.2.1\r\n",
      "    Uninstalling pillow-11.2.1:\r\n",
      "      Successfully uninstalled pillow-11.2.1\r\n",
      "  Attempting uninstall: packaging\r\n",
      "    Found existing installation: packaging 25.0\r\n",
      "    Uninstalling packaging-25.0:\r\n",
      "      Successfully uninstalled packaging-25.0\r\n",
      "  Attempting uninstall: nvidia-nvtx-cu12\r\n",
      "    Found existing installation: nvidia-nvtx-cu12 12.6.77\r\n",
      "    Uninstalling nvidia-nvtx-cu12-12.6.77:\r\n",
      "      Successfully uninstalled nvidia-nvtx-cu12-12.6.77\r\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\r\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.6.85\r\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.6.85:\r\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.6.85\r\n",
      "  Attempting uninstall: nvidia-nccl-cu12\r\n",
      "    Found existing installation: nvidia-nccl-cu12 2.26.2\r\n",
      "    Uninstalling nvidia-nccl-cu12-2.26.2:\r\n",
      "      Successfully uninstalled nvidia-nccl-cu12-2.26.2\r\n",
      "  Attempting uninstall: nvidia-curand-cu12\r\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.7.77\r\n",
      "    Uninstalling nvidia-curand-cu12-10.3.7.77:\r\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\r\n",
      "  Attempting uninstall: nvidia-cufile-cu12\r\n",
      "    Found existing installation: nvidia-cufile-cu12 1.11.1.6\r\n",
      "    Uninstalling nvidia-cufile-cu12-1.11.1.6:\r\n",
      "      Successfully uninstalled nvidia-cufile-cu12-1.11.1.6\r\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\r\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\r\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\r\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\r\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\r\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.6.77\r\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.6.77:\r\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.6.77\r\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\r\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\r\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\r\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\r\n",
      "  Attempting uninstall: nvidia-cublas-cu12\r\n",
      "    Found existing installation: nvidia-cublas-cu12 12.6.4.1\r\n",
      "    Uninstalling nvidia-cublas-cu12-12.6.4.1:\r\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.6.4.1\r\n",
      "  Attempting uninstall: numpy\r\n",
      "    Found existing installation: numpy 2.3.0\r\n",
      "    Uninstalling numpy-2.3.0:\r\n",
      "      Successfully uninstalled numpy-2.3.0\r\n",
      "  Attempting uninstall: networkx\r\n",
      "    Found existing installation: networkx 3.5\r\n",
      "    Uninstalling networkx-3.5:\r\n",
      "      Successfully uninstalled networkx-3.5\r\n",
      "  Attempting uninstall: multidict\r\n",
      "    Found existing installation: multidict 6.4.4\r\n",
      "    Uninstalling multidict-6.4.4:\r\n",
      "      Successfully uninstalled multidict-6.4.4\r\n",
      "  Attempting uninstall: msgspec\r\n",
      "    Found existing installation: msgspec 0.19.0\r\n",
      "    Uninstalling msgspec-0.19.0:\r\n",
      "      Successfully uninstalled msgspec-0.19.0\r\n",
      "  Attempting uninstall: mdurl\r\n",
      "    Found existing installation: mdurl 0.1.2\r\n",
      "    Uninstalling mdurl-0.1.2:\r\n",
      "      Successfully uninstalled mdurl-0.1.2\r\n",
      "  Attempting uninstall: MarkupSafe\r\n",
      "    Found existing installation: MarkupSafe 3.0.2\r\n",
      "    Uninstalling MarkupSafe-3.0.2:\r\n",
      "      Successfully uninstalled MarkupSafe-3.0.2\r\n",
      "  Attempting uninstall: idna\r\n",
      "    Found existing installation: idna 3.10\r\n",
      "    Uninstalling idna-3.10:\r\n",
      "      Successfully uninstalled idna-3.10\r\n",
      "  Attempting uninstall: hf-xet\r\n",
      "    Found existing installation: hf-xet 1.1.3\r\n",
      "    Uninstalling hf-xet-1.1.3:\r\n",
      "      Successfully uninstalled hf-xet-1.1.3\r\n",
      "  Attempting uninstall: hf_transfer\r\n",
      "    Found existing installation: hf_transfer 0.1.9\r\n",
      "    Uninstalling hf_transfer-0.1.9:\r\n",
      "      Successfully uninstalled hf_transfer-0.1.9\r\n",
      "  Attempting uninstall: fsspec\r\n",
      "    Found existing installation: fsspec 2025.3.0\r\n",
      "    Uninstalling fsspec-2025.3.0:\r\n",
      "      Successfully uninstalled fsspec-2025.3.0\r\n",
      "  Attempting uninstall: frozenlist\r\n",
      "    Found existing installation: frozenlist 1.6.2\r\n",
      "    Uninstalling frozenlist-1.6.2:\r\n",
      "      Successfully uninstalled frozenlist-1.6.2\r\n",
      "  Attempting uninstall: filelock\r\n",
      "    Found existing installation: filelock 3.18.0\r\n",
      "    Uninstalling filelock-3.18.0:\r\n",
      "      Successfully uninstalled filelock-3.18.0\r\n",
      "  Attempting uninstall: docstring-parser\r\n",
      "    Found existing installation: docstring_parser 0.16\r\n",
      "    Uninstalling docstring_parser-0.16:\r\n",
      "      Successfully uninstalled docstring_parser-0.16\r\n",
      "  Attempting uninstall: dill\r\n",
      "    Found existing installation: dill 0.3.8\r\n",
      "    Uninstalling dill-0.3.8:\r\n",
      "      Successfully uninstalled dill-0.3.8\r\n",
      "  Attempting uninstall: charset_normalizer\r\n",
      "    Found existing installation: charset-normalizer 3.4.2\r\n",
      "    Uninstalling charset-normalizer-3.4.2:\r\n",
      "      Successfully uninstalled charset-normalizer-3.4.2\r\n",
      "  Attempting uninstall: certifi\r\n",
      "    Found existing installation: certifi 2025.4.26\r\n",
      "    Uninstalling certifi-2025.4.26:\r\n",
      "      Successfully uninstalled certifi-2025.4.26\r\n",
      "  Attempting uninstall: attrs\r\n",
      "    Found existing installation: attrs 25.3.0\r\n",
      "    Uninstalling attrs-25.3.0:\r\n",
      "      Successfully uninstalled attrs-25.3.0\r\n",
      "  Attempting uninstall: aiohappyeyeballs\r\n",
      "    Found existing installation: aiohappyeyeballs 2.6.1\r\n",
      "    Uninstalling aiohappyeyeballs-2.6.1:\r\n",
      "      Successfully uninstalled aiohappyeyeballs-2.6.1\r\n",
      "  Attempting uninstall: yarl\r\n",
      "    Found existing installation: yarl 1.20.0\r\n",
      "    Uninstalling yarl-1.20.0:\r\n",
      "      Successfully uninstalled yarl-1.20.0\r\n",
      "  Attempting uninstall: typeguard\r\n",
      "    Found existing installation: typeguard 4.4.3\r\n",
      "    Uninstalling typeguard-4.4.3:\r\n",
      "      Successfully uninstalled typeguard-4.4.3\r\n",
      "  Attempting uninstall: triton\r\n",
      "    Found existing installation: triton 3.3.0\r\n",
      "    Uninstalling triton-3.3.0:\r\n",
      "      Successfully uninstalled triton-3.3.0\r\n",
      "  Attempting uninstall: requests\r\n",
      "    Found existing installation: requests 2.32.3\r\n",
      "    Uninstalling requests-2.32.3:\r\n",
      "      Successfully uninstalled requests-2.32.3\r\n",
      "  Attempting uninstall: python-dateutil\r\n",
      "    Found existing installation: python-dateutil 2.9.0.post0\r\n",
      "    Uninstalling python-dateutil-2.9.0.post0:\r\n",
      "      Successfully uninstalled python-dateutil-2.9.0.post0\r\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\r\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\r\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\r\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\r\n",
      "  Attempting uninstall: nvidia-cufft-cu12\r\n",
      "    Found existing installation: nvidia-cufft-cu12 11.3.0.4\r\n",
      "    Uninstalling nvidia-cufft-cu12-11.3.0.4:\r\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\r\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\r\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.5.1.17\r\n",
      "    Uninstalling nvidia-cudnn-cu12-9.5.1.17:\r\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.5.1.17\r\n",
      "  Attempting uninstall: multiprocess\r\n",
      "    Found existing installation: multiprocess 0.70.16\r\n",
      "    Uninstalling multiprocess-0.70.16:\r\n",
      "      Successfully uninstalled multiprocess-0.70.16\r\n",
      "  Attempting uninstall: markdown-it-py\r\n",
      "    Found existing installation: markdown-it-py 3.0.0\r\n",
      "    Uninstalling markdown-it-py-3.0.0:\r\n",
      "      Successfully uninstalled markdown-it-py-3.0.0\r\n",
      "  Attempting uninstall: jinja2\r\n",
      "    Found existing installation: Jinja2 3.1.6\r\n",
      "    Uninstalling Jinja2-3.1.6:\r\n",
      "      Successfully uninstalled Jinja2-3.1.6\r\n",
      "  Attempting uninstall: importlib-metadata\r\n",
      "    Found existing installation: importlib_metadata 8.7.0\r\n",
      "    Uninstalling importlib_metadata-8.7.0:\r\n",
      "      Successfully uninstalled importlib_metadata-8.7.0\r\n",
      "  Attempting uninstall: aiosignal\r\n",
      "    Found existing installation: aiosignal 1.3.2\r\n",
      "    Uninstalling aiosignal-1.3.2:\r\n",
      "      Successfully uninstalled aiosignal-1.3.2\r\n",
      "  Attempting uninstall: rich\r\n",
      "    Found existing installation: rich 14.0.0\r\n",
      "    Uninstalling rich-14.0.0:\r\n",
      "      Successfully uninstalled rich-14.0.0\r\n",
      "  Attempting uninstall: pandas\r\n",
      "    Found existing installation: pandas 2.3.0\r\n",
      "    Uninstalling pandas-2.3.0:\r\n",
      "      Successfully uninstalled pandas-2.3.0\r\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\r\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\r\n",
      "    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\r\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\r\n",
      "  Attempting uninstall: huggingface_hub\r\n",
      "    Found existing installation: huggingface-hub 0.32.4\r\n",
      "    Uninstalling huggingface-hub-0.32.4:\r\n",
      "      Successfully uninstalled huggingface-hub-0.32.4\r\n",
      "  Attempting uninstall: aiohttp\r\n",
      "    Found existing installation: aiohttp 3.12.11\r\n",
      "    Uninstalling aiohttp-3.12.11:\r\n",
      "      Successfully uninstalled aiohttp-3.12.11\r\n",
      "  Attempting uninstall: tyro\r\n",
      "    Found existing installation: tyro 0.9.24\r\n",
      "    Uninstalling tyro-0.9.24:\r\n",
      "      Successfully uninstalled tyro-0.9.24\r\n",
      "  Attempting uninstall: torch\r\n",
      "    Found existing installation: torch 2.7.0\r\n",
      "    Uninstalling torch-2.7.0:\r\n",
      "      Successfully uninstalled torch-2.7.0\r\n",
      "  Attempting uninstall: tokenizers\r\n",
      "    Found existing installation: tokenizers 0.21.1\r\n",
      "    Uninstalling tokenizers-0.21.1:\r\n",
      "      Successfully uninstalled tokenizers-0.21.1\r\n",
      "  Attempting uninstall: diffusers\r\n",
      "    Found existing installation: diffusers 0.33.1\r\n",
      "    Uninstalling diffusers-0.33.1:\r\n",
      "      Successfully uninstalled diffusers-0.33.1\r\n",
      "  Attempting uninstall: xformers\r\n",
      "    Found existing installation: xformers 0.0.30\r\n",
      "    Uninstalling xformers-0.0.30:\r\n",
      "      Successfully uninstalled xformers-0.0.30\r\n",
      "  Attempting uninstall: transformers\r\n",
      "    Found existing installation: transformers 4.52.4\r\n",
      "    Uninstalling transformers-4.52.4:\r\n",
      "      Successfully uninstalled transformers-4.52.4\r\n",
      "  Attempting uninstall: torchvision\r\n",
      "    Found existing installation: torchvision 0.22.0\r\n",
      "    Uninstalling torchvision-0.22.0:\r\n",
      "      Successfully uninstalled torchvision-0.22.0\r\n",
      "  Attempting uninstall: datasets\r\n",
      "    Found existing installation: datasets 3.6.0\r\n",
      "    Uninstalling datasets-3.6.0:\r\n",
      "      Successfully uninstalled datasets-3.6.0\r\n",
      "  Attempting uninstall: cut_cross_entropy\r\n",
      "    Found existing installation: cut-cross-entropy 25.1.1\r\n",
      "    Uninstalling cut-cross-entropy-25.1.1:\r\n",
      "      Successfully uninstalled cut-cross-entropy-25.1.1\r\n",
      "  Attempting uninstall: bitsandbytes\r\n",
      "    Found existing installation: bitsandbytes 0.46.0\r\n",
      "    Uninstalling bitsandbytes-0.46.0:\r\n",
      "      Successfully uninstalled bitsandbytes-0.46.0\r\n",
      "  Attempting uninstall: accelerate\r\n",
      "    Found existing installation: accelerate 1.7.0\r\n",
      "    Uninstalling accelerate-1.7.0:\r\n",
      "      Successfully uninstalled accelerate-1.7.0\r\n",
      "  Attempting uninstall: trl\r\n",
      "    Found existing installation: trl 0.18.1\r\n",
      "    Uninstalling trl-0.18.1:\r\n",
      "      Successfully uninstalled trl-0.18.1\r\n",
      "  Attempting uninstall: peft\r\n",
      "    Found existing installation: peft 0.15.2\r\n",
      "    Uninstalling peft-0.15.2:\r\n",
      "      Successfully uninstalled peft-0.15.2\r\n",
      "  Attempting uninstall: unsloth_zoo\r\n",
      "    Found existing installation: unsloth_zoo 2025.6.1\r\n",
      "    Uninstalling unsloth_zoo-2025.6.1:\r\n",
      "      Successfully uninstalled unsloth_zoo-2025.6.1\r\n",
      "  Attempting uninstall: unsloth\r\n",
      "    Found existing installation: unsloth 2025.6.1\r\n",
      "    Uninstalling unsloth-2025.6.1:\r\n",
      "      Successfully uninstalled unsloth-2025.6.1\r\n",
      "Successfully installed MarkupSafe-3.0.2 accelerate-1.7.0 aiohappyeyeballs-2.6.1 aiohttp-3.12.13 aiosignal-1.3.2 attrs-25.3.0 bitsandbytes-0.46.0 certifi-2025.6.15 charset_normalizer-3.4.2 cut_cross_entropy-25.1.1 datasets-3.6.0 diffusers-0.33.1 dill-0.3.8 docstring-parser-0.16 filelock-3.18.0 frozenlist-1.7.0 fsspec-2025.3.0 hf-xet-1.1.4 hf_transfer-0.1.9 huggingface_hub-0.33.0 idna-3.10 importlib-metadata-8.7.0 jinja2-3.1.6 markdown-it-py-3.0.0 mdurl-0.1.2 mpmath-1.3.0 msgspec-0.19.0 multidict-6.5.0 multiprocess-0.70.16 networkx-3.5 numpy-2.3.0 nvidia-cublas-cu12-12.6.4.1 nvidia-cuda-cupti-cu12-12.6.80 nvidia-cuda-nvrtc-cu12-12.6.77 nvidia-cuda-runtime-cu12-12.6.77 nvidia-cudnn-cu12-9.5.1.17 nvidia-cufft-cu12-11.3.0.4 nvidia-cufile-cu12-1.11.1.6 nvidia-curand-cu12-10.3.7.77 nvidia-cusolver-cu12-11.7.1.2 nvidia-cusparse-cu12-12.5.4.2 nvidia-cusparselt-cu12-0.6.3 nvidia-nccl-cu12-2.26.2 nvidia-nvjitlink-cu12-12.6.85 nvidia-nvtx-cu12-12.6.77 packaging-25.0 pandas-2.3.0 peft-0.15.2 pillow-11.2.1 propcache-0.3.2 protobuf-3.20.3 psutil-7.0.0 pyarrow-20.0.0 pygments-2.19.1 python-dateutil-2.9.0.post0 pytz-2025.2 pyyaml-6.0.2 regex-2024.11.6 requests-2.32.4 rich-14.0.0 safetensors-0.5.3 sentencepiece-0.2.0 setuptools-80.9.0 shtab-1.7.2 six-1.17.0 sympy-1.14.0 tokenizers-0.21.1 torch-2.7.0 torchvision-0.22.0 tqdm-4.67.1 transformers-4.52.4 triton-3.3.0 trl-0.18.2 typeguard-4.4.3 typing-extensions-4.14.0 tyro-0.9.24 tzdata-2025.2 unsloth-2025.6.2 unsloth_zoo-2025.6.1 urllib3-2.4.0 wheel-0.45.1 xformers-0.0.30 xxhash-3.5.0 yarl-1.20.1 zipp-3.23.0\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m25.0.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m25.1.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n",
      "Looking in indexes: https://download.pytorch.org/whl/cu118\r\n",
      "Collecting torch\r\n",
      "  Downloading https://download.pytorch.org/whl/cu118/torch-2.7.1%2Bcu118-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (28 kB)\r\n",
      "Collecting torchvision\r\n",
      "  Downloading https://download.pytorch.org/whl/cu118/torchvision-0.22.1%2Bcu118-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\r\n",
      "Collecting torchaudio\r\n",
      "  Downloading https://download.pytorch.org/whl/cu118/torchaudio-2.7.1%2Bcu118-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (6.6 kB)\r\n",
      "Collecting filelock (from torch)\r\n",
      "  Downloading https://download.pytorch.org/whl/filelock-3.13.1-py3-none-any.whl.metadata (2.8 kB)\r\n",
      "Collecting typing-extensions>=4.10.0 (from torch)\r\n",
      "  Downloading https://download.pytorch.org/whl/typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\r\n",
      "Collecting setuptools (from torch)\r\n",
      "  Downloading https://download.pytorch.org/whl/setuptools-70.2.0-py3-none-any.whl.metadata (5.8 kB)\r\n",
      "Collecting sympy>=1.13.3 (from torch)\r\n",
      "  Downloading https://download.pytorch.org/whl/sympy-1.13.3-py3-none-any.whl.metadata (12 kB)\r\n",
      "Collecting networkx (from torch)\r\n",
      "  Downloading https://download.pytorch.org/whl/networkx-3.3-py3-none-any.whl.metadata (5.1 kB)\r\n",
      "Collecting jinja2 (from torch)\r\n",
      "  Downloading https://download.pytorch.org/whl/Jinja2-3.1.4-py3-none-any.whl.metadata (2.6 kB)\r\n",
      "Collecting fsspec (from torch)\r\n",
      "  Downloading https://download.pytorch.org/whl/fsspec-2024.6.1-py3-none-any.whl.metadata (11 kB)\r\n",
      "Collecting nvidia-cuda-nvrtc-cu11==11.8.89 (from torch)\r\n",
      "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_nvrtc_cu11-11.8.89-py3-none-manylinux1_x86_64.whl (23.2 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m23.2/23.2 MB\u001B[0m \u001B[31m7.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m:00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hCollecting nvidia-cuda-runtime-cu11==11.8.89 (from torch)\r\n",
      "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_runtime_cu11-11.8.89-py3-none-manylinux1_x86_64.whl (875 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m875.6/875.6 kB\u001B[0m \u001B[31m1.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hCollecting nvidia-cuda-cupti-cu11==11.8.87 (from torch)\r\n",
      "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_cupti_cu11-11.8.87-py3-none-manylinux1_x86_64.whl (13.1 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m13.1/13.1 MB\u001B[0m \u001B[31m4.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hCollecting nvidia-cudnn-cu11==9.1.0.70 (from torch)\r\n",
      "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cudnn_cu11-9.1.0.70-py3-none-manylinux2014_x86_64.whl (663.9 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m663.9/663.9 MB\u001B[0m \u001B[31m7.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:02\u001B[0m\r\n",
      "\u001B[?25hCollecting nvidia-cublas-cu11==11.11.3.6 (from torch)\r\n",
      "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cublas_cu11-11.11.3.6-py3-none-manylinux1_x86_64.whl (417.9 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m417.9/417.9 MB\u001B[0m \u001B[31m9.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:02\u001B[0m\r\n",
      "\u001B[?25hCollecting nvidia-cufft-cu11==10.9.0.58 (from torch)\r\n",
      "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m168.4/168.4 MB\u001B[0m \u001B[31m9.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hCollecting nvidia-curand-cu11==10.3.0.86 (from torch)\r\n",
      "  Downloading https://download.pytorch.org/whl/cu118/nvidia_curand_cu11-10.3.0.86-py3-none-manylinux1_x86_64.whl (58.1 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m58.1/58.1 MB\u001B[0m \u001B[31m12.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hCollecting nvidia-cusolver-cu11==11.4.1.48 (from torch)\r\n",
      "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cusolver_cu11-11.4.1.48-py3-none-manylinux1_x86_64.whl (128.2 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m128.2/128.2 MB\u001B[0m \u001B[31m11.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hCollecting nvidia-cusparse-cu11==11.7.5.86 (from torch)\r\n",
      "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cusparse_cu11-11.7.5.86-py3-none-manylinux1_x86_64.whl (204.1 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m204.1/204.1 MB\u001B[0m \u001B[31m11.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hCollecting nvidia-nccl-cu11==2.21.5 (from torch)\r\n",
      "  Downloading https://download.pytorch.org/whl/cu118/nvidia_nccl_cu11-2.21.5-py3-none-manylinux2014_x86_64.whl (147.8 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m147.8/147.8 MB\u001B[0m \u001B[31m10.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hCollecting nvidia-nvtx-cu11==11.8.86 (from torch)\r\n",
      "  Downloading https://download.pytorch.org/whl/cu118/nvidia_nvtx_cu11-11.8.86-py3-none-manylinux1_x86_64.whl (99 kB)\r\n",
      "Collecting triton==3.3.1 (from torch)\r\n",
      "  Downloading https://download.pytorch.org/whl/triton-3.3.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting numpy (from torchvision)\r\n",
      "  Downloading https://download.pytorch.org/whl/numpy-2.1.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\r\n",
      "Collecting pillow!=8.3.*,>=5.3.0 (from torchvision)\r\n",
      "  Downloading https://download.pytorch.org/whl/pillow-11.0.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (9.1 kB)\r\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch)\r\n",
      "  Downloading https://download.pytorch.org/whl/mpmath-1.3.0-py3-none-any.whl (536 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m536.2/536.2 kB\u001B[0m \u001B[31m6.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hCollecting MarkupSafe>=2.0 (from jinja2->torch)\r\n",
      "  Downloading https://download.pytorch.org/whl/MarkupSafe-2.1.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (28 kB)\r\n",
      "Downloading https://download.pytorch.org/whl/cu118/torch-2.7.1%2Bcu118-cp312-cp312-manylinux_2_28_x86_64.whl (905.2 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m905.2/905.2 MB\u001B[0m \u001B[31m10.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:02\u001B[0m\r\n",
      "\u001B[?25hDownloading https://download.pytorch.org/whl/triton-3.3.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (155.7 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m155.7/155.7 MB\u001B[0m \u001B[31m10.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hDownloading https://download.pytorch.org/whl/cu118/torchvision-0.22.1%2Bcu118-cp312-cp312-manylinux_2_28_x86_64.whl (6.7 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m6.7/6.7 MB\u001B[0m \u001B[31m10.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hDownloading https://download.pytorch.org/whl/cu118/torchaudio-2.7.1%2Bcu118-cp312-cp312-manylinux_2_28_x86_64.whl (3.3 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m3.3/3.3 MB\u001B[0m \u001B[31m10.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hDownloading https://download.pytorch.org/whl/pillow-11.0.0-cp312-cp312-manylinux_2_28_x86_64.whl (4.4 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m4.4/4.4 MB\u001B[0m \u001B[31m10.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hDownloading https://download.pytorch.org/whl/setuptools-70.2.0-py3-none-any.whl (930 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m930.8/930.8 kB\u001B[0m \u001B[31m9.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hDownloading https://download.pytorch.org/whl/sympy-1.13.3-py3-none-any.whl (6.2 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m6.2/6.2 MB\u001B[0m \u001B[31m13.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hDownloading https://download.pytorch.org/whl/typing_extensions-4.12.2-py3-none-any.whl (37 kB)\r\n",
      "Downloading https://download.pytorch.org/whl/filelock-3.13.1-py3-none-any.whl (11 kB)\r\n",
      "Downloading https://download.pytorch.org/whl/fsspec-2024.6.1-py3-none-any.whl (177 kB)\r\n",
      "Downloading https://download.pytorch.org/whl/Jinja2-3.1.4-py3-none-any.whl (133 kB)\r\n",
      "Downloading https://download.pytorch.org/whl/networkx-3.3-py3-none-any.whl (1.7 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.7/1.7 MB\u001B[0m \u001B[31m15.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hDownloading https://download.pytorch.org/whl/numpy-2.1.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.0 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m16.0/16.0 MB\u001B[0m \u001B[31m16.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hInstalling collected packages: mpmath, typing-extensions, sympy, setuptools, pillow, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, numpy, networkx, MarkupSafe, fsspec, filelock, triton, nvidia-cusolver-cu11, nvidia-cudnn-cu11, jinja2, torch, torchvision, torchaudio\r\n",
      "  Attempting uninstall: mpmath\r\n",
      "    Found existing installation: mpmath 1.3.0\r\n",
      "    Uninstalling mpmath-1.3.0:\r\n",
      "      Successfully uninstalled mpmath-1.3.0\r\n",
      "  Attempting uninstall: typing-extensions\r\n",
      "    Found existing installation: typing_extensions 4.14.0\r\n",
      "    Uninstalling typing_extensions-4.14.0:\r\n",
      "      Successfully uninstalled typing_extensions-4.14.0\r\n",
      "  Attempting uninstall: sympy\r\n",
      "    Found existing installation: sympy 1.14.0\r\n",
      "    Uninstalling sympy-1.14.0:\r\n",
      "      Successfully uninstalled sympy-1.14.0\r\n",
      "  Attempting uninstall: setuptools\r\n",
      "    Found existing installation: setuptools 80.9.0\r\n",
      "    Uninstalling setuptools-80.9.0:\r\n",
      "      Successfully uninstalled setuptools-80.9.0\r\n",
      "  Attempting uninstall: pillow\r\n",
      "    Found existing installation: pillow 11.2.1\r\n",
      "    Uninstalling pillow-11.2.1:\r\n",
      "      Successfully uninstalled pillow-11.2.1\r\n",
      "  Attempting uninstall: numpy\r\n",
      "    Found existing installation: numpy 2.3.0\r\n",
      "    Uninstalling numpy-2.3.0:\r\n",
      "      Successfully uninstalled numpy-2.3.0\r\n",
      "  Attempting uninstall: networkx\r\n",
      "    Found existing installation: networkx 3.5\r\n",
      "    Uninstalling networkx-3.5:\r\n",
      "      Successfully uninstalled networkx-3.5\r\n",
      "  Attempting uninstall: MarkupSafe\r\n",
      "    Found existing installation: MarkupSafe 3.0.2\r\n",
      "    Uninstalling MarkupSafe-3.0.2:\r\n",
      "      Successfully uninstalled MarkupSafe-3.0.2\r\n",
      "  Attempting uninstall: fsspec\r\n",
      "    Found existing installation: fsspec 2025.3.0\r\n",
      "    Uninstalling fsspec-2025.3.0:\r\n",
      "      Successfully uninstalled fsspec-2025.3.0\r\n",
      "  Attempting uninstall: filelock\r\n",
      "    Found existing installation: filelock 3.18.0\r\n",
      "    Uninstalling filelock-3.18.0:\r\n",
      "      Successfully uninstalled filelock-3.18.0\r\n",
      "  Attempting uninstall: triton\r\n",
      "    Found existing installation: triton 3.3.0\r\n",
      "    Uninstalling triton-3.3.0:\r\n",
      "      Successfully uninstalled triton-3.3.0\r\n",
      "  Attempting uninstall: jinja2\r\n",
      "    Found existing installation: Jinja2 3.1.6\r\n",
      "    Uninstalling Jinja2-3.1.6:\r\n",
      "      Successfully uninstalled Jinja2-3.1.6\r\n",
      "  Attempting uninstall: torch\r\n",
      "    Found existing installation: torch 2.7.0\r\n",
      "    Uninstalling torch-2.7.0:\r\n",
      "      Successfully uninstalled torch-2.7.0\r\n",
      "  Attempting uninstall: torchvision\r\n",
      "    Found existing installation: torchvision 0.22.0\r\n",
      "    Uninstalling torchvision-0.22.0:\r\n",
      "      Successfully uninstalled torchvision-0.22.0\r\n",
      "\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "unsloth-zoo 2025.6.1 requires torch<=2.7.0, but you have torch 2.7.1+cu118 which is incompatible.\r\n",
      "typeguard 4.4.3 requires typing_extensions>=4.14.0, but you have typing-extensions 4.12.2 which is incompatible.\r\n",
      "xformers 0.0.30 requires torch==2.7.0, but you have torch 2.7.1+cu118 which is incompatible.\r\n",
      "tyro 0.9.24 requires typing-extensions>=4.13.0, but you have typing-extensions 4.12.2 which is incompatible.\r\n",
      "unsloth 2025.6.2 requires torch<=2.7.0,>=2.4.0, but you have torch 2.7.1+cu118 which is incompatible.\u001B[0m\u001B[31m\r\n",
      "\u001B[0mSuccessfully installed MarkupSafe-2.1.5 filelock-3.13.1 fsspec-2024.6.1 jinja2-3.1.4 mpmath-1.3.0 networkx-3.3 numpy-2.1.2 nvidia-cublas-cu11-11.11.3.6 nvidia-cuda-cupti-cu11-11.8.87 nvidia-cuda-nvrtc-cu11-11.8.89 nvidia-cuda-runtime-cu11-11.8.89 nvidia-cudnn-cu11-9.1.0.70 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.3.0.86 nvidia-cusolver-cu11-11.4.1.48 nvidia-cusparse-cu11-11.7.5.86 nvidia-nccl-cu11-2.21.5 nvidia-nvtx-cu11-11.8.86 pillow-11.0.0 setuptools-70.2.0 sympy-1.13.3 torch-2.7.1+cu118 torchaudio-2.7.1+cu118 torchvision-0.22.1+cu118 triton-3.3.1 typing-extensions-4.12.2\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m25.0.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m25.1.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "import unsloth\n",
   "id": "8f31c95c6739d77a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T08:45:31.501808Z",
     "start_time": "2025-06-18T08:45:31.491483Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# import unsloth\n",
    "\n",
    "import torch\n",
    "\n",
    "print(f\"Версия PyTorch: {torch.__version__}\")\n",
    "print(f\"Доступна ли CUDA: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Версия CUDA для PyTorch: {torch.version.cuda}\")\n",
    "    print(f\"Имя GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\">>> CUDA недоступна. Установлена CPU-версия PyTorch или есть проблема с совместимостью.\")"
   ],
   "id": "5eb20d35ab155ba9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Версия PyTorch: 2.7.0+cu126\n",
      "Доступна ли CUDA: False\n",
      ">>> CUDA недоступна. Установлена CPU-версия PyTorch или есть проблема с совместимостью.\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T08:12:33.483717Z",
     "start_time": "2025-06-18T08:12:31.448903Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "import unsloth\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "from typing import Optional, List, Dict, Any\n",
    "\n",
    "from transformers import AutoConfig, TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. DEFINITION OF THE CUSTOM MODEL WRAPPER\n",
    "# ==============================================================================\n",
    "class EmotionUnslothModel(nn.Module):\n",
    "    \"\"\"\n",
    "    An unsloth-optimized wrapper that includes a trainable vector projector.\n",
    "    This class takes a raw, fixed-size emotion vector, projects it to the\n",
    "    model's hidden dimension, and then injects it into the forward pass.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name_or_path: str,\n",
    "        raw_emotion_vector_size: int,\n",
    "        lora_rank: int = 16,\n",
    "        lora_alpha: int = 16,\n",
    "        use_4bit: bool = True,\n",
    "        max_seq_length: int = 2048,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes the EmotionUnslothModel with a vector projector.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Load unsloth model and tokenizer\n",
    "        self.model, self.tokenizer = FastLanguageModel.from_pretrained(\n",
    "            model_name=model_name_or_path,\n",
    "            max_seq_length=max_seq_length,\n",
    "            load_in_4bit=use_4bit,\n",
    "            cache_dir=\"./model_cache\",\n",
    "        )\n",
    "        model_hidden_size = self.model.config.hidden_size\n",
    "\n",
    "        # Define the vector projector\n",
    "        self.vector_projector = nn.Linear(\n",
    "            in_features=raw_emotion_vector_size,\n",
    "            out_features=model_hidden_size,\n",
    "            bias=False\n",
    "        )\n",
    "        self.vector_projector.to(\"cuda\",self.model.dtype)\n",
    "\n",
    "        # Apply LoRA using unsloth's function\n",
    "        self.peft_model = FastLanguageModel.get_peft_model(\n",
    "            self.model,\n",
    "            r=lora_rank,\n",
    "            lora_alpha=lora_alpha,\n",
    "            lora_dropout=0,\n",
    "            bias=\"none\",\n",
    "            use_gradient_checkpointing=True,\n",
    "            random_state=42,\n",
    "            target_modules=[\n",
    "                \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                \"gate_proj\", \"up_proj\", \"down_proj\"\n",
    "            ],\n",
    "            # modules_to_save=[\"vector_projector\"], # <--- УДАЛИТЕ ЭТУ СТРОКУ\n",
    "        )\n",
    "\n",
    "        # --- ДОБАВЬТЕ ЭТОТ БЛОК ---\n",
    "        # Manually unfreeze the projector weights after creating the PEFT model.\n",
    "        # This makes them trainable without using the restricted 'modules_to_save'.\n",
    "        for param in self.vector_projector.parameters():\n",
    "            param.requires_grad = True\n",
    "        # ---------------------------\n",
    "\n",
    "        self.peft_model.print_trainable_parameters()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor,\n",
    "        attention_mask: torch.Tensor,\n",
    "        emotion_vector: torch.Tensor,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        **kwargs,\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Performs the forward pass with projection and injection.\n",
    "        \"\"\"\n",
    "        projected_vector = self.vector_projector(emotion_vector)\n",
    "        embedding_layer = self.peft_model.get_input_embeddings()\n",
    "        token_embeddings = embedding_layer(input_ids)\n",
    "        combined_embeddings = token_embeddings + projected_vector.unsqueeze(1)\n",
    "\n",
    "        model_outputs = self.peft_model(\n",
    "            inputs_embeds=combined_embeddings,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels,\n",
    "            return_dict=True\n",
    "        )\n",
    "        return model_outputs\n",
    "\n",
    "    def generate(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor,\n",
    "        attention_mask: torch.Tensor,\n",
    "        emotion_vector: torch.Tensor,\n",
    "        **generation_kwargs: Any\n",
    "    ) -> List[int]:\n",
    "        \"\"\"\n",
    "        Generates text conditioned on a prompt and an emotion vector.\n",
    "        \"\"\"\n",
    "        if \"pad_token_id\" not in generation_kwargs:\n",
    "            generation_kwargs[\"pad_token_id\"] = self.tokenizer.eos_token_id\n",
    "\n",
    "        return self.peft_model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            emotion_vector=emotion_vector,\n",
    "            **generation_kwargs\n",
    "        )\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. DEFINITION OF THE CUSTOM DATA COLLATOR\n",
    "# ==============================================================================\n",
    "\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "class DataCollatorForEmotionLM(DataCollatorForLanguageModeling):\n",
    "    \"\"\"\n",
    "    Custom data collator that handles tokenizing text and stacking emotion vectors.\n",
    "    \"\"\"\n",
    "    def __call__(\n",
    "        self,\n",
    "        features: List[Dict[str, Any]]\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Processes a list of features to create a batch.\n",
    "        \"\"\"\n",
    "        emotion_vectors = [feature.pop(\"emotion_vector\") for feature in features]\n",
    "        batch = super().__call__(features)\n",
    "        batch['emotion_vector'] = torch.stack(emotion_vectors)\n",
    "        return batch\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. MAIN TRAINING SCRIPT\n",
    "# ==============================================================================\n",
    "\n",
    "def main():\n",
    "    # --- Configuration ---\n",
    "    MODEL_NAME = \"unsloth/Qwen3-0.6B-unsloth-bnb-4bit\"\n",
    "    MAX_SEQ_LENGTH = 2048\n",
    "    RAW_EMOTION_VECTOR_SIZE = 12\n",
    "\n",
    "    # --- Model Initialization ---\n",
    "    print(\"Initializing the model...\")\n",
    "    emotion_model_wrapper = EmotionUnslothModel(\n",
    "        model_name_or_path=MODEL_NAME,\n",
    "        raw_emotion_vector_size=RAW_EMOTION_VECTOR_SIZE,\n",
    "        max_seq_length=MAX_SEQ_LENGTH\n",
    "    )\n",
    "    model_dtype = emotion_model_wrapper.model.dtype\n",
    "\n",
    "    # --- Data Preparation ---\n",
    "    print(\"Preparing the dataset...\")\n",
    "    # For Llama-3 instruct model, we should use its specific chat template\n",
    "    prompt_template = \"\"\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "{}<|eot_id|>\"\"\"\n",
    "\n",
    "    raw_data = [\n",
    "        {\"text\": prompt_template.format(\"Write a happy poem about spring.\", \"The sunbeams dance, a joyful sight,\\nNew flowers bloom in colors bright.\"), \"emotion\": \"happy\"},\n",
    "        {\"text\": prompt_template.format(\"Describe a spooky, abandoned mansion.\", \"The old manor stood in chilling dread,\\nWhere silent ghosts and shadows tread.\"), \"emotion\": \"spooky\"},\n",
    "        {\"text\": prompt_template.format(\"Compose a short, joyful song about a river.\", \"The river flows, a happy tune,\\nBeneath the sunny afternoon.\"), \"emotion\": \"happy\"},\n",
    "        {\"text\": prompt_template.format(\"Tell a short, eerie tale about a forest at night.\", \"Deep in the woods, when moonlight fails,\\nA whisper rides on chilling gales.\"), \"emotion\": \"spooky\"},\n",
    "    ]\n",
    "    dataset = Dataset.from_list(raw_data)\n",
    "\n",
    "    # Create and map emotion vectors to the dataset\n",
    "    emotion_mapping = {\n",
    "        \"happy\": torch.from_numpy(np.random.rand(RAW_EMOTION_VECTOR_SIZE) * 0.1),\n",
    "        \"spooky\": torch.from_numpy(np.random.rand(RAW_EMOTION_VECTOR_SIZE) * -0.1)\n",
    "    }\n",
    "\n",
    "    # CORRECTED: Define the function inside main() to access model_dtype\n",
    "    def add_emotion_vector(example: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Adds the emotion vector to a dataset example with the correct dtype on CPU.\"\"\"\n",
    "        em_vector = emotion_mapping[example[\"emotion\"]]\n",
    "        # The tensor should be created with the model's dtype, but remain on the CPU.\n",
    "        example[\"emotion_vector\"] = em_vector.to(dtype=model_dtype)\n",
    "        return example\n",
    "\n",
    "    # CORRECTED: Call .map() with only the function argument\n",
    "    dataset = dataset.map(add_emotion_vector)\n",
    "\n",
    "    # --- Pre-processing and Tokenization ---\n",
    "    def preprocess_function(example: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Tokenizes the text and prepares labels.\"\"\"\n",
    "        tokenized_example = emotion_model_wrapper.tokenizer(\n",
    "            example[\"text\"],\n",
    "            truncation=True,\n",
    "            max_length=MAX_SEQ_LENGTH,\n",
    "            padding=False,\n",
    "            return_tensors=None,\n",
    "        )\n",
    "        tokenized_example[\"labels\"] = tokenized_example[\"input_ids\"][:]\n",
    "        return tokenized_example\n",
    "\n",
    "    tokenized_dataset = dataset.map(\n",
    "        preprocess_function,\n",
    "        batched=True,\n",
    "        remove_columns=dataset.column_names\n",
    "    )\n",
    "\n",
    "    # --- Trainer Setup ---\n",
    "    print(\"Setting up the trainer...\")\n",
    "    data_collator = DataCollatorForEmotionLM(\n",
    "        tokenizer=emotion_model_wrapper.tokenizer,\n",
    "        mlm=False\n",
    "    )\n",
    "\n",
    "    trainer = SFTTrainer(\n",
    "        model=emotion_model_wrapper.peft_model,\n",
    "        tokenizer=emotion_model_wrapper.tokenizer,\n",
    "        train_dataset=tokenized_dataset,\n",
    "        data_collator=data_collator,\n",
    "        max_seq_length=MAX_SEQ_LENGTH,\n",
    "        args=TrainingArguments(\n",
    "            per_device_train_batch_size=2,\n",
    "            gradient_accumulation_steps=4,\n",
    "            warmup_steps=5,\n",
    "            max_steps=50,  # Increase for real training\n",
    "            learning_rate=2e-4,\n",
    "            fp16=not torch.cuda.is_bf16_supported(),\n",
    "            bf16=torch.cuda.is_bf16_supported(),\n",
    "            logging_steps=1,\n",
    "            optim=\"adamw_8bit\",\n",
    "            weight_decay=0.01,\n",
    "            lr_scheduler_type=\"linear\",\n",
    "            seed=42,\n",
    "            output_dir=\"outputs\",\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # --- Start Training ---\n",
    "    print(\"Starting training...\")\n",
    "    trainer.train()\n",
    "    print(\"Training finished!\")\n",
    "\n",
    "      # --- Inference Example ---\n",
    "\n",
    "    print(\"\\n--- Пример генерации текста (инференс) ---\")\n",
    "\n",
    "    inference_prompt = \"Tell me about a sunny day.\"\n",
    "    inference_emotion = \"happy\"\n",
    "\n",
    "\n",
    "    formatted_prompt = prompt_template.format(inference_prompt, \"\")\n",
    "    inputs = emotion_model_wrapper.tokenizer(formatted_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    # Manually prepare `inputs_embeds` for inference\n",
    "    # 1. Get the emotion vector and project it\n",
    "    emotion_vector_tensor = emotion_mapping[inference_emotion].unsqueeze(0).to(\"cuda\",dtype=model_dtype)\n",
    "    projected_vector = emotion_model_wrapper.vector_projector(emotion_vector_tensor)\n",
    "\n",
    "    # 2. Get the token embeddings from the input_ids\n",
    "    embedding_layer = emotion_model_wrapper.peft_model.get_input_embeddings()\n",
    "    token_embeddings = embedding_layer(inputs.input_ids)\n",
    "\n",
    "    # 3. Combine them to create final inputs_embeds\n",
    "    combined_embeddings = token_embeddings + projected_vector.unsqueeze(1)\n",
    "\n",
    "    # Call generate with `inputs_embeds` instead of `input_ids`.\n",
    "    generated_ids = emotion_model_wrapper.peft_model.generate(\n",
    "        input_ids=inputs.input_ids, # <--- ВОЗВРАЩАЕМ ЭТОТ АРГУМЕНТ\n",
    "        inputs_embeds=combined_embeddings,\n",
    "        attention_mask=inputs.attention_mask,\n",
    "        max_new_tokens=50,\n",
    "        use_cache=True,\n",
    "        do_sample=True,\n",
    "        top_p=0.9,\n",
    "        temperature=0.7,\n",
    "        pad_token_id=emotion_model_wrapper.tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "    generated_text = emotion_model_wrapper.tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "    print(\"\\n--- Сгенерированный текст ---\")\n",
    "    print(generated_text)\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "1d69364d2ad355a6",
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Unsloth currently only works on NVIDIA GPUs and Intel GPUs.",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNotImplementedError\u001B[39m                       Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[16]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01munsloth\u001B[39;00m\n\u001B[32m      2\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01munsloth\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m FastLanguageModel\n\u001B[32m      4\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtorch\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Feelent/.venv/lib/python3.12/site-packages/unsloth/__init__.py:88\u001B[39m\n\u001B[32m     86\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mNotImplementedError\u001B[39;00m(\u001B[33m\"\u001B[39m\u001B[33mUnsloth currently only works on NVIDIA GPUs and Intel GPUs.\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m     87\u001B[39m \u001B[38;5;28;01mpass\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m88\u001B[39m DEVICE_TYPE : \u001B[38;5;28mstr\u001B[39m = \u001B[43mget_device_type\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     90\u001B[39m \u001B[38;5;66;03m# Reduce VRAM usage by reducing fragmentation\u001B[39;00m\n\u001B[32m     91\u001B[39m \u001B[38;5;66;03m# And optimize pinning of memory\u001B[39;00m\n\u001B[32m     92\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m DEVICE_TYPE == \u001B[33m\"\u001B[39m\u001B[33mcuda\u001B[39m\u001B[33m\"\u001B[39m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Feelent/.venv/lib/python3.12/site-packages/unsloth/__init__.py:86\u001B[39m, in \u001B[36mget_device_type\u001B[39m\u001B[34m()\u001B[39m\n\u001B[32m     84\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(torch, \u001B[33m\"\u001B[39m\u001B[33mxpu\u001B[39m\u001B[33m\"\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m torch.xpu.is_available():\n\u001B[32m     85\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[33m\"\u001B[39m\u001B[33mxpu\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m---> \u001B[39m\u001B[32m86\u001B[39m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mNotImplementedError\u001B[39;00m(\u001B[33m\"\u001B[39m\u001B[33mUnsloth currently only works on NVIDIA GPUs and Intel GPUs.\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[31mNotImplementedError\u001B[39m: Unsloth currently only works on NVIDIA GPUs and Intel GPUs."
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "88d95436dc834e08"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T08:12:00.830795Z",
     "start_time": "2025-06-18T08:12:00.801434Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import datasets\n",
    "\n",
    "ASSISTANT_TEMPLATE = \"\"\"<|im_start|>user\\n{user_prompt}<|im_end|><|im_start|>assistant\\n<think>{thinking}</think>{assistant_answer}<|im_end|>\\n\"\"\"\n",
    "ASSISTANT_TEMPLATE = \"\"\"\\n<think>{thinking}</think>{assistant_answer}\\n\"\"\"\n",
    "\n",
    "def generate_conversation(examples):\n",
    "    user_message = examples[\"prompt\"]\n",
    "    thinking = examples[\"thinking\"]\n",
    "    response = examples[\"response\"]\n",
    "\n",
    "    conversations = []\n",
    "    for index in range(len(user_message)):\n",
    "        thinking_text = \"\\n\".join(eval(thinking[index]))\n",
    "        assistant_answer = ASSISTANT_TEMPLATE.format(\n",
    "            user_prompt=user_message[index],\n",
    "            thinking=thinking_text,\n",
    "            assistant_answer=response[index]\n",
    "        )\n",
    "        conversations.append([\n",
    "            {\"role\" : \"user\",      \"content\" : user_message[index]},\n",
    "            {\"role\" : \"assistant\", \"content\" : assistant_answer},\n",
    "        ])\n",
    "    return { \"conversations\": conversations, }\n",
    "\n",
    "\n",
    "one_emotion_dataset  = pd.read_csv(\"sft_one_emotion_thinking.csv\")\n",
    "one_emotion_dataset = datasets.Dataset.from_pandas(one_emotion_dataset)\n",
    "one_emotion_dataset = one_emotion_dataset.map(\n",
    "    generate_conversation,\n",
    "    remove_columns=one_emotion_dataset.column_names,\n",
    "    batched=True,\n",
    "    batch_size=1000,\n",
    ")"
   ],
   "id": "a77570839bfa90c0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/600 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "eb369b7c29b54fb99db5bd720aa29e46"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T08:11:31.420604Z",
     "start_time": "2025-06-18T08:11:31.416305Z"
    }
   },
   "cell_type": "code",
   "source": "one_emotion_dataset",
   "id": "cb4e062a542f9c08",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'conversations': [{'content': 'How are you feeling at this moment?',\n",
       "   'role': 'user'},\n",
       "  {'content': \"\\n<think>They're asking how I am.\\nI feel this incredible buzz, a thrumming energy deep inside me.\\nIt's like waiting for the curtain to rise on the best show ever.\\nMy mind is racing with all the wonderful possibilities. It's so close!\\nI should convey this excitement, this feeling that something great is just around the corner.</think>I'm on the edge of my seat! Something amazing is about to happen, I can feel it.\\n\",\n",
       "   'role': 'assistant'}]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
