{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "!pip install --force-reinstall -U ipywidgets\n",
    "!pip install --force-reinstall unsloth\n",
    "\n",
    "!pip3 install --force-reinstall torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ],
   "id": "77499415f6338f7d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "import unsloth\n",
   "id": "8f31c95c6739d77a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# import unsloth\n",
    "\n",
    "import torch\n",
    "\n",
    "print(f\"Версия PyTorch: {torch.__version__}\")\n",
    "print(f\"Доступна ли CUDA: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Версия CUDA для PyTorch: {torch.version.cuda}\")\n",
    "    print(f\"Имя GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\">>> CUDA недоступна. Установлена CPU-версия PyTorch или есть проблема с совместимостью.\")"
   ],
   "id": "5eb20d35ab155ba9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T17:58:32.502853Z",
     "start_time": "2025-06-19T17:58:32.494430Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ast\n",
    "from datasets import Dataset\n",
    "from typing import Optional, List, Dict, Any\n",
    "import os\n",
    "import unsloth\n",
    "from unsloth import FastLanguageModel\n",
    "from transformers import AutoConfig, TrainingArguments, DataCollatorForLanguageModeling\n",
    "from trl import SFTTrainer\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. DEFINITION OF THE CUSTOM MODEL WRAPPER\n",
    "# (This class remains unchanged)\n",
    "# ==============================================================================\n",
    "class EmotionUnslothModel(nn.Module):\n",
    "    \"\"\"\n",
    "    An unsloth-optimized wrapper that includes a trainable vector projector.\n",
    "    This class takes a raw, fixed-size emotion vector, projects it to the\n",
    "    model's hidden dimension, and then injects it into the forward pass.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name_or_path: str,\n",
    "        raw_emotion_vector_size: int,\n",
    "        lora_rank: int = 16,\n",
    "        lora_alpha: int = 16,\n",
    "        use_4bit: bool = True,\n",
    "        max_seq_length: int = 2048,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes the EmotionUnslothModel with a vector projector.\n",
    "\n",
    "        Args:\n",
    "            model_name_or_path (str): The name or path of the base model.\n",
    "            raw_emotion_vector_size (int): The dimension of the input emotion vector.\n",
    "            lora_rank (int): The rank for LoRA decomposition.\n",
    "            lora_alpha (int): The alpha parameter for LoRA.\n",
    "            use_4bit (bool): Whether to load the model in 4-bit.\n",
    "            max_seq_length (int): The maximum sequence length for the model.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.model, self.tokenizer = FastLanguageModel.from_pretrained(\n",
    "            model_name=model_name_or_path,\n",
    "            max_seq_length=max_seq_length,\n",
    "            load_in_4bit=use_4bit,\n",
    "            cache_dir=\"./model_cache\",\n",
    "        )\n",
    "        model_hidden_size = self.model.config.hidden_size\n",
    "        self.vector_projector = nn.Linear(\n",
    "            in_features=raw_emotion_vector_size,\n",
    "            out_features=model_hidden_size,\n",
    "            bias=False\n",
    "        )\n",
    "        self.vector_projector.to(\"cuda\", self.model.dtype)\n",
    "        self.peft_model = FastLanguageModel.get_peft_model(\n",
    "            self.model,\n",
    "            r=lora_rank,\n",
    "            lora_alpha=lora_alpha,\n",
    "            lora_dropout=0,\n",
    "            bias=\"none\",\n",
    "            use_gradient_checkpointing=True,\n",
    "            random_state=42,\n",
    "            target_modules=[\n",
    "                \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                \"gate_proj\", \"up_proj\", \"down_proj\"\n",
    "            ],\n",
    "        )\n",
    "        for param in self.vector_projector.parameters():\n",
    "            param.requires_grad = True\n",
    "        self.peft_model.print_trainable_parameters()\n",
    "\n",
    "    def save_pretrained(\n",
    "        self,\n",
    "        save_directory: str\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Saves the PEFT model adapters and the custom vector projector's state.\n",
    "\n",
    "        Args:\n",
    "            save_directory (str): The directory where the model components will be saved.\n",
    "        \"\"\"\n",
    "        # Create directory if it doesn't exist\n",
    "        os.makedirs(save_directory, exist_ok=True)\n",
    "\n",
    "        # 1. Save PEFT adapters using the standard method\n",
    "        self.peft_model.save_pretrained(save_directory)\n",
    "        print(f\"PEFT adapters saved to {save_directory}\")\n",
    "\n",
    "        # 2. Save the tokenizer\n",
    "        self.tokenizer.save_pretrained(save_directory)\n",
    "        print(f\"Tokenizer saved to {save_directory}\")\n",
    "\n",
    "        # 3. Save the custom vector projector's state dictionary\n",
    "        projector_path = os.path.join(save_directory, \"vector_projector.pth\")\n",
    "        torch.save(self.vector_projector.state_dict(), projector_path)\n",
    "        print(f\"Vector projector saved to {projector_path}\")\n",
    "\n",
    "    def load_pretrained(\n",
    "        self,\n",
    "        load_directory: str\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Loads the PEFT model adapters and the custom vector projector's state.\n",
    "        This method should be called on an already initialized model object.\n",
    "\n",
    "        Args:\n",
    "            load_directory (str): The directory from which to load the model components.\n",
    "        \"\"\"\n",
    "        # 1. Load the custom vector projector's state dictionary\n",
    "        projector_path = os.path.join(load_directory, \"vector_projector.pth\")\n",
    "        if not os.path.exists(projector_path):\n",
    "            raise FileNotFoundError(f\"Vector projector state file not found at {projector_path}\")\n",
    "\n",
    "        # Load state dict, ensuring it's on the correct device\n",
    "        projector_state_dict = torch.load(projector_path, map_location=self.model.device)\n",
    "        self.vector_projector.load_state_dict(projector_state_dict)\n",
    "        self.vector_projector.to(self.model.device, dtype=self.model.dtype)\n",
    "        print(f\"Vector projector loaded from {projector_path}\")\n",
    "\n",
    "        # 2. Load PEFT adapters into the existing PEFT model\n",
    "        # The `load_adapter` method is part of the PEFT library and works with the unsloth model\n",
    "        self.peft_model.load_adapter(load_directory)\n",
    "        print(f\"PEFT adapters loaded from {load_directory}\")\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor,\n",
    "        attention_mask: torch.Tensor,\n",
    "        emotion_vector: torch.Tensor,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        **kwargs,\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Performs the forward pass with projection and injection.\n",
    "        \"\"\"\n",
    "        projected_vector = self.vector_projector(emotion_vector)\n",
    "        embedding_layer = self.peft_model.get_input_embeddings()\n",
    "        token_embeddings = embedding_layer(input_ids)\n",
    "        projected_vector = projected_vector.unsqueeze(1)\n",
    "\n",
    "        combined_embeddings = torch.cat(\n",
    "            [projected_vector, token_embeddings],\n",
    "            dim=1\n",
    "        )\n",
    "\n",
    "        control_token_attention_mask = torch.ones(\n",
    "            (1, 1), # Shape: [batch_size, 1] for our single new token\n",
    "            dtype=torch.long,\n",
    "            device=inputs.attention_mask.device\n",
    "        )\n",
    "\n",
    "        attention_mask = torch.cat(\n",
    "            [control_token_attention_mask, inputs.attention_mask],\n",
    "            dim=1\n",
    "        )\n",
    "\n",
    "        model_outputs = self.peft_model(\n",
    "            inputs_embeds=combined_embeddings,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels,\n",
    "            return_dict=True\n",
    "        )\n",
    "        return model_outputs\n",
    "\n"
   ],
   "id": "1d69364d2ad355a6",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "from feelings import EmotionalState, DEFAULT_RELATIONSHIP_MAP",
   "id": "f1351db6de1bb93",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T17:58:45.233317Z",
     "start_time": "2025-06-19T17:58:45.226894Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ==============================================================================\n",
    "# 2. DEFINITION OF THE CUSTOM DATA COLLATOR\n",
    "# (This class remains unchanged)\n",
    "# ==============================================================================\n",
    "from feelings import EmotionalState, DEFAULT_RELATIONSHIP_MAP\n",
    "\n",
    "class DataCollatorForEmotionLM(DataCollatorForLanguageModeling):\n",
    "    \"\"\"\n",
    "    Custom data collator that handles tokenizing text and stacking emotion vectors.\n",
    "    \"\"\"\n",
    "    neutral_vector = torch.tensor(\n",
    "        EmotionalState(DEFAULT_RELATIONSHIP_MAP).to_vector(),\n",
    "        dtype=torch.float32\n",
    "    )\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        features: List[Dict[str, Any]]\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Processes a list of features to create a batch.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            emotion_vectors = [feature.pop(\"emotion_vector\") for feature in features]\n",
    "            batch = super().__call__(features)\n",
    "            batch['emotion_vector'] = torch.stack(emotion_vectors)\n",
    "        except KeyError as e:\n",
    "            # If emotion_vector is missing, use the neutral vector\n",
    "            batch = super().__call__(features)\n",
    "            batch['emotion_vector'] = self.neutral_vector.repeat(len(features), 1)\n",
    "        return batch\n",
    "\n",
    "\n",
    "def process_and_tokenize_example(\n",
    "    example: Dict[str, Any]\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Processes a single dataset example by formatting text, parsing the\n",
    "    emotion vector, and tokenizing the text.\n",
    "    \"\"\"\n",
    "    thinking = \"\\n\".join(ast.literal_eval(example[\"thinking\"]))\n",
    "    assistant_output = f\"<thinking>{thinking}</thinking>\\n{example['response']}\"\n",
    "    formatted_text = prompt_template.format(example[\"prompt\"], assistant_output)\n",
    "    vector_as_list = ast.literal_eval(example[\"feelings_vector\"])\n",
    "    emotion_vector = torch.tensor(vector_as_list, dtype=model_dtype)\n",
    "    tokenized_example = tokenizer(\n",
    "        formatted_text,\n",
    "        truncation=True,\n",
    "        max_length=MAX_SEQ_LENGTH,\n",
    "        padding=False,\n",
    "        return_tensors=None,\n",
    "    )\n",
    "    tokenized_example[\"labels\"] = tokenized_example[\"input_ids\"][:]\n",
    "    tokenized_example[\"emotion_vector\"] = emotion_vector\n",
    "    return tokenized_example"
   ],
   "id": "25f1ae0cf4752392",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T17:58:58.685845Z",
     "start_time": "2025-06-19T17:58:48.298501Z"
    }
   },
   "cell_type": "code",
   "source": [
    "MODEL_NAME = \"unsloth/Qwen3-0.6B-unsloth-bnb-4bit\"\n",
    "MAX_SEQ_LENGTH = 2048\n",
    "RAW_EMOTION_VECTOR_SIZE = 16\n",
    "SAVE_PATH = \"./my_emotion_model_checkpoint\"\n",
    "\n",
    "# --- Model Initialization ---\n",
    "print(\"Initializing the model...\")\n",
    "emotion_model_wrapper = EmotionUnslothModel(\n",
    "    model_name_or_path=MODEL_NAME,\n",
    "    raw_emotion_vector_size=RAW_EMOTION_VECTOR_SIZE,\n",
    "    max_seq_length=MAX_SEQ_LENGTH\n",
    ")\n",
    "model_dtype = emotion_model_wrapper.model.dtype\n",
    "tokenizer = emotion_model_wrapper.tokenizer"
   ],
   "id": "befcfdc5dbb79660",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing the model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\my_projects\\llm_vision\\.venv\\Lib\\site-packages\\unsloth_zoo\\gradient_checkpointing.py:330: UserWarning: expandable_segments not supported on this platform (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\c10/cuda/CUDAAllocatorConfig.h:28.)\n",
      "  GPU_BUFFERS = tuple([torch.empty(2*256*2048, dtype = dtype, device = f\"cuda:{i}\") for i in range(n_gpus)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.3.19: Fast Qwen3 patching. Transformers: 4.51.3.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 3090. Num GPUs = 1. Max memory: 24.0 GB. Platform: Windows.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu118. CUDA: 8.6. CUDA Toolkit: 11.8. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = None. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: Making `model.base_model.model.model` require gradients\n",
      "trainable params: 10,092,544 || all params: 606,142,464 || trainable%: 1.6650\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Step 1: SFT with  standard (non-emotions) dataset and neutral feeling_vector",
   "id": "a6d9a1c00bf5c04a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T16:54:14.250418Z",
     "start_time": "2025-06-19T16:54:07.343659Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "reasoning_dataset = load_dataset(\"unsloth/OpenMathReasoning-mini\", split = \"cot\")\n",
    "non_reasoning_dataset = load_dataset(\"mlabonne/FineTome-100k\", split = \"train\")"
   ],
   "id": "8f68655fdbd5bce9",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T17:59:20.359606Z",
     "start_time": "2025-06-19T17:59:20.255410Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from unsloth.chat_templates import standardize_sharegpt\n",
    "\n",
    "\n",
    "def generate_conversation(examples):\n",
    "    problems  = examples[\"problem\"]\n",
    "    solutions = examples[\"generated_solution\"]\n",
    "    conversations = []\n",
    "    for problem, solution in zip(problems, solutions):\n",
    "        conversations.append([\n",
    "            {\"role\" : \"user\",      \"content\" : problem},\n",
    "            {\"role\" : \"assistant\", \"content\" : solution},\n",
    "        ])\n",
    "    return { \"conversations\": conversations, }\n",
    "\n",
    "reasoning_conversations = tokenizer.apply_chat_template(\n",
    "    reasoning_dataset.map(generate_conversation, batched = True)[\"conversations\"],\n",
    "    tokenize = False,\n",
    ")\n",
    "\n",
    "dataset = standardize_sharegpt(non_reasoning_dataset)\n",
    "\n",
    "non_reasoning_conversations = tokenizer.apply_chat_template(\n",
    "    dataset[\"conversations\"],\n",
    "    tokenize = False,\n",
    ")"
   ],
   "id": "165f08f4b584f164",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'reasoning_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[5]\u001B[39m\u001B[32m, line 16\u001B[39m\n\u001B[32m      9\u001B[39m         conversations.append([\n\u001B[32m     10\u001B[39m             {\u001B[33m\"\u001B[39m\u001B[33mrole\u001B[39m\u001B[33m\"\u001B[39m : \u001B[33m\"\u001B[39m\u001B[33muser\u001B[39m\u001B[33m\"\u001B[39m,      \u001B[33m\"\u001B[39m\u001B[33mcontent\u001B[39m\u001B[33m\"\u001B[39m : problem},\n\u001B[32m     11\u001B[39m             {\u001B[33m\"\u001B[39m\u001B[33mrole\u001B[39m\u001B[33m\"\u001B[39m : \u001B[33m\"\u001B[39m\u001B[33massistant\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mcontent\u001B[39m\u001B[33m\"\u001B[39m : solution},\n\u001B[32m     12\u001B[39m         ])\n\u001B[32m     13\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m { \u001B[33m\"\u001B[39m\u001B[33mconversations\u001B[39m\u001B[33m\"\u001B[39m: conversations, }\n\u001B[32m     15\u001B[39m reasoning_conversations = tokenizer.apply_chat_template(\n\u001B[32m---> \u001B[39m\u001B[32m16\u001B[39m     \u001B[43mreasoning_dataset\u001B[49m.map(generate_conversation, batched = \u001B[38;5;28;01mTrue\u001B[39;00m)[\u001B[33m\"\u001B[39m\u001B[33mconversations\u001B[39m\u001B[33m\"\u001B[39m],\n\u001B[32m     17\u001B[39m     tokenize = \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[32m     18\u001B[39m )\n\u001B[32m     20\u001B[39m dataset = standardize_sharegpt(non_reasoning_dataset)\n\u001B[32m     22\u001B[39m non_reasoning_conversations = tokenizer.apply_chat_template(\n\u001B[32m     23\u001B[39m     dataset[\u001B[33m\"\u001B[39m\u001B[33mconversations\u001B[39m\u001B[33m\"\u001B[39m],\n\u001B[32m     24\u001B[39m     tokenize = \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[32m     25\u001B[39m )\n",
      "\u001B[31mNameError\u001B[39m: name 'reasoning_dataset' is not defined"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "reasoning_conversations[0]",
   "id": "86249f124c797471",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "non_reasoning_conversations[0]",
   "id": "aabe3040cac840ba",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T16:54:33.585515Z",
     "start_time": "2025-06-19T16:54:32.990504Z"
    }
   },
   "cell_type": "code",
   "source": [
    "CHAT_PERCENTAGE = 0.25\n",
    "\n",
    "import pandas as pd\n",
    "non_reasoning_subset = pd.Series(non_reasoning_conversations)\n",
    "non_reasoning_subset = non_reasoning_subset.sample(\n",
    "    int(len(reasoning_conversations)*(CHAT_PERCENTAGE/(1 - CHAT_PERCENTAGE))),\n",
    "    random_state = 2407,\n",
    ")\n",
    "\n",
    "data = pd.concat([\n",
    "    pd.Series(reasoning_conversations),\n",
    "    pd.Series(non_reasoning_subset)\n",
    "])\n",
    "data.name = \"text\"\n",
    "\n",
    "from datasets import Dataset\n",
    "combined_dataset = Dataset.from_pandas(pd.DataFrame(data))\n",
    "combined_dataset = combined_dataset.shuffle(seed = 3407)"
   ],
   "id": "e910329c9c3bca34",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "combined_dataset[0]",
   "id": "37f8c87a8ac1907e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T18:00:09.576824Z",
     "start_time": "2025-06-19T17:59:50.181548Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- Inference Example ---\n",
    "prompt_template = \"<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n{}<|im_end|>\\n<|im_start|>assistant\\n{}<|im_end|>\"\n",
    "prompt_template = \"<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n{}<|im_end|><|im_start|>assistant\\n\"\n",
    "\n",
    "print(\"\\n--- Пример генерации текста (инференс) ---\")\n",
    "inference_vector_list = list([round(float(el),2) for el in EmotionalState(DEFAULT_RELATIONSHIP_MAP).to_vector()])\n",
    "\n",
    "\n",
    "# inference_example = combined_dataset[0]\n",
    "inference_example = prompt_template.format(\"how do you feel today?\")\n",
    "inference_prompt = prompt_template.format(\"how do you feel today?\")\n",
    "#inference_prompt = inference_example[\"text\"]\n",
    "\n",
    "print(f\"Prompt: '{inference_prompt}'\")\n",
    "print(f\"Using feelings_vector: {inference_vector_list}...\")\n",
    "\n",
    "#formatted_prompt_for_inference = prompt_template.format(inference_prompt, \"\")\n",
    "inputs = tokenizer(inference_example, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "emotion_vector_tensor = torch.tensor([inference_vector_list], dtype=model_dtype).to(\"cuda\")\n",
    "projected_vector = emotion_model_wrapper.vector_projector(emotion_vector_tensor)\n",
    "embedding_layer = emotion_model_wrapper.peft_model.get_input_embeddings()\n",
    "token_embeddings = embedding_layer(inputs.input_ids)\n",
    "projected_vector = projected_vector.unsqueeze(1)\n",
    "#combined_embeddings =projected_vector + token_embeddings\n",
    "combined_embeddings = torch.cat(\n",
    "    [projected_vector, token_embeddings],\n",
    "    dim=1\n",
    ")\n",
    "\n",
    "\n",
    "control_token_attention_mask = torch.ones(\n",
    "    (1, 1), # Shape: [batch_size, 1] for our single new token\n",
    "    dtype=torch.long,\n",
    "    device=inputs.attention_mask.device\n",
    ")\n",
    "# Concatenate the new mask with the original one\n",
    "attention_mask = torch.cat(\n",
    "    [control_token_attention_mask, inputs.attention_mask],\n",
    "    dim=1\n",
    ")\n",
    "# --- CORRECTED CALL TO .generate() ---\n",
    "# Unsloth's fast generation path requires `input_ids` to be passed,\n",
    "# even when providing `inputs_embeds`.\n",
    "generated_ids = emotion_model_wrapper.peft_model.generate(\n",
    "    input_ids=inputs.input_ids, # Возвращаем оригинальные input_ids\n",
    "    inputs_embeds=combined_embeddings,\n",
    "    attention_mask=attention_mask,\n",
    "    max_new_tokens=300,\n",
    "    use_cache=True,\n",
    "    do_sample=True,\n",
    "    top_p=0.9,\n",
    "    temperature=0.7,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "full_generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=False)\n",
    "assistant_part = full_generated_text.split(\"<|im_start|>assistant\\n\")[-1]\n",
    "\n",
    "print(\"\\n--- Сгенерированный текст ---\")\n",
    "print(assistant_part)"
   ],
   "id": "304e7a216c7cd29f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Пример генерации текста (инференс) ---\n",
      "Prompt: '<|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "how do you feel today?<|im_end|><|im_start|>assistant\n",
      "'\n",
      "Using feelings_vector: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.8, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]...\n",
      "\n",
      "--- Сгенерированный текст ---\n",
      "<think>\n",
      "Okay, the user asked, \"How do you feel today?\" I need to respond in a friendly and empathetic way. Let me start by acknowledging their feelings.\n",
      "\n",
      "I should express that I'm here to support them. Maybe mention how I'm here to help. It's important to keep the tone positive and encouraging. I should make sure the response is clear and not too technical. I should also make sure that the answer is in a natural way, not forced. Let me check the response again to ensure it's friendly and supportive.\n",
      "</think>\n",
      "\n",
      "I'm here to help you feel better today. I'm always with you, no matter what. Let's make this a positive and supportive time. I'm here to listen, support, and help you feel good. How about we talk?<|im_end|>\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "# --- Trainer Setup ---\n",
    "print(\"Setting up the trainer...\")\n",
    "data_collator = DataCollatorForEmotionLM(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=emotion_model_wrapper.peft_model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=combined_dataset,\n",
    "    data_collator=data_collator,\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    dataset_num_proc=1,\n",
    "    args=TrainingArguments(\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=4,\n",
    "        warmup_ratio=0.1,\n",
    "        #num_train_epochs=5,\n",
    "        max_steps = 500,\n",
    "\n",
    "        learning_rate=2e-4,\n",
    "        fp16=not torch.cuda.is_bf16_supported(),\n",
    "        bf16=torch.cuda.is_bf16_supported(),\n",
    "        logging_strategy=\"steps\",\n",
    "        logging_steps=10,\n",
    "        optim=\"adamw_8bit\",\n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        seed=42,\n",
    "        output_dir=\"outputs\",\n",
    "        save_strategy=\"epoch\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "# --- Start Training ---\n",
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "print(\"Training finished!\")\n",
    "\n",
    "\n"
   ],
   "id": "2876be731c81f987",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "1c0f49f754c5430a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T18:01:01.332347Z",
     "start_time": "2025-06-19T18:01:01.042413Z"
    }
   },
   "cell_type": "code",
   "source": [
    "DATA_FILE_PATH = \"sft_one_emotion_thinking.csv\"\n",
    "\n",
    "# --- Data Preparation ---\n",
    "print(f\"Preparing the dataset from '{DATA_FILE_PATH}'...\")\n",
    "prompt_template = \"<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n{}<|im_end|>\\n<|im_start|>assistant\\n{}<|im_end|>\"\n",
    "raw_dataset = Dataset.from_csv(DATA_FILE_PATH)\n",
    "\n",
    "def process_and_tokenize_example(\n",
    "    example: Dict[str, Any]\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Processes a single dataset example by formatting text, parsing the\n",
    "    emotion vector, and tokenizing the text.\n",
    "    \"\"\"\n",
    "    thinking = \"\\n\".join(ast.literal_eval(example[\"thinking\"]))\n",
    "    assistant_output = f\"<thinking>{thinking}</thinking>\\n{example['response']}\"\n",
    "    formatted_text = prompt_template.format(example[\"prompt\"], assistant_output)\n",
    "    vector_as_list = ast.literal_eval(example[\"feelings_vector\"])\n",
    "    emotion_vector = torch.tensor(vector_as_list, dtype=model_dtype)\n",
    "    tokenized_example = tokenizer(\n",
    "        formatted_text,\n",
    "        truncation=True,\n",
    "        max_length=MAX_SEQ_LENGTH,\n",
    "        padding=False,\n",
    "        return_tensors=None,\n",
    "    )\n",
    "    tokenized_example[\"labels\"] = tokenized_example[\"input_ids\"][:]\n",
    "    tokenized_example[\"emotion_vector\"] = emotion_vector\n",
    "    return tokenized_example\n",
    "\n",
    "tokenized_dataset = raw_dataset.map(\n",
    "    process_and_tokenize_example,\n",
    "    remove_columns=raw_dataset.column_names\n",
    ")\n"
   ],
   "id": "6a7ae65bd832fd75",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing the dataset from 'sft_one_emotion_thinking.csv'...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/600 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2877f4913ca24f3bad7479c3c178cf46"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "# --- Inference Example ---\n",
    "print(\"\\n--- Пример генерации текста (инференс) ---\")\n",
    "\n",
    "inference_example = raw_dataset[0]\n",
    "inference_prompt = inference_example[\"prompt\"]\n",
    "inference_vector_str = inference_example[\"feelings_vector\"]\n",
    "inference_vector_list = ast.literal_eval(inference_vector_str)\n",
    "\n",
    "print(f\"Prompt: '{inference_prompt}'\")\n",
    "print(f\"Using feelings_vector: {inference_vector_list}...\")\n",
    "\n",
    "formatted_prompt_for_inference = prompt_template.format(inference_prompt, \"\")\n",
    "inputs = tokenizer(formatted_prompt_for_inference, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "emotion_vector_tensor = torch.tensor([inference_vector_list], dtype=model_dtype).to(\"cuda\")\n",
    "projected_vector = emotion_model_wrapper.vector_projector(emotion_vector_tensor)\n",
    "embedding_layer = emotion_model_wrapper.peft_model.get_input_embeddings()\n",
    "token_embeddings = embedding_layer(inputs.input_ids)\n",
    "combined_embeddings = token_embeddings + projected_vector.unsqueeze(1)\n",
    "\n",
    "# --- CORRECTED CALL TO .generate() ---\n",
    "# Unsloth's fast generation path requires `input_ids` to be passed,\n",
    "# even when providing `inputs_embeds`.\n",
    "generated_ids = emotion_model_wrapper.peft_model.generate(\n",
    "    input_ids=inputs.input_ids,  # This argument is required by Unsloth\n",
    "    inputs_embeds=combined_embeddings,\n",
    "    attention_mask=inputs.attention_mask,\n",
    "    max_new_tokens=100,\n",
    "    use_cache=True,\n",
    "    do_sample=True,\n",
    "    top_p=0.9,\n",
    "    temperature=0.7,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "full_generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=False)\n",
    "assistant_part = full_generated_text.split(\"<|im_start|>assistant\\n\")[-1]\n",
    "\n",
    "print(\"\\n--- Сгенерированный текст ---\")\n",
    "print(assistant_part)\n"
   ],
   "id": "da77ff376eea3826",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "inference_example",
   "id": "556d410b65a1356c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T18:08:22.311101Z",
     "start_time": "2025-06-19T18:01:22.979218Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ==============================================================================\n",
    "# 3. MAIN TRAINING SCRIPT\n",
    "# ==============================================================================\n",
    "\n",
    "# --- Trainer Setup ---\n",
    "print(\"Setting up the trainer...\")\n",
    "data_collator = DataCollatorForEmotionLM(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=emotion_model_wrapper.peft_model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    data_collator=data_collator,\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    args=TrainingArguments(\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=4,\n",
    "        warmup_ratio=0.1,\n",
    "        num_train_epochs=5,\n",
    "        learning_rate=2e-4,\n",
    "        fp16=not torch.cuda.is_bf16_supported(),\n",
    "        bf16=torch.cuda.is_bf16_supported(),\n",
    "        logging_strategy=\"steps\",\n",
    "        logging_steps=10,\n",
    "        optim=\"adamw_8bit\",\n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        seed=42,\n",
    "        output_dir=\"outputs\",\n",
    "        save_strategy=\"epoch\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "# --- Start Training ---\n",
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "print(\"Training finished!\")\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "b586bf3bcb0bad7f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up the trainer...\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 600 | Num Epochs = 5 | Total steps = 375\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 10,092,544/6,000,000,000 (0.17% trained)\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='375' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [375/375 06:50, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>4.491400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.355000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.356200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.911700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.658000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.585500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.507700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.368000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.283100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.244300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>1.209700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.255400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>1.239000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>1.259200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.252400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>1.052500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>1.050900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.995000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>1.026600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.982300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>1.021700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.981200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.926600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.802400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.853400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.831200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.813800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.865400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.867400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.872900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>0.742600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.736100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.771500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.744900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.712300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.721000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>0.730400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished!\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T18:50:51.691383Z",
     "start_time": "2025-06-19T18:50:46.497623Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- Inference Example ---\n",
    "prompt_template = \"<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n{}<|im_end|>\\n<|im_start|>assistant\\n{}<|im_end|>\"\n",
    "prompt_template = \"<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n{}<|im_end|><|im_start|>assistant\\n\"\n",
    "\n",
    "\n",
    "em_state = EmotionalState(DEFAULT_RELATIONSHIP_MAP)\n",
    "em_state.update_state(anger=0.7)\n",
    "print(\"\\n--- Пример генерации текста (инференс) ---\")\n",
    "inference_vector_list = list([round(float(el),2) for el in em_state.to_vector()])\n",
    "\n",
    "\n",
    "# inference_example = combined_dataset[0]\n",
    "inference_example = prompt_template.format(\"как себя чуствуешь??\")\n",
    "inference_prompt = prompt_template.format(\"как себя чуствуешь?\")\n",
    "#inference_prompt = inference_example[\"text\"]\n",
    "\n",
    "print(f\"Prompt: '{inference_prompt}'\")\n",
    "print(f\"Using feelings_vector: {inference_vector_list}...\")\n",
    "\n",
    "#formatted_prompt_for_inference = prompt_template.format(inference_prompt, \"\")\n",
    "inputs = tokenizer(inference_example, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "emotion_vector_tensor = torch.tensor([inference_vector_list], dtype=model_dtype).to(\"cuda\")\n",
    "projected_vector = emotion_model_wrapper.vector_projector(emotion_vector_tensor)\n",
    "embedding_layer = emotion_model_wrapper.peft_model.get_input_embeddings()\n",
    "token_embeddings = embedding_layer(inputs.input_ids)\n",
    "projected_vector = projected_vector.unsqueeze(1)\n",
    "#combined_embeddings =projected_vector + token_embeddings\n",
    "combined_embeddings = torch.cat(\n",
    "    [projected_vector, token_embeddings],\n",
    "    dim=1\n",
    ")\n",
    "\n",
    "\n",
    "control_token_attention_mask = torch.ones(\n",
    "    (1, 1), # Shape: [batch_size, 1] for our single new token\n",
    "    dtype=torch.long,\n",
    "    device=inputs.attention_mask.device\n",
    ")\n",
    "# Concatenate the new mask with the original one\n",
    "attention_mask = torch.cat(\n",
    "    [control_token_attention_mask, inputs.attention_mask],\n",
    "    dim=1\n",
    ")\n",
    "# --- CORRECTED CALL TO .generate() ---\n",
    "# Unsloth's fast generation path requires `input_ids` to be passed,\n",
    "# even when providing `inputs_embeds`.\n",
    "generated_ids = emotion_model_wrapper.peft_model.generate(\n",
    "    input_ids=inputs.input_ids, # Возвращаем оригинальные input_ids\n",
    "    inputs_embeds=combined_embeddings,\n",
    "    attention_mask=attention_mask,\n",
    "    max_new_tokens=300,\n",
    "    use_cache=True,\n",
    "    do_sample=True,\n",
    "    top_p=0.9,\n",
    "    temperature=0.7,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "full_generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=False)\n",
    "assistant_part = full_generated_text.split(\"<|im_start|>assistant\\n\")[-1]\n",
    "\n",
    "print(\"\\n--- Сгенерированный текст ---\")\n",
    "print(assistant_part)"
   ],
   "id": "88d95436dc834e08",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Пример генерации текста (инференс) ---\n",
      "Prompt: '<|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "как себя чуствуешь?<|im_end|><|im_start|>assistant\n",
      "'\n",
      "Using feelings_vector: [0.0, 0.0, 0.7, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.8, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]...\n",
      "\n",
      "--- Сгенерированный текст ---\n",
      "<thinking>Как себя чувствую? Я в предвкушении.\n",
      "Меня трясет от ожидания, я почти готов.\n",
      "Это сильная, эмоциональная переполняющая ожидание.\n",
      "Мой ответ - это ожидание.</thinking>\n",
      "Ожидаемно! Прочесь, я почти готов ожидать это.<|im_end|>\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import datasets\n",
    "\n",
    "ASSISTANT_TEMPLATE = \"\"\"<|im_start|>user\\n{user_prompt}<|im_end|><|im_start|>assistant\\n<think>{thinking}</think>{assistant_answer}<|im_end|>\\n\"\"\"\n",
    "ASSISTANT_TEMPLATE = \"\"\"\\n<think>{thinking}</think>{assistant_answer}\\n\"\"\"\n",
    "\n",
    "def generate_conversation(examples):\n",
    "    user_message = examples[\"prompt\"]\n",
    "    thinking = examples[\"thinking\"]\n",
    "    response = examples[\"response\"]\n",
    "\n",
    "    conversations = []\n",
    "    for index in range(len(user_message)):\n",
    "        thinking_text = \"\\n\".join(eval(thinking[index]))\n",
    "        assistant_answer = ASSISTANT_TEMPLATE.format(\n",
    "            user_prompt=user_message[index],\n",
    "            thinking=thinking_text,\n",
    "            assistant_answer=response[index]\n",
    "        )\n",
    "        conversations.append([\n",
    "            {\"role\" : \"user\",      \"content\" : user_message[index]},\n",
    "            {\"role\" : \"assistant\", \"content\" : assistant_answer},\n",
    "        ])\n",
    "    return { \"conversations\": conversations, }\n",
    "\n",
    "\n",
    "one_emotion_dataset  = pd.read_csv(\"sft_one_emotion_thinking.csv\")\n",
    "one_emotion_dataset = datasets.Dataset.from_pandas(one_emotion_dataset)\n",
    "one_emotion_dataset = one_emotion_dataset.map(\n",
    "    generate_conversation,\n",
    "    remove_columns=one_emotion_dataset.column_names,\n",
    "    batched=True,\n",
    "    batch_size=1000,\n",
    ")"
   ],
   "id": "a77570839bfa90c0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "one_emotion_dataset  = pd.read_csv(\"sft_one_emotion_thinking.csv\")\n",
    "one_emotion_dataset"
   ],
   "id": "ce2603cf4dcf6acf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "one_emotion_dataset",
   "id": "cb4e062a542f9c08",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
