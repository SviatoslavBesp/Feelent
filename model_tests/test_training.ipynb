{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "ca98ff38f425f4f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "!pip install torch==2.6.0 torchvision==0.21.0 torchaudio==2.6.0 --index-url https://download.pytorch.org/whl/cu118  # Version for rtx3090",
   "id": "3c4a4a27d467483b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T12:33:20.797187Z",
     "start_time": "2025-06-09T12:33:20.787017Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from test_data_gen import generate_synthetic_dataset\n",
    "\n",
    "NUMBER_OF_RECORDS = 200\n",
    "VECTOR_DIMENSION = 30\n",
    "\n",
    "synthetic_dataset = generate_synthetic_dataset(\n",
    "    number_of_records=NUMBER_OF_RECORDS,\n",
    "    vector_dimension=VECTOR_DIMENSION\n",
    ")\n",
    "synthetic_dataset[50]"
   ],
   "id": "ab63280958928dc",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': 'Как работает GPS?',\n",
       " 'response': 'Система глобального позиционирования (GPS) использует сигналы от спутников для определения точного местоположения приёмника на Земле путем трилатерации.',\n",
       " 'custom_vector': [0.9241718649864197,\n",
       "  0.8862757086753845,\n",
       "  0.9418854117393494,\n",
       "  0.8010405898094177,\n",
       "  0.9979512691497803,\n",
       "  0.9523987174034119,\n",
       "  0.9858677387237549,\n",
       "  0.8162245750427246,\n",
       "  0.9277184009552002,\n",
       "  0.8832427263259888,\n",
       "  0.12798306345939636,\n",
       "  0.1623060256242752,\n",
       "  0.04388086497783661,\n",
       "  0.11491576582193375,\n",
       "  0.08334853500127792,\n",
       "  0.07457728683948517,\n",
       "  0.0013142724055796862,\n",
       "  0.06693758070468903,\n",
       "  0.18873128294944763,\n",
       "  0.12715288996696472,\n",
       "  0.038583215326070786,\n",
       "  0.09092347323894501,\n",
       "  0.0837830901145935,\n",
       "  0.01080994587391615,\n",
       "  0.1938796192407608,\n",
       "  0.13350287079811096,\n",
       "  0.01674053631722927,\n",
       "  0.005774518009275198,\n",
       "  0.1798594743013382,\n",
       "  0.07867621630430222]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T12:34:44.236554Z",
     "start_time": "2025-06-09T12:33:24.606181Z"
    }
   },
   "cell_type": "code",
   "source": "!pip install unsloth",
   "id": "d59634d321abb2bd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting unsloth\n",
      "  Downloading unsloth-2025.6.1-py3-none-any.whl.metadata (47 kB)\n",
      "Collecting unsloth_zoo>=2025.6.1 (from unsloth)\n",
      "  Downloading unsloth_zoo-2025.6.1-py3-none-any.whl.metadata (8.1 kB)\n",
      "Collecting torch<=2.7.0,>=2.4.0 (from unsloth)\n",
      "  Using cached torch-2.7.0-cp312-cp312-win_amd64.whl.metadata (29 kB)\n",
      "Collecting xformers>=0.0.27.post2 (from unsloth)\n",
      "  Downloading xformers-0.0.30-cp312-cp312-win_amd64.whl.metadata (1.3 kB)\n",
      "Collecting bitsandbytes (from unsloth)\n",
      "  Downloading bitsandbytes-0.46.0-py3-none-win_amd64.whl.metadata (10 kB)\n",
      "Collecting triton-windows (from unsloth)\n",
      "  Downloading triton_windows-3.3.1.post19-cp312-cp312-win_amd64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: packaging in c:\\my_projects\\feelent\\.venv\\lib\\site-packages (from unsloth) (25.0)\n",
      "Collecting tyro (from unsloth)\n",
      "  Downloading tyro-0.9.24-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting transformers!=4.47.0,!=4.52.0,!=4.52.1,!=4.52.2,>=4.51.3 (from unsloth)\n",
      "  Downloading transformers-4.52.4-py3-none-any.whl.metadata (38 kB)\n",
      "Requirement already satisfied: datasets>=3.4.1 in c:\\my_projects\\feelent\\.venv\\lib\\site-packages (from unsloth) (3.6.0)\n",
      "Collecting sentencepiece>=0.2.0 (from unsloth)\n",
      "  Using cached sentencepiece-0.2.0-cp312-cp312-win_amd64.whl.metadata (8.3 kB)\n",
      "Requirement already satisfied: tqdm in c:\\my_projects\\feelent\\.venv\\lib\\site-packages (from unsloth) (4.67.1)\n",
      "Requirement already satisfied: psutil in c:\\my_projects\\feelent\\.venv\\lib\\site-packages (from unsloth) (7.0.0)\n",
      "Collecting wheel>=0.42.0 (from unsloth)\n",
      "  Using cached wheel-0.45.1-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: numpy in c:\\my_projects\\feelent\\.venv\\lib\\site-packages (from unsloth) (2.3.0)\n",
      "Collecting accelerate>=0.34.1 (from unsloth)\n",
      "  Downloading accelerate-1.7.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9 (from unsloth)\n",
      "  Downloading trl-0.18.1-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting peft!=0.11.0,>=0.7.1 (from unsloth)\n",
      "  Using cached peft-0.15.2-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting protobuf<4.0.0 (from unsloth)\n",
      "  Using cached protobuf-3.20.3-py2.py3-none-any.whl.metadata (720 bytes)\n",
      "Requirement already satisfied: huggingface_hub in c:\\my_projects\\feelent\\.venv\\lib\\site-packages (from unsloth) (0.32.4)\n",
      "Collecting hf_transfer (from unsloth)\n",
      "  Using cached hf_transfer-0.1.9-cp38-abi3-win_amd64.whl.metadata (1.8 kB)\n",
      "Collecting diffusers (from unsloth)\n",
      "  Using cached diffusers-0.33.1-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting torchvision (from unsloth)\n",
      "  Downloading torchvision-0.22.1-cp312-cp312-win_amd64.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: filelock in c:\\my_projects\\feelent\\.venv\\lib\\site-packages (from torch<=2.7.0,>=2.4.0->unsloth) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\my_projects\\feelent\\.venv\\lib\\site-packages (from torch<=2.7.0,>=2.4.0->unsloth) (4.14.0)\n",
      "Collecting sympy>=1.13.3 (from torch<=2.7.0,>=2.4.0->unsloth)\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch<=2.7.0,>=2.4.0->unsloth)\n",
      "  Downloading networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: jinja2 in c:\\my_projects\\feelent\\.venv\\lib\\site-packages (from torch<=2.7.0,>=2.4.0->unsloth) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\my_projects\\feelent\\.venv\\lib\\site-packages (from torch<=2.7.0,>=2.4.0->unsloth) (2025.3.0)\n",
      "Requirement already satisfied: setuptools in c:\\my_projects\\feelent\\.venv\\lib\\site-packages (from torch<=2.7.0,>=2.4.0->unsloth) (80.9.0)\n",
      "Requirement already satisfied: pyyaml in c:\\my_projects\\feelent\\.venv\\lib\\site-packages (from accelerate>=0.34.1->unsloth) (6.0.2)\n",
      "Collecting safetensors>=0.4.3 (from accelerate>=0.34.1->unsloth)\n",
      "  Using cached safetensors-0.5.3-cp38-abi3-win_amd64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\my_projects\\feelent\\.venv\\lib\\site-packages (from datasets>=3.4.1->unsloth) (20.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\my_projects\\feelent\\.venv\\lib\\site-packages (from datasets>=3.4.1->unsloth) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\my_projects\\feelent\\.venv\\lib\\site-packages (from datasets>=3.4.1->unsloth) (2.3.0)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\my_projects\\feelent\\.venv\\lib\\site-packages (from datasets>=3.4.1->unsloth) (2.32.3)\n",
      "Requirement already satisfied: xxhash in c:\\my_projects\\feelent\\.venv\\lib\\site-packages (from datasets>=3.4.1->unsloth) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\my_projects\\feelent\\.venv\\lib\\site-packages (from datasets>=3.4.1->unsloth) (0.70.16)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\my_projects\\feelent\\.venv\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1->unsloth) (3.12.11)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\my_projects\\feelent\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1->unsloth) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\my_projects\\feelent\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1->unsloth) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\my_projects\\feelent\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1->unsloth) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\my_projects\\feelent\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1->unsloth) (1.6.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\my_projects\\feelent\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1->unsloth) (6.4.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\my_projects\\feelent\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1->unsloth) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\my_projects\\feelent\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1->unsloth) (1.20.0)\n",
      "Requirement already satisfied: idna>=2.0 in c:\\my_projects\\feelent\\.venv\\lib\\site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1->unsloth) (3.10)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\my_projects\\feelent\\.venv\\lib\\site-packages (from requests>=2.32.2->datasets>=3.4.1->unsloth) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\my_projects\\feelent\\.venv\\lib\\site-packages (from requests>=2.32.2->datasets>=3.4.1->unsloth) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\my_projects\\feelent\\.venv\\lib\\site-packages (from requests>=2.32.2->datasets>=3.4.1->unsloth) (2025.4.26)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch<=2.7.0,>=2.4.0->unsloth)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: colorama in c:\\my_projects\\feelent\\.venv\\lib\\site-packages (from tqdm->unsloth) (0.4.6)\n",
      "Collecting regex!=2019.12.17 (from transformers!=4.47.0,!=4.52.0,!=4.52.1,!=4.52.2,>=4.51.3->unsloth)\n",
      "  Using cached regex-2024.11.6-cp312-cp312-win_amd64.whl.metadata (41 kB)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers!=4.47.0,!=4.52.0,!=4.52.1,!=4.52.2,>=4.51.3->unsloth)\n",
      "  Using cached tokenizers-0.21.1-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting cut_cross_entropy (from unsloth_zoo>=2025.6.1->unsloth)\n",
      "  Using cached cut_cross_entropy-25.1.1-py3-none-any.whl.metadata (9.3 kB)\n",
      "Collecting pillow (from unsloth_zoo>=2025.6.1->unsloth)\n",
      "  Using cached pillow-11.2.1-cp312-cp312-win_amd64.whl.metadata (9.1 kB)\n",
      "Collecting msgspec (from unsloth_zoo>=2025.6.1->unsloth)\n",
      "  Using cached msgspec-0.19.0-cp312-cp312-win_amd64.whl.metadata (7.1 kB)\n",
      "Collecting importlib-metadata (from diffusers->unsloth)\n",
      "  Downloading importlib_metadata-8.7.0-py3-none-any.whl.metadata (4.8 kB)\n",
      "Collecting zipp>=3.20 (from importlib-metadata->diffusers->unsloth)\n",
      "  Downloading zipp-3.23.0-py3-none-any.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\my_projects\\feelent\\.venv\\lib\\site-packages (from jinja2->torch<=2.7.0,>=2.4.0->unsloth) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\my_projects\\feelent\\.venv\\lib\\site-packages (from pandas->datasets>=3.4.1->unsloth) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\my_projects\\feelent\\.venv\\lib\\site-packages (from pandas->datasets>=3.4.1->unsloth) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\my_projects\\feelent\\.venv\\lib\\site-packages (from pandas->datasets>=3.4.1->unsloth) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\my_projects\\feelent\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets>=3.4.1->unsloth) (1.17.0)\n",
      "INFO: pip is looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting torchvision (from unsloth)\n",
      "  Using cached torchvision-0.22.0-cp312-cp312-win_amd64.whl.metadata (6.3 kB)\n",
      "Collecting docstring-parser>=0.15 (from tyro->unsloth)\n",
      "  Using cached docstring_parser-0.16-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting rich>=11.1.0 (from tyro->unsloth)\n",
      "  Using cached rich-14.0.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting shtab>=1.5.6 (from tyro->unsloth)\n",
      "  Using cached shtab-1.7.2-py3-none-any.whl.metadata (7.4 kB)\n",
      "Collecting typeguard>=4.0.0 (from tyro->unsloth)\n",
      "  Downloading typeguard-4.4.3-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=11.1.0->tyro->unsloth)\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\my_projects\\feelent\\.venv\\lib\\site-packages (from rich>=11.1.0->tyro->unsloth) (2.19.1)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro->unsloth)\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Downloading unsloth-2025.6.1-py3-none-any.whl (276 kB)\n",
      "Using cached protobuf-3.20.3-py2.py3-none-any.whl (162 kB)\n",
      "Using cached torch-2.7.0-cp312-cp312-win_amd64.whl (212.5 MB)\n",
      "Downloading accelerate-1.7.0-py3-none-any.whl (362 kB)\n",
      "Using cached peft-0.15.2-py3-none-any.whl (411 kB)\n",
      "Using cached safetensors-0.5.3-cp38-abi3-win_amd64.whl (308 kB)\n",
      "Using cached sentencepiece-0.2.0-cp312-cp312-win_amd64.whl (991 kB)\n",
      "Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "   ---------------------------------------- 0.0/6.3 MB ? eta -:--:--\n",
      "   --------------------- ------------------ 3.4/6.3 MB 16.8 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 5.0/6.3 MB 17.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.3/6.3 MB 10.2 MB/s eta 0:00:00\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Downloading transformers-4.52.4-py3-none-any.whl (10.5 MB)\n",
      "   ---------------------------------------- 0.0/10.5 MB ? eta -:--:--\n",
      "   --------------------- ------------------ 5.5/10.5 MB 55.8 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 8.7/10.5 MB 29.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.5/10.5 MB 27.2 MB/s eta 0:00:00\n",
      "Using cached tokenizers-0.21.1-cp39-abi3-win_amd64.whl (2.4 MB)\n",
      "Using cached regex-2024.11.6-cp312-cp312-win_amd64.whl (273 kB)\n",
      "Downloading trl-0.18.1-py3-none-any.whl (366 kB)\n",
      "Downloading unsloth_zoo-2025.6.1-py3-none-any.whl (147 kB)\n",
      "Using cached wheel-0.45.1-py3-none-any.whl (72 kB)\n",
      "Downloading xformers-0.0.30-cp312-cp312-win_amd64.whl (108.0 MB)\n",
      "   ---------------------------------------- 0.0/108.0 MB ? eta -:--:--\n",
      "   - -------------------------------------- 3.1/108.0 MB 15.4 MB/s eta 0:00:07\n",
      "   -- ------------------------------------- 6.0/108.0 MB 14.2 MB/s eta 0:00:08\n",
      "   --- ------------------------------------ 8.1/108.0 MB 14.8 MB/s eta 0:00:07\n",
      "   ---- ----------------------------------- 11.5/108.0 MB 13.9 MB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 13.6/108.0 MB 13.4 MB/s eta 0:00:08\n",
      "   ----- ---------------------------------- 16.0/108.0 MB 13.1 MB/s eta 0:00:08\n",
      "   ------ --------------------------------- 17.8/108.0 MB 12.6 MB/s eta 0:00:08\n",
      "   ------- -------------------------------- 19.7/108.0 MB 12.1 MB/s eta 0:00:08\n",
      "   ------- -------------------------------- 21.2/108.0 MB 11.7 MB/s eta 0:00:08\n",
      "   -------- ------------------------------- 23.1/108.0 MB 11.3 MB/s eta 0:00:08\n",
      "   --------- ------------------------------ 24.6/108.0 MB 11.1 MB/s eta 0:00:08\n",
      "   --------- ------------------------------ 26.5/108.0 MB 10.9 MB/s eta 0:00:08\n",
      "   ---------- ----------------------------- 28.3/108.0 MB 10.8 MB/s eta 0:00:08\n",
      "   ----------- ---------------------------- 30.1/108.0 MB 10.6 MB/s eta 0:00:08\n",
      "   ----------- ---------------------------- 32.0/108.0 MB 10.5 MB/s eta 0:00:08\n",
      "   ------------ --------------------------- 33.8/108.0 MB 10.4 MB/s eta 0:00:08\n",
      "   ------------- -------------------------- 35.9/108.0 MB 10.4 MB/s eta 0:00:07\n",
      "   ------------- -------------------------- 37.7/108.0 MB 10.3 MB/s eta 0:00:07\n",
      "   -------------- ------------------------- 39.6/108.0 MB 10.2 MB/s eta 0:00:07\n",
      "   --------------- ------------------------ 41.7/108.0 MB 10.2 MB/s eta 0:00:07\n",
      "   ---------------- ----------------------- 43.5/108.0 MB 10.2 MB/s eta 0:00:07\n",
      "   ---------------- ----------------------- 45.6/108.0 MB 10.2 MB/s eta 0:00:07\n",
      "   ----------------- ---------------------- 47.4/108.0 MB 10.1 MB/s eta 0:00:06\n",
      "   ------------------ --------------------- 49.3/108.0 MB 10.1 MB/s eta 0:00:06\n",
      "   ------------------- -------------------- 51.4/108.0 MB 10.1 MB/s eta 0:00:06\n",
      "   ------------------- -------------------- 53.5/108.0 MB 10.1 MB/s eta 0:00:06\n",
      "   -------------------- ------------------- 55.3/108.0 MB 10.1 MB/s eta 0:00:06\n",
      "   --------------------- ------------------ 57.1/108.0 MB 10.0 MB/s eta 0:00:06\n",
      "   --------------------- ------------------ 59.2/108.0 MB 10.0 MB/s eta 0:00:05\n",
      "   ---------------------- ----------------- 61.1/108.0 MB 10.0 MB/s eta 0:00:05\n",
      "   ----------------------- ---------------- 63.2/108.0 MB 10.0 MB/s eta 0:00:05\n",
      "   ------------------------ --------------- 65.0/108.0 MB 10.0 MB/s eta 0:00:05\n",
      "   ------------------------ --------------- 67.1/108.0 MB 10.0 MB/s eta 0:00:05\n",
      "   ------------------------- -------------- 68.9/108.0 MB 10.0 MB/s eta 0:00:04\n",
      "   -------------------------- ------------- 71.0/108.0 MB 10.0 MB/s eta 0:00:04\n",
      "   -------------------------- ------------- 72.9/108.0 MB 10.0 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 75.0/108.0 MB 10.0 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 76.8/108.0 MB 10.0 MB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 78.9/108.0 MB 9.9 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 80.7/108.0 MB 9.9 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 82.8/108.0 MB 9.9 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 84.9/108.0 MB 9.9 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 86.8/108.0 MB 9.9 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 88.9/108.0 MB 9.9 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 90.7/108.0 MB 9.9 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 92.8/108.0 MB 9.9 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 94.9/108.0 MB 9.9 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 96.7/108.0 MB 9.9 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 98.8/108.0 MB 9.9 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 100.9/108.0 MB 9.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 103.0/108.0 MB 9.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 104.9/108.0 MB 9.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  106.7/108.0 MB 9.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 108.0/108.0 MB 9.8 MB/s eta 0:00:00\n",
      "Downloading bitsandbytes-0.46.0-py3-none-win_amd64.whl (66.5 MB)\n",
      "   ---------------------------------------- 0.0/66.5 MB ? eta -:--:--\n",
      "   - -------------------------------------- 2.1/66.5 MB 10.7 MB/s eta 0:00:07\n",
      "   -- ------------------------------------- 4.2/66.5 MB 10.5 MB/s eta 0:00:06\n",
      "   --- ------------------------------------ 6.3/66.5 MB 10.4 MB/s eta 0:00:06\n",
      "   ----- ---------------------------------- 8.4/66.5 MB 10.2 MB/s eta 0:00:06\n",
      "   ------ --------------------------------- 10.5/66.5 MB 10.2 MB/s eta 0:00:06\n",
      "   ------- -------------------------------- 12.6/66.5 MB 10.2 MB/s eta 0:00:06\n",
      "   -------- ------------------------------- 14.7/66.5 MB 10.3 MB/s eta 0:00:06\n",
      "   ---------- ----------------------------- 17.0/66.5 MB 10.4 MB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 19.1/66.5 MB 10.4 MB/s eta 0:00:05\n",
      "   ------------ --------------------------- 21.2/66.5 MB 10.5 MB/s eta 0:00:05\n",
      "   -------------- ------------------------- 23.6/66.5 MB 10.5 MB/s eta 0:00:05\n",
      "   --------------- ------------------------ 26.0/66.5 MB 10.6 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 28.3/66.5 MB 10.7 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 30.7/66.5 MB 10.8 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 33.0/66.5 MB 10.8 MB/s eta 0:00:04\n",
      "   --------------------- ------------------ 35.4/66.5 MB 10.9 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 38.0/66.5 MB 11.0 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 40.6/66.5 MB 11.1 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 43.3/66.5 MB 11.2 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 45.9/66.5 MB 11.3 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 48.5/66.5 MB 11.3 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 51.4/66.5 MB 11.4 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 54.3/66.5 MB 11.6 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 57.1/66.5 MB 11.7 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 60.0/66.5 MB 11.8 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 63.2/66.5 MB 11.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  65.3/66.5 MB 11.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 66.5/66.5 MB 11.7 MB/s eta 0:00:00\n",
      "Using cached cut_cross_entropy-25.1.1-py3-none-any.whl (22 kB)\n",
      "Using cached diffusers-0.33.1-py3-none-any.whl (3.6 MB)\n",
      "Using cached hf_transfer-0.1.9-cp38-abi3-win_amd64.whl (1.2 MB)\n",
      "Downloading importlib_metadata-8.7.0-py3-none-any.whl (27 kB)\n",
      "Downloading zipp-3.23.0-py3-none-any.whl (10 kB)\n",
      "Using cached msgspec-0.19.0-cp312-cp312-win_amd64.whl (187 kB)\n",
      "Downloading networkx-3.5-py3-none-any.whl (2.0 MB)\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ------------------------------ --------- 1.6/2.0 MB 7.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.0/2.0 MB 7.1 MB/s eta 0:00:00\n",
      "Using cached pillow-11.2.1-cp312-cp312-win_amd64.whl (2.7 MB)\n",
      "Using cached torchvision-0.22.0-cp312-cp312-win_amd64.whl (1.7 MB)\n",
      "Downloading triton_windows-3.3.1.post19-cp312-cp312-win_amd64.whl (41.9 MB)\n",
      "   ---------------------------------------- 0.0/41.9 MB ? eta -:--:--\n",
      "   - -------------------------------------- 1.6/41.9 MB 8.4 MB/s eta 0:00:05\n",
      "   --- ------------------------------------ 3.1/41.9 MB 8.4 MB/s eta 0:00:05\n",
      "   ---- ----------------------------------- 5.0/41.9 MB 8.4 MB/s eta 0:00:05\n",
      "   ------ --------------------------------- 6.6/41.9 MB 8.2 MB/s eta 0:00:05\n",
      "   -------- ------------------------------- 8.4/41.9 MB 8.3 MB/s eta 0:00:05\n",
      "   --------- ------------------------------ 10.0/41.9 MB 8.4 MB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 11.8/41.9 MB 8.4 MB/s eta 0:00:04\n",
      "   ------------- -------------------------- 13.6/41.9 MB 8.5 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 15.5/41.9 MB 8.5 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 17.0/41.9 MB 8.5 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 18.9/41.9 MB 8.6 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 21.0/41.9 MB 8.7 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 22.5/41.9 MB 8.6 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 24.4/41.9 MB 8.7 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 26.2/41.9 MB 8.7 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 28.0/41.9 MB 8.7 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 29.9/41.9 MB 8.7 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 31.7/41.9 MB 8.8 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 33.8/41.9 MB 8.8 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 35.7/41.9 MB 8.8 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 37.5/41.9 MB 8.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 39.3/41.9 MB 8.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  41.4/41.9 MB 8.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 41.9/41.9 MB 8.8 MB/s eta 0:00:00\n",
      "Downloading tyro-0.9.24-py3-none-any.whl (128 kB)\n",
      "Using cached docstring_parser-0.16-py3-none-any.whl (36 kB)\n",
      "Using cached rich-14.0.0-py3-none-any.whl (243 kB)\n",
      "Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Using cached shtab-1.7.2-py3-none-any.whl (14 kB)\n",
      "Downloading typeguard-4.4.3-py3-none-any.whl (34 kB)\n",
      "Installing collected packages: sentencepiece, mpmath, zipp, wheel, typeguard, triton-windows, sympy, shtab, safetensors, regex, protobuf, pillow, networkx, msgspec, mdurl, hf_transfer, docstring-parser, torch, markdown-it-py, importlib-metadata, xformers, torchvision, tokenizers, rich, diffusers, cut_cross_entropy, bitsandbytes, accelerate, tyro, transformers, trl, peft, unsloth_zoo, unsloth\n",
      "\n",
      "   - --------------------------------------  1/34 [mpmath]\n",
      "   --- ------------------------------------  3/34 [wheel]\n",
      "   ----- ----------------------------------  5/34 [triton-windows]\n",
      "   ----- ----------------------------------  5/34 [triton-windows]\n",
      "   ----- ----------------------------------  5/34 [triton-windows]\n",
      "   ----- ----------------------------------  5/34 [triton-windows]\n",
      "   ----- ----------------------------------  5/34 [triton-windows]\n",
      "   ------- --------------------------------  6/34 [sympy]\n",
      "   ------- --------------------------------  6/34 [sympy]\n",
      "   ------- --------------------------------  6/34 [sympy]\n",
      "   ------- --------------------------------  6/34 [sympy]\n",
      "   ------- --------------------------------  6/34 [sympy]\n",
      "   ------- --------------------------------  6/34 [sympy]\n",
      "   ------- --------------------------------  6/34 [sympy]\n",
      "   ------- --------------------------------  6/34 [sympy]\n",
      "   ------- --------------------------------  6/34 [sympy]\n",
      "   ------- --------------------------------  6/34 [sympy]\n",
      "   ------- --------------------------------  6/34 [sympy]\n",
      "   ------- --------------------------------  6/34 [sympy]\n",
      "   ------- --------------------------------  6/34 [sympy]\n",
      "   ------- --------------------------------  6/34 [sympy]\n",
      "   ------- --------------------------------  6/34 [sympy]\n",
      "   ------- --------------------------------  6/34 [sympy]\n",
      "   ------- --------------------------------  6/34 [sympy]\n",
      "   ------- --------------------------------  6/34 [sympy]\n",
      "   ------- --------------------------------  6/34 [sympy]\n",
      "   ------- --------------------------------  6/34 [sympy]\n",
      "   ------- --------------------------------  6/34 [sympy]\n",
      "   ------- --------------------------------  6/34 [sympy]\n",
      "   ------- --------------------------------  6/34 [sympy]\n",
      "   ------- --------------------------------  6/34 [sympy]\n",
      "   ------- --------------------------------  6/34 [sympy]\n",
      "   ------- --------------------------------  6/34 [sympy]\n",
      "   ------- --------------------------------  6/34 [sympy]\n",
      "   ----------- ---------------------------- 10/34 [protobuf]\n",
      "   ------------ --------------------------- 11/34 [pillow]\n",
      "   -------------- ------------------------- 12/34 [networkx]\n",
      "   -------------- ------------------------- 12/34 [networkx]\n",
      "   -------------- ------------------------- 12/34 [networkx]\n",
      "   -------------- ------------------------- 12/34 [networkx]\n",
      "   -------------- ------------------------- 12/34 [networkx]\n",
      "   -------------- ------------------------- 12/34 [networkx]\n",
      "   -------------- ------------------------- 12/34 [networkx]\n",
      "   -------------------- ------------------- 17/34 [torch]\n",
      "   -------------------- ------------------- 17/34 [torch]\n",
      "   -------------------- ------------------- 17/34 [torch]\n",
      "   -------------------- ------------------- 17/34 [torch]\n",
      "   -------------------- ------------------- 17/34 [torch]\n",
      "   -------------------- ------------------- 17/34 [torch]\n",
      "   -------------------- ------------------- 17/34 [torch]\n",
      "   -------------------- ------------------- 17/34 [torch]\n",
      "   -------------------- ------------------- 17/34 [torch]\n",
      "   -------------------- ------------------- 17/34 [torch]\n",
      "   -------------------- ------------------- 17/34 [torch]\n",
      "   -------------------- ------------------- 17/34 [torch]\n",
      "   -------------------- ------------------- 17/34 [torch]\n",
      "   -------------------- ------------------- 17/34 [torch]\n",
      "   -------------------- ------------------- 17/34 [torch]\n",
      "   -------------------- ------------------- 17/34 [torch]\n",
      "   -------------------- ------------------- 17/34 [torch]\n",
      "   -------------------- ------------------- 17/34 [torch]\n",
      "   -------------------- ------------------- 17/34 [torch]\n",
      "   -------------------- ------------------- 17/34 [torch]\n",
      "   -------------------- ------------------- 17/34 [torch]\n",
      "   -------------------- ------------------- 17/34 [torch]\n",
      "   -------------------- ------------------- 17/34 [torch]\n",
      "   -------------------- ------------------- 17/34 [torch]\n",
      "   -------------------- ------------------- 17/34 [torch]\n",
      "   -------------------- ------------------- 17/34 [torch]\n",
      "   -------------------- ------------------- 17/34 [torch]\n",
      "   -------------------- ------------------- 17/34 [torch]\n",
      "   -------------------- ------------------- 17/34 [torch]\n",
      "   -------------------- ------------------- 17/34 [torch]\n",
      "   -------------------- ------------------- 17/34 [torch]\n",
      "   -------------------- ------------------- 17/34 [torch]\n",
      "   -------------------- ------------------- 17/34 [torch]\n",
      "   -------------------- ------------------- 17/34 [torch]\n",
      "   -------------------- ------------------- 17/34 [torch]\n",
      "   -------------------- ------------------- 17/34 [torch]\n",
      "   -------------------- ------------------- 17/34 [torch]\n",
      "   -------------------- ------------------- 17/34 [torch]\n",
      "   -------------------- ------------------- 17/34 [torch]\n",
      "   -------------------- ------------------- 17/34 [torch]\n",
      "   -------------------- ------------------- 17/34 [torch]\n",
      "   -------------------- ------------------- 17/34 [torch]\n",
      "   -------------------- ------------------- 17/34 [torch]\n",
      "   -------------------- ------------------- 17/34 [torch]\n",
      "   -------------------- ------------------- 17/34 [torch]\n",
      "   -------------------- ------------------- 17/34 [torch]\n",
      "   -------------------- ------------------- 17/34 [torch]\n",
      "   -------------------- ------------------- 17/34 [torch]\n",
      "   -------------------- ------------------- 17/34 [torch]\n",
      "   -------------------- ------------------- 17/34 [torch]\n",
      "   -------------------- ------------------- 17/34 [torch]\n",
      "   -------------------- ------------------- 17/34 [torch]\n",
      "   -------------------- ------------------- 17/34 [torch]\n",
      "   -------------------- ------------------- 17/34 [torch]\n",
      "   -------------------- ------------------- 17/34 [torch]\n",
      "   -------------------- ------------------- 17/34 [torch]\n",
      "   -------------------- ------------------- 17/34 [torch]\n",
      "   -------------------- ------------------- 17/34 [torch]\n",
      "   -------------------- ------------------- 17/34 [torch]\n",
      "   -------------------- ------------------- 17/34 [torch]\n",
      "   -------------------- ------------------- 17/34 [torch]\n",
      "   -------------------- ------------------- 17/34 [torch]\n",
      "   -------------------- ------------------- 17/34 [torch]\n",
      "   -------------------- ------------------- 17/34 [torch]\n",
      "   -------------------- ------------------- 17/34 [torch]\n",
      "   -------------------- ------------------- 17/34 [torch]\n",
      "   -------------------- ------------------- 17/34 [torch]\n",
      "   -------------------- ------------------- 17/34 [torch]\n",
      "   -------------------- ------------------- 17/34 [torch]\n",
      "   -------------------- ------------------- 17/34 [torch]\n",
      "   -------------------- ------------------- 17/34 [torch]\n",
      "   -------------------- ------------------- 17/34 [torch]\n",
      "   -------------------- ------------------- 17/34 [torch]\n",
      "   -------------------- ------------------- 17/34 [torch]\n",
      "   -------------------- ------------------- 17/34 [torch]\n",
      "   -------------------- ------------------- 17/34 [torch]\n",
      "   -------------------- ------------------- 17/34 [torch]\n",
      "   -------------------- ------------------- 17/34 [torch]\n",
      "   -------------------- ------------------- 17/34 [torch]\n",
      "   -------------------- ------------------- 17/34 [torch]\n",
      "   -------------------- ------------------- 17/34 [torch]\n",
      "   -------------------- ------------------- 17/34 [torch]\n",
      "   -------------------- ------------------- 17/34 [torch]\n",
      "   -------------------- ------------------- 17/34 [torch]\n",
      "   -------------------- ------------------- 17/34 [torch]\n",
      "   -------------------- ------------------- 17/34 [torch]\n",
      "   -------------------- ------------------- 17/34 [torch]\n",
      "   -------------------- ------------------- 17/34 [torch]\n",
      "   -------------------- ------------------- 17/34 [torch]\n",
      "   -------------------- ------------------- 17/34 [torch]\n",
      "   -------------------- ------------------- 17/34 [torch]\n",
      "   -------------------- ------------------- 17/34 [torch]\n",
      "   -------------------- ------------------- 17/34 [torch]\n",
      "   -------------------- ------------------- 17/34 [torch]\n",
      "   -------------------- ------------------- 17/34 [torch]\n",
      "   -------------------- ------------------- 17/34 [torch]\n",
      "   -------------------- ------------------- 17/34 [torch]\n",
      "   -------------------- ------------------- 17/34 [torch]\n",
      "   -------------------- ------------------- 17/34 [torch]\n",
      "   -------------------- ------------------- 17/34 [torch]\n",
      "   -------------------- ------------------- 17/34 [torch]\n",
      "   --------------------- ------------------ 18/34 [markdown-it-py]\n",
      "   ----------------------- ---------------- 20/34 [xformers]\n",
      "   ----------------------- ---------------- 20/34 [xformers]\n",
      "   ----------------------- ---------------- 20/34 [xformers]\n",
      "   ----------------------- ---------------- 20/34 [xformers]\n",
      "   ----------------------- ---------------- 20/34 [xformers]\n",
      "   ----------------------- ---------------- 20/34 [xformers]\n",
      "   ----------------------- ---------------- 20/34 [xformers]\n",
      "   ------------------------ --------------- 21/34 [torchvision]\n",
      "   ------------------------ --------------- 21/34 [torchvision]\n",
      "   ------------------------ --------------- 21/34 [torchvision]\n",
      "   ------------------------ --------------- 21/34 [torchvision]\n",
      "   --------------------------- ------------ 23/34 [rich]\n",
      "   ---------------------------- ----------- 24/34 [diffusers]\n",
      "   ---------------------------- ----------- 24/34 [diffusers]\n",
      "   ---------------------------- ----------- 24/34 [diffusers]\n",
      "   ---------------------------- ----------- 24/34 [diffusers]\n",
      "   ---------------------------- ----------- 24/34 [diffusers]\n",
      "   ---------------------------- ----------- 24/34 [diffusers]\n",
      "   ---------------------------- ----------- 24/34 [diffusers]\n",
      "   ---------------------------- ----------- 24/34 [diffusers]\n",
      "   ---------------------------- ----------- 24/34 [diffusers]\n",
      "   ---------------------------- ----------- 24/34 [diffusers]\n",
      "   ---------------------------- ----------- 24/34 [diffusers]\n",
      "   ---------------------------- ----------- 24/34 [diffusers]\n",
      "   ---------------------------- ----------- 24/34 [diffusers]\n",
      "   ---------------------------- ----------- 24/34 [diffusers]\n",
      "   ---------------------------- ----------- 24/34 [diffusers]\n",
      "   ---------------------------- ----------- 24/34 [diffusers]\n",
      "   ---------------------------- ----------- 24/34 [diffusers]\n",
      "   ---------------------------- ----------- 24/34 [diffusers]\n",
      "   ---------------------------- ----------- 24/34 [diffusers]\n",
      "   ---------------------------- ----------- 24/34 [diffusers]\n",
      "   ---------------------------- ----------- 24/34 [diffusers]\n",
      "   ---------------------------- ----------- 24/34 [diffusers]\n",
      "   ---------------------------- ----------- 24/34 [diffusers]\n",
      "   ---------------------------- ----------- 24/34 [diffusers]\n",
      "   ----------------------------- ---------- 25/34 [cut_cross_entropy]\n",
      "   ------------------------------ --------- 26/34 [bitsandbytes]\n",
      "   ------------------------------ --------- 26/34 [bitsandbytes]\n",
      "   ------------------------------ --------- 26/34 [bitsandbytes]\n",
      "   ------------------------------ --------- 26/34 [bitsandbytes]\n",
      "   ------------------------------- -------- 27/34 [accelerate]\n",
      "   ------------------------------- -------- 27/34 [accelerate]\n",
      "   ---------------------------------- ----- 29/34 [transformers]\n",
      "   ---------------------------------- ----- 29/34 [transformers]\n",
      "   ---------------------------------- ----- 29/34 [transformers]\n",
      "   ---------------------------------- ----- 29/34 [transformers]\n",
      "   ---------------------------------- ----- 29/34 [transformers]\n",
      "   ---------------------------------- ----- 29/34 [transformers]\n",
      "   ---------------------------------- ----- 29/34 [transformers]\n",
      "   ---------------------------------- ----- 29/34 [transformers]\n",
      "   ---------------------------------- ----- 29/34 [transformers]\n",
      "   ---------------------------------- ----- 29/34 [transformers]\n",
      "   ---------------------------------- ----- 29/34 [transformers]\n",
      "   ---------------------------------- ----- 29/34 [transformers]\n",
      "   ---------------------------------- ----- 29/34 [transformers]\n",
      "   ---------------------------------- ----- 29/34 [transformers]\n",
      "   ---------------------------------- ----- 29/34 [transformers]\n",
      "   ---------------------------------- ----- 29/34 [transformers]\n",
      "   ---------------------------------- ----- 29/34 [transformers]\n",
      "   ---------------------------------- ----- 29/34 [transformers]\n",
      "   ---------------------------------- ----- 29/34 [transformers]\n",
      "   ---------------------------------- ----- 29/34 [transformers]\n",
      "   ---------------------------------- ----- 29/34 [transformers]\n",
      "   ---------------------------------- ----- 29/34 [transformers]\n",
      "   ---------------------------------- ----- 29/34 [transformers]\n",
      "   ---------------------------------- ----- 29/34 [transformers]\n",
      "   ---------------------------------- ----- 29/34 [transformers]\n",
      "   ---------------------------------- ----- 29/34 [transformers]\n",
      "   ---------------------------------- ----- 29/34 [transformers]\n",
      "   ---------------------------------- ----- 29/34 [transformers]\n",
      "   ---------------------------------- ----- 29/34 [transformers]\n",
      "   ---------------------------------- ----- 29/34 [transformers]\n",
      "   ---------------------------------- ----- 29/34 [transformers]\n",
      "   ---------------------------------- ----- 29/34 [transformers]\n",
      "   ---------------------------------- ----- 29/34 [transformers]\n",
      "   ---------------------------------- ----- 29/34 [transformers]\n",
      "   ---------------------------------- ----- 29/34 [transformers]\n",
      "   ---------------------------------- ----- 29/34 [transformers]\n",
      "   ---------------------------------- ----- 29/34 [transformers]\n",
      "   ---------------------------------- ----- 29/34 [transformers]\n",
      "   ---------------------------------- ----- 29/34 [transformers]\n",
      "   ---------------------------------- ----- 29/34 [transformers]\n",
      "   ---------------------------------- ----- 29/34 [transformers]\n",
      "   ---------------------------------- ----- 29/34 [transformers]\n",
      "   ---------------------------------- ----- 29/34 [transformers]\n",
      "   ---------------------------------- ----- 29/34 [transformers]\n",
      "   ---------------------------------- ----- 29/34 [transformers]\n",
      "   ---------------------------------- ----- 29/34 [transformers]\n",
      "   ---------------------------------- ----- 29/34 [transformers]\n",
      "   ---------------------------------- ----- 29/34 [transformers]\n",
      "   ---------------------------------- ----- 29/34 [transformers]\n",
      "   ---------------------------------- ----- 29/34 [transformers]\n",
      "   ---------------------------------- ----- 29/34 [transformers]\n",
      "   ---------------------------------- ----- 29/34 [transformers]\n",
      "   ---------------------------------- ----- 29/34 [transformers]\n",
      "   ----------------------------------- ---- 30/34 [trl]\n",
      "   ------------------------------------ --- 31/34 [peft]\n",
      "   ------------------------------------ --- 31/34 [peft]\n",
      "   -------------------------------------- - 33/34 [unsloth]\n",
      "   -------------------------------------- - 33/34 [unsloth]\n",
      "   ---------------------------------------- 34/34 [unsloth]\n",
      "\n",
      "Successfully installed accelerate-1.7.0 bitsandbytes-0.46.0 cut_cross_entropy-25.1.1 diffusers-0.33.1 docstring-parser-0.16 hf_transfer-0.1.9 importlib-metadata-8.7.0 markdown-it-py-3.0.0 mdurl-0.1.2 mpmath-1.3.0 msgspec-0.19.0 networkx-3.5 peft-0.15.2 pillow-11.2.1 protobuf-3.20.3 regex-2024.11.6 rich-14.0.0 safetensors-0.5.3 sentencepiece-0.2.0 shtab-1.7.2 sympy-1.14.0 tokenizers-0.21.1 torch-2.7.0 torchvision-0.22.0 transformers-4.52.4 triton-windows-3.3.1.post19 trl-0.18.1 typeguard-4.4.3 tyro-0.9.24 unsloth-2025.6.1 unsloth_zoo-2025.6.1 wheel-0.45.1 xformers-0.0.30 zipp-3.23.0\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T13:07:39.925075Z",
     "start_time": "2025-06-09T13:07:38.702522Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "def check_gpu_availability() -> None:\n",
    "    \"\"\"\n",
    "    Checks for the availability of a CUDA-enabled GPU and prints its status.\n",
    "    \"\"\"\n",
    "    # Check if a CUDA-enabled GPU is available\n",
    "    is_gpu_available: bool = torch.cuda.is_available()\n",
    "\n",
    "    if is_gpu_available:\n",
    "        # Get the number of available GPUs\n",
    "        gpu_count: int = torch.cuda.device_count()\n",
    "        # Get the name of the current GPU\n",
    "        current_gpu_name: str = torch.cuda.get_device_name(torch.cuda.current_device())\n",
    "        print(f\"✅ GPU доступен.\")\n",
    "        print(f\"Количество GPU: {gpu_count}\")\n",
    "        print(f\"Имя устройства: {current_gpu_name}\")\n",
    "    else:\n",
    "        print(\"❌ GPU не доступен. PyTorch будет использовать CPU.\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    check_gpu_availability()"
   ],
   "id": "d9bd7c0a6fc467d2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ GPU доступен.\n",
      "Количество GPU: 1\n",
      "Имя устройства: NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T13:06:51.218312Z",
     "start_time": "2025-06-09T13:06:50.352971Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "2fe71776c703b662",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
      "Requirement already satisfied: torch==2.6.0 in c:\\my_projects\\feelent\\.venv\\lib\\site-packages (2.6.0+cu118)\n",
      "Requirement already satisfied: torchvision==0.21.0 in c:\\my_projects\\feelent\\.venv\\lib\\site-packages (0.21.0+cu118)\n",
      "Requirement already satisfied: torchaudio==2.6.0 in c:\\my_projects\\feelent\\.venv\\lib\\site-packages (2.6.0+cu118)\n",
      "Requirement already satisfied: filelock in c:\\my_projects\\feelent\\.venv\\lib\\site-packages (from torch==2.6.0) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\my_projects\\feelent\\.venv\\lib\\site-packages (from torch==2.6.0) (4.14.0)\n",
      "Requirement already satisfied: networkx in c:\\my_projects\\feelent\\.venv\\lib\\site-packages (from torch==2.6.0) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\my_projects\\feelent\\.venv\\lib\\site-packages (from torch==2.6.0) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\my_projects\\feelent\\.venv\\lib\\site-packages (from torch==2.6.0) (2025.3.0)\n",
      "Requirement already satisfied: setuptools in c:\\my_projects\\feelent\\.venv\\lib\\site-packages (from torch==2.6.0) (80.9.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\my_projects\\feelent\\.venv\\lib\\site-packages (from torch==2.6.0) (1.13.1)\n",
      "Requirement already satisfied: numpy in c:\\my_projects\\feelent\\.venv\\lib\\site-packages (from torchvision==0.21.0) (2.3.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\my_projects\\feelent\\.venv\\lib\\site-packages (from torchvision==0.21.0) (11.2.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\my_projects\\feelent\\.venv\\lib\\site-packages (from sympy==1.13.1->torch==2.6.0) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\my_projects\\feelent\\.venv\\lib\\site-packages (from jinja2->torch==2.6.0) (3.0.2)\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from typing import List, Dict, Any, Tuple, Optional\n",
    "\n",
    "# Unsloth, Transformers, TRL and PEFT imports\n",
    "from unsloth import FastLanguageModel\n",
    "from transformers import TrainingArguments, PreTrainedModel\n",
    "from transformers.modeling_outputs import CausalLMOutputWithPast\n",
    "import torch.nn as nn\n",
    "from trl import SFTTrainer\n",
    "from peft import LoraConfig\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. MODEL LOADING (Updated for Qwen3 1.7B)\n",
    "# ==============================================================================\n",
    "# We will load the 4-bit quantized version of Qwen3-1.7B-Instruct from Unsloth.\n",
    "# Create a folder named 'model_cache' in your project directory\n",
    "model_cache_path: str = \"./model_cache\"\n",
    "\n",
    "max_seq_length: int = 2048\n",
    "dtype = None # Let Unsloth auto-select the best dtype (float16 or bfloat16)\n",
    "load_in_4bit: bool = True\n",
    "\n",
    "print(\"==> Step 1: Loading the Qwen3-1.7B model...\")\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/qwen3-1.7b-instruct-bnb-4bit\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    cache_dir = model_cache_path, # <-- ВОТ ЭТОТ ПАРАМЕТР\n",
    ")\n",
    "print(\"==> Model loaded successfully!\\n\")\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. CUSTOM MODEL WRAPPER (No changes needed)\n",
    "# ==============================================================================\n",
    "# This wrapper class is generic and works with any model.\n",
    "\n",
    "class ConditionalLM(PreTrainedModel):\n",
    "    \"\"\"\n",
    "    A custom model that wraps a pre-trained language model and adds a conditional projection layer.\n",
    "    \"\"\"\n",
    "    supports_gradient_checkpointing = True\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        language_model: PreTrainedModel,\n",
    "        custom_vector_size: int\n",
    "    ):\n",
    "        super().__init__(language_model.config)\n",
    "        self.language_model = language_model\n",
    "        self.custom_vector_size = custom_vector_size\n",
    "        self.embedding_size = self.language_model.get_input_embeddings().embedding_dim\n",
    "        self.projection_layer = nn.Sequential(\n",
    "            nn.Linear(self.custom_vector_size, self.embedding_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.embedding_size, self.embedding_size)\n",
    "        )\n",
    "\n",
    "    def get_input_embeddings(self) -> nn.Embedding:\n",
    "        return self.language_model.get_input_embeddings()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.LongTensor] = None,\n",
    "        attention_mask: Optional[torch.LongTensor] = None,\n",
    "        custom_vector: Optional[torch.Tensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        **kwargs,\n",
    "    ) -> CausalLMOutputWithPast:\n",
    "        if custom_vector is None:\n",
    "            return self.language_model(\n",
    "                input_ids=input_ids, attention_mask=attention_mask, labels=labels,\n",
    "                inputs_embeds=inputs_embeds, **kwargs\n",
    "            )\n",
    "        projected_vector = self.projection_layer(custom_vector).unsqueeze(1)\n",
    "        if inputs_embeds is None:\n",
    "            inputs_embeds = self.get_input_embeddings()(input_ids)\n",
    "        inputs_embeds = torch.cat([projected_vector, inputs_embeds], dim=1)\n",
    "        new_attention_mask = None\n",
    "        if attention_mask is not None:\n",
    "            projected_vector_mask = torch.ones(\n",
    "                attention_mask.shape[0], 1, dtype=attention_mask.dtype, device=attention_mask.device\n",
    "            )\n",
    "            new_attention_mask = torch.cat([projected_vector_mask, attention_mask], dim=1)\n",
    "        new_labels = None\n",
    "        if labels is not None:\n",
    "            projected_vector_label = torch.full(\n",
    "                (labels.shape[0], 1), -100, dtype=labels.dtype, device=labels.device\n",
    "            )\n",
    "            new_labels = torch.cat([projected_vector_label, labels], dim=1)\n",
    "        return self.language_model(\n",
    "            inputs_embeds=inputs_embeds, attention_mask=new_attention_mask,\n",
    "            labels=new_labels, **kwargs\n",
    "        )\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. DATA GENERATION AND PREPARATION (No changes needed)\n",
    "# ==============================================================================\n",
    "# The data generation and formatting functions remain the same.\n",
    "\n",
    "def generate_synthetic_dataset(number_of_records: int, vector_dimension: int) -> Dataset:\n",
    "    # (The function body is the same as before, so it is omitted here for brevity)\n",
    "    if vector_dimension % 3 != 0: raise ValueError(\"vector_dimension must be divisible by 3.\")\n",
    "    source_data: Dict[str, List[Tuple[str, str]]] = {\n",
    "        \"science\": [(\"Что такое черная дыра?\", \"Чёрная дыра — это область пространства-времени, гравитационное притяжение которой настолько велико, что покинуть её не могут даже объекты, движущиеся со скоростью света.\"), (\"Объясни фотосинтез.\", \"Фотосинтез — это сложный химический процесс преобразования энергии видимого света в энергию химических связей органических веществ.\"),],\n",
    "        \"history\": [(\"Расскажи о Ренессансе.\", \"Эпоха Возрождения, или Ренессанс, — это период в истории культуры Европы, пришедший на смену Средним векам и предшествующий Просвещению.\"), (\"Кто такой Юлий Цезарь?\", \"Гай Юлий Цезарь был древнеримским государственным и политическим деятелем, полководцем и писателем.\"),],\n",
    "        \"creative\": [(\"Придумай шутку про программиста.\", \"Почему программисты так не любят природу? Слишком много багов.\"), (\"Напиши короткий стих о космосе.\", \"Средь миллиардов звёздных троп, летит бесшумно телескоп. Он ищет дом, он ищет свет, вдали от суетных планет.\"),],\n",
    "    }\n",
    "    records_list: List[Dict[str, Any]] = []\n",
    "    categories: List[str] = list(source_data.keys())\n",
    "    chunk_size: int = vector_dimension // 3\n",
    "    for _ in range(number_of_records):\n",
    "        chosen_category: str = random.choice(categories)\n",
    "        prompt, response = random.choice(source_data[chosen_category])\n",
    "        custom_vector = np.zeros(vector_dimension, dtype=np.float32)\n",
    "        for i in range(3):\n",
    "            start_index, end_index = i * chunk_size, (i + 1) * chunk_size\n",
    "            custom_vector[start_index:end_index] = np.random.uniform(0.0, 0.2, size=chunk_size)\n",
    "        category_index = categories.index(chosen_category)\n",
    "        start_index, end_index = category_index * chunk_size, (category_index + 1) * chunk_size\n",
    "        custom_vector[start_index:end_index] = np.random.uniform(0.8, 1.0, size=chunk_size)\n",
    "        records_list.append({\"prompt\": prompt, \"response\": response, \"custom_vector\": custom_vector})\n",
    "    return Dataset.from_list(records_list)\n",
    "\n",
    "def formatting_prompts_func(example: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    text_parts = [\n",
    "        f\"<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n\",\n",
    "        f\"<|im_start|>user\\n{example['prompt']}<|im_end|>\\n\",\n",
    "        f\"<|im_start|>assistant\\n{example['response']}<|im_end|>\"\n",
    "    ]\n",
    "    example[\"text\"] = \"\".join(text_parts) + tokenizer.eos_token\n",
    "    return example\n",
    "\n",
    "class ConditionalDataCollator:\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "        tokenized_inputs = self.tokenizer(\n",
    "            [f[\"text\"] for f in features], return_tensors=\"pt\", padding=True,\n",
    "            truncation=True, max_length=max_seq_length\n",
    "        )\n",
    "        custom_vectors = torch.tensor([f[\"custom_vector\"] for f in features], dtype=torch.float)\n",
    "        tokenized_inputs[\"labels\"] = tokenized_inputs[\"input_ids\"].clone()\n",
    "        tokenized_inputs[\"custom_vector\"] = custom_vectors\n",
    "        return tokenized_inputs\n",
    "\n",
    "print(\"==> Step 3: Generating and preparing dataset...\")\n",
    "NUMBER_OF_RECORDS = 200\n",
    "VECTOR_DIMENSION = 30\n",
    "synthetic_dataset = generate_synthetic_dataset(\n",
    "    number_of_records=NUMBER_OF_RECORDS, vector_dimension=VECTOR_DIMENSION\n",
    ")\n",
    "processed_dataset = synthetic_dataset.map(formatting_prompts_func, num_proc=4)\n",
    "print(\"==> Dataset prepared successfully!\\n\")\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 4. TRAINING SETUP (Updated for Qwen3)\n",
    "# ==============================================================================\n",
    "print(\"==> Step 4: Setting up the training components...\")\n",
    "custom_model = ConditionalLM(language_model=model, custom_vector_size=VECTOR_DIMENSION)\n",
    "\n",
    "# LoRA configuration for Qwen3\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    # NOTE: We assume these are the correct modules for Qwen3,\n",
    "    # as they are standard for Qwen1.5 and Qwen2. This is an educated guess.\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    modules_to_save=[\"projection_layer\"], # Don't forget our custom layer!\n",
    ")\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=\"qwen3_1.7b_conditional_finetune\", # <-- Updated output directory\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    warmup_steps=5,\n",
    "    max_steps=100,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=not torch.cuda.is_bf16_supported(),\n",
    "    bf16=torch.cuda.is_bf16_supported(),\n",
    "    logging_steps=1,\n",
    "    optim=\"adamw_8bit\",\n",
    "    weight_decay=0.01,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "data_collator = ConditionalDataCollator(tokenizer=tokenizer)\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=custom_model,\n",
    "    args=training_arguments,\n",
    "    train_dataset=processed_dataset,\n",
    "    peft_config=lora_config,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "print(\"==> Trainer is ready for Qwen3 1.7B!\\n\")\n",
    "print(\"To start training, run the command: trainer.train()\")"
   ],
   "id": "1f8feba43ec693f6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "44ded371243c612f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import numpy as np\n",
    "import copy\n",
    "from datasets import Dataset\n",
    "from typing import List, Dict, Any, Tuple, Optional\n",
    "from enum import Enum\n",
    "\n",
    "# TRL, Transformers, Unsloth imports\n",
    "from unsloth import FastLanguageModel\n",
    "from transformers import PreTrainedModel, AutoTokenizer\n",
    "from trl import PPOTrainer, PPOConfig\n",
    "from trl.core import LengthSampler\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. Core Classes (ConditionalLM and InjectionMethod Enum)\n",
    "# ==============================================================================\n",
    "# These are the same classes we finalized before.\n",
    "\n",
    "class InjectionMethod(Enum):\n",
    "    PREPEND_EMBEDDING = \"prepend_embedding\"\n",
    "    ADD_AFTER_LAYER_N = \"add_after_layer_n\"\n",
    "    ADD_TO_EVERY_LAYER = \"add_to_every_layer\"\n",
    "\n",
    "class ConditionalLM(PreTrainedModel):\n",
    "    supports_gradient_checkpointing = True\n",
    "    def __init__(self, language_model: PreTrainedModel, custom_vector_size: int,\n",
    "                 injection_method: InjectionMethod = InjectionMethod.PREPEND_EMBEDDING,\n",
    "                 injection_layer_index: Optional[int] = None):\n",
    "        super().__init__(language_model.config)\n",
    "        self.language_model = language_model\n",
    "        self.custom_vector_size = custom_vector_size\n",
    "        self.injection_method = injection_method\n",
    "        self.injection_layer_index = injection_layer_index\n",
    "        self.embedding_size = self.language_model.get_input_embeddings().embedding_dim\n",
    "        self._validate_settings()\n",
    "        self.projection_layer = nn.Sequential(nn.Linear(self.custom_vector_size, self.embedding_size), nn.ReLU(), nn.Linear(self.embedding_size, self.embedding_size))\n",
    "        self.projected_vector_cache: Optional[torch.Tensor] = None\n",
    "        self._register_hooks()\n",
    "    def _validate_settings(self):\n",
    "        if self.injection_method == InjectionMethod.ADD_AFTER_LAYER_N:\n",
    "            if self.injection_layer_index is None: raise ValueError(\"`injection_layer_index` must be set.\")\n",
    "            num_layers = len(self.language_model.model.layers)\n",
    "            if not (0 <= self.injection_layer_index < num_layers): raise ValueError(f\"`injection_layer_index` must be between 0 and {num_layers - 1}.\")\n",
    "    def _register_hooks(self):\n",
    "        if self.injection_method == InjectionMethod.ADD_TO_EVERY_LAYER:\n",
    "            for layer in self.language_model.model.layers: layer.register_forward_hook(self._addition_hook)\n",
    "        elif self.injection_method == InjectionMethod.ADD_AFTER_LAYER_N:\n",
    "            self.language_model.model.layers[self.injection_layer_index].register_forward_hook(self._addition_hook)\n",
    "    def _addition_hook(self, module: nn.Module, inputs: Any, outputs: Any) -> Any:\n",
    "        hidden_states = outputs[0]\n",
    "        modified_hidden_states = hidden_states + self.projected_vector_cache.unsqueeze(1)\n",
    "        return (modified_hidden_states,) + outputs[1:]\n",
    "    def forward(self, input_ids: Optional[torch.LongTensor] = None, custom_vector: Optional[torch.Tensor] = None, **kwargs):\n",
    "        if custom_vector is None: return self.language_model(input_ids=input_ids, **kwargs)\n",
    "        if self.injection_method == InjectionMethod.PREPEND_EMBEDDING:\n",
    "            projected_vector = self.projection_layer(custom_vector).unsqueeze(1)\n",
    "            token_embeddings = self.get_input_embeddings()(input_ids)\n",
    "            inputs_embeds = torch.cat([projected_vector, token_embeddings], dim=1)\n",
    "            attention_mask, labels = kwargs.get(\"attention_mask\"), kwargs.get(\"labels\")\n",
    "            if attention_mask is not None:\n",
    "                proj_mask = torch.ones(attention_mask.shape[0], 1, dtype=attention_mask.dtype, device=attention_mask.device)\n",
    "                kwargs[\"attention_mask\"] = torch.cat([proj_mask, attention_mask], dim=1)\n",
    "            if labels is not None:\n",
    "                proj_label = torch.full((labels.shape[0], 1), -100, dtype=labels.dtype, device=labels.device)\n",
    "                kwargs[\"labels\"] = torch.cat([proj_label, labels], dim=1)\n",
    "            return self.language_model(inputs_embeds=inputs_embeds, **kwargs)\n",
    "        elif self.injection_method in [InjectionMethod.ADD_TO_EVERY_LAYER, InjectionMethod.ADD_AFTER_LAYER_N]:\n",
    "            self.projected_vector_cache = self.projection_layer(custom_vector)\n",
    "            outputs = self.language_model(input_ids=input_ids, **kwargs)\n",
    "            self.projected_vector_cache = None\n",
    "            return outputs\n",
    "        else: raise NotImplementedError(f\"Injection method {self.injection_method} is not implemented.\")\n",
    "    def get_input_embeddings(self) -> nn.Embedding: return self.language_model.get_input_embeddings()\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. NEW: Placeholder Reward Model (The Supervisor)\n",
    "# ==============================================================================\n",
    "# In a real project, this would be a separately trained model.\n",
    "# Here, it just returns a random score for any given input.\n",
    "\n",
    "def get_rewards_from_api(\n",
    "    prompts: List[str],\n",
    "    responses: List[str],\n",
    "    api_url: str,\n",
    "    api_token: str,\n",
    "    device: str\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Gets rewards for a batch of prompts and responses from an external API.\n",
    "\n",
    "    Args:\n",
    "        prompts (List[str]): A list of prompts sent to the policy model.\n",
    "        responses (List[str]): A list of responses generated by the policy model.\n",
    "        api_url (str): The endpoint URL of the external reward model.\n",
    "        api_token (str): The authentication token for the API.\n",
    "        device (str): The torch device to place the resulting tensor on.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: A tensor of shape (batch_size, 1) with rewards.\n",
    "    \"\"\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {api_token}\",\n",
    "        \"Content-Type\": \"application/json\",\n",
    "    }\n",
    "\n",
    "    # We assume the API can handle batch requests for efficiency.\n",
    "    # The payload is a list of objects, each with a prompt and a response.\n",
    "    payload = {\n",
    "        \"inputs\": [\n",
    "            {\"prompt\": p, \"response\": r} for p, r in zip(prompts, responses)\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    rewards = []\n",
    "    try:\n",
    "        # Make the POST request to the external API\n",
    "        response = requests.post(api_url, headers=headers, json=payload)\n",
    "        response.raise_for_status()  # Raise an exception for bad status codes (4xx or 5xx)\n",
    "\n",
    "        # Parse the JSON response\n",
    "        results = response.json()\n",
    "\n",
    "        # We expect the API to return a list of scores\n",
    "        # Example format: {\"scores\": [0.8, 0.2, 0.9, 0.5]}\n",
    "        scores = results.get(\"scores\", [])\n",
    "        if len(scores) != len(prompts):\n",
    "             raise ValueError(\"API returned a different number of scores than expected.\")\n",
    "\n",
    "        rewards = [torch.tensor(score) for score in scores]\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error calling external API: {e}\")\n",
    "        # On error, return a neutral reward (0.0) for the whole batch\n",
    "        rewards = [torch.tensor(0.0) for _ in prompts]\n",
    "\n",
    "    # The PPOTrainer expects a tensor of shape (batch_size,).\n",
    "    return torch.tensor(rewards, dtype=torch.float32, device=device)\n",
    "\n",
    "class PlaceholderRewardModel(nn.Module):\n",
    "    \"\"\"\n",
    "    A placeholder for a real reward model. It returns a random scalar reward.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name_or_path: str):\n",
    "        super().__init__()\n",
    "        # In a real scenario, you might load a model with a scalar output head.\n",
    "        # For this example, we don't need to load anything.\n",
    "        print(f\"Initialized PlaceholderRewardModel. It will return random rewards.\")\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor,\n",
    "        **kwargs,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_ids (torch.LongTensor): Tokenized sequence of (prompt + response).\n",
    "        Returns:\n",
    "            torch.Tensor: A tensor of shape (batch_size, 1) with random rewards.\n",
    "        \"\"\"\n",
    "        batch_size = input_ids.shape[0]\n",
    "        # Return a random reward for each item in the batch\n",
    "        return torch.randn(batch_size, 1, device=input_ids.device)\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. Data Generation for PPO\n",
    "# ==============================================================================\n",
    "# For PPO, we only need prompts and their corresponding vectors to start generation.\n",
    "\n",
    "def generate_ppo_dataset(number_of_prompts: int, vector_dimension: int) -> Dataset:\n",
    "    \"\"\"\n",
    "    Generates a synthetic dataset of prompts and custom vectors for PPO.\n",
    "    \"\"\"\n",
    "    prompts = [\n",
    "        \"Что такое черная дыра?\", \"Расскажи о Ренессансе.\", \"Придумай шутку про программиста.\",\n",
    "        \"Объясни фотосинтез.\", \"Кто такой Юлий Цезарь?\", \"Напиши короткий стих о космосе.\"\n",
    "    ]\n",
    "    records_list = []\n",
    "    for i in range(number_of_prompts):\n",
    "        records_list.append({\n",
    "            \"prompt\": random.choice(prompts),\n",
    "            # In a real case, the vector would be meaningful. Here, it's random.\n",
    "            \"custom_vector\": np.random.randn(vector_dimension).astype(np.float32)\n",
    "        })\n",
    "    return Dataset.from_list(records_list)\n",
    "\n",
    "# ==============================================================================\n",
    "# 4. Main PPO Script Setup\n",
    "# ==============================================================================\n",
    "\n",
    "# --- Configuration ---\n",
    "SFT_MODEL_PATH = \"unsloth/qwen3-1.7b-instruct-bnb-4bit\" # Path to your SFT model\n",
    "BASE_MODEL_NAME = \"unsloth/qwen3-1.7b-instruct-bnb-4bit\"\n",
    "VECTOR_DIMENSION = 30\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "ppo_config = PPOConfig(\n",
    "    batch_size=4,\n",
    "    mini_batch_size=2,\n",
    "    gradient_accumulation_steps=1,\n",
    "    learning_rate=1.4e-6, # Use a very low learning rate for PPO\n",
    "    log_with=\"wandb\", # or \"tensorboard\" or None\n",
    ")\n",
    "\n",
    "# --- Load Base Model and Tokenizer ---\n",
    "print(\"Loading base model and tokenizer...\")\n",
    "base_model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=BASE_MODEL_NAME,\n",
    "    max_seq_length=512,\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "# Qwen models need a padding token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "# --- Initialize Models for PPO ---\n",
    "print(\"Initializing Policy, Reference, and Reward Models...\")\n",
    "\n",
    "# In a real workflow, you would load the weights from your SFT training here.\n",
    "# For this example, we start from the base model.\n",
    "policy_model = ConditionalLM(\n",
    "    language_model=base_model,\n",
    "    custom_vector_size=VECTOR_DIMENSION,\n",
    "    injection_method=InjectionMethod.PREPEND_EMBEDDING,\n",
    ").to(DEVICE)\n",
    "\n",
    "# The reference model is a frozen copy of the policy model before PPO training.\n",
    "ref_model = copy.deepcopy(policy_model)\n",
    "for param in ref_model.parameters():\n",
    "    param.requires_grad = False\n",
    "ref_model.eval()\n",
    "\n",
    "# Our placeholder reward model\n",
    "reward_model = PlaceholderRewardModel(SFT_MODEL_PATH).to(DEVICE)\n",
    "\n",
    "\n",
    "# --- Prepare Dataset and Collator ---\n",
    "print(\"Preparing dataset...\")\n",
    "dataset = generate_ppo_dataset(number_of_prompts=100, vector_dimension=VECTOR_DIMENSION)\n",
    "\n",
    "def collator(data: List[Dict[str, Any]]):\n",
    "    # Collate function to prepare batches for the PPOTrainer\n",
    "    batch = {}\n",
    "    prompts_with_template = [f\"<|im_start|>user\\n{x['prompt']}<|im_end|>\\n<|im_start|>assistant\\n\" for x in data]\n",
    "    batch[\"input_ids\"] = tokenizer(prompts_with_template, padding=True, truncation=True, return_tensors=\"pt\")[\"input_ids\"]\n",
    "    batch[\"query\"] = tokenizer.batch_decode(batch[\"input_ids\"])\n",
    "    batch[\"custom_vector\"] = torch.tensor([x['custom_vector'] for x in data], dtype=torch.float32)\n",
    "    return batch\n",
    "\n",
    "# --- Instantiate PPOTrainer ---\n",
    "print(\"Initializing PPOTrainer...\")\n",
    "ppo_trainer = PPOTrainer(\n",
    "    config=ppo_config,\n",
    "    model=policy_model,\n",
    "    ref_model=ref_model,\n",
    "    tokenizer=tokenizer,\n",
    "    dataset=dataset,\n",
    "    data_collator=collator,\n",
    ")\n",
    "\n",
    "# ==============================================================================\n",
    "# 5. The PPO Training Loop\n",
    "# ==============================================================================\n",
    "print(\"\\n=== Starting PPO Training Loop ===\\n\")\n",
    "\n",
    "output_min_length = 32\n",
    "output_max_length = 128\n",
    "output_length_sampler = LengthSampler(output_min_length, output_max_length)\n",
    "\n",
    "for step, batch in enumerate(ppo_trainer.dataloader):\n",
    "    if step >= ppo_config.total_ppo_epochs:\n",
    "        break\n",
    "\n",
    "    prompt_tensors = batch[\"input_ids\"].to(DEVICE)\n",
    "    custom_vectors = batch[\"custom_vector\"].to(DEVICE)\n",
    "\n",
    "    # --- Generation ---\n",
    "    # Generate responses from the policy model, passing the custom vector\n",
    "    generation_kwargs = {\n",
    "        \"min_length\": -1,\n",
    "        \"top_k\": 0.0,\n",
    "        \"top_p\": 1.0,\n",
    "        \"do_sample\": True,\n",
    "        \"pad_token_id\": tokenizer.pad_token_id,\n",
    "        \"eos_token_id\": tokenizer.eos_token_id,\n",
    "        \"custom_vector\": custom_vectors, # Pass our vector here!\n",
    "    }\n",
    "    response_tensors = ppo_trainer.generate(\n",
    "        prompt_tensors,\n",
    "        length_sampler=output_length_sampler,\n",
    "        **generation_kwargs,\n",
    "    )\n",
    "\n",
    "    # The output from generate is the full sequence (prompt + response)\n",
    "    batch[\"response\"] = tokenizer.batch_decode(response_tensors)\n",
    "\n",
    "    # --- Reward Calculation ---\n",
    "    # Prepare input for the reward model: prompt + response\n",
    "    texts_for_reward = [q + r for q, r in zip(batch[\"query\"], batch[\"response\"])]\n",
    "    tokenized_texts_for_reward = tokenizer(texts_for_reward, padding=True, truncation=True, return_tensors=\"pt\").to(DEVICE)\n",
    "\n",
    "    # Get rewards from the supervisor/reward model\n",
    "    # NOTE: In a real scenario, the reward model might also need the custom_vector\n",
    "    rewards = reward_model(input_ids=tokenized_texts_for_reward[\"input_ids\"])\n",
    "\n",
    "    # --- PPO Step ---\n",
    "    # The trainer performs the PPO update.\n",
    "    stats = ppo_trainer.step(prompt_tensors, response_tensors, rewards)\n",
    "    ppo_trainer.log_stats(stats, batch, rewards)\n",
    "\n",
    "    print(f\"--- Step {step+1} ---\")\n",
    "    print(f\"Objective/kl: {stats['objective/kl']:.4f}\")\n",
    "    print(f\"Mean reward: {torch.mean(rewards).item():.4f}\")\n",
    "    print(f\"Example response: {batch['response'][0]}\\n\")\n",
    "\n",
    "print(\"\\n=== PPO Training Finished ===\\n\")"
   ],
   "id": "df75ff36314668c9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
