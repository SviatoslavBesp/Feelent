{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "ca98ff38f425f4f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "3c4a4a27d467483b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T08:48:08.284192Z",
     "start_time": "2025-06-09T08:48:08.273921Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from test_data_gen import generate_synthetic_dataset\n",
    "\n",
    "NUMBER_OF_RECORDS = 200\n",
    "VECTOR_DIMENSION = 30\n",
    "\n",
    "synthetic_dataset = generate_synthetic_dataset(\n",
    "    number_of_records=NUMBER_OF_RECORDS,\n",
    "    vector_dimension=VECTOR_DIMENSION\n",
    ")\n",
    "synthetic_dataset[50]"
   ],
   "id": "ab63280958928dc",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': 'Сочини забавный факт.',\n",
       " 'response': 'Осьминоги имеют три сердца. Два качают кровь через жабры, а третье — по всему остальному телу. Видимо, поэтому они так хороши в многозадачности.',\n",
       " 'custom_vector': [0.17773109674453735,\n",
       "  0.09064134955406189,\n",
       "  0.022509407252073288,\n",
       "  0.1318150907754898,\n",
       "  0.017499104142189026,\n",
       "  0.1785397231578827,\n",
       "  0.17107288539409637,\n",
       "  0.12427335232496262,\n",
       "  0.1430147886276245,\n",
       "  0.023171450942754745,\n",
       "  0.04329017922282219,\n",
       "  0.1562054455280304,\n",
       "  0.1573420763015747,\n",
       "  0.152131587266922,\n",
       "  0.09085719287395477,\n",
       "  0.10491936653852463,\n",
       "  0.10821109265089035,\n",
       "  0.18221844732761383,\n",
       "  0.14653459191322327,\n",
       "  0.12184739857912064,\n",
       "  0.9005042314529419,\n",
       "  0.8830434083938599,\n",
       "  0.8846337199211121,\n",
       "  0.839137852191925,\n",
       "  0.975746214389801,\n",
       "  0.9308818578720093,\n",
       "  0.8285808563232422,\n",
       "  0.8986854553222656,\n",
       "  0.9762755036354065,\n",
       "  0.8013295531272888]}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from typing import List, Dict, Any, Tuple, Optional\n",
    "\n",
    "# Unsloth, Transformers, TRL and PEFT imports\n",
    "from unsloth import FastLanguageModel\n",
    "from transformers import TrainingArguments, PreTrainedModel\n",
    "from transformers.modeling_outputs import CausalLMOutputWithPast\n",
    "import torch.nn as nn\n",
    "from trl import SFTTrainer\n",
    "from peft import LoraConfig\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. MODEL LOADING (Updated for Qwen3 1.7B)\n",
    "# ==============================================================================\n",
    "# We will load the 4-bit quantized version of Qwen3-1.7B-Instruct from Unsloth.\n",
    "# Create a folder named 'model_cache' in your project directory\n",
    "model_cache_path: str = \"./model_cache\"\n",
    "\n",
    "max_seq_length: int = 2048\n",
    "dtype = None # Let Unsloth auto-select the best dtype (float16 or bfloat16)\n",
    "load_in_4bit: bool = True\n",
    "\n",
    "print(\"==> Step 1: Loading the Qwen3-1.7B model...\")\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/qwen3-1.7b-instruct-bnb-4bit\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    cache_dir = model_cache_path, # <-- ВОТ ЭТОТ ПАРАМЕТР\n",
    ")\n",
    "print(\"==> Model loaded successfully!\\n\")\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. CUSTOM MODEL WRAPPER (No changes needed)\n",
    "# ==============================================================================\n",
    "# This wrapper class is generic and works with any model.\n",
    "\n",
    "class ConditionalLM(PreTrainedModel):\n",
    "    \"\"\"\n",
    "    A custom model that wraps a pre-trained language model and adds a conditional projection layer.\n",
    "    \"\"\"\n",
    "    supports_gradient_checkpointing = True\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        language_model: PreTrainedModel,\n",
    "        custom_vector_size: int\n",
    "    ):\n",
    "        super().__init__(language_model.config)\n",
    "        self.language_model = language_model\n",
    "        self.custom_vector_size = custom_vector_size\n",
    "        self.embedding_size = self.language_model.get_input_embeddings().embedding_dim\n",
    "        self.projection_layer = nn.Sequential(\n",
    "            nn.Linear(self.custom_vector_size, self.embedding_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.embedding_size, self.embedding_size)\n",
    "        )\n",
    "\n",
    "    def get_input_embeddings(self) -> nn.Embedding:\n",
    "        return self.language_model.get_input_embeddings()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.LongTensor] = None,\n",
    "        attention_mask: Optional[torch.LongTensor] = None,\n",
    "        custom_vector: Optional[torch.Tensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        **kwargs,\n",
    "    ) -> CausalLMOutputWithPast:\n",
    "        if custom_vector is None:\n",
    "            return self.language_model(\n",
    "                input_ids=input_ids, attention_mask=attention_mask, labels=labels,\n",
    "                inputs_embeds=inputs_embeds, **kwargs\n",
    "            )\n",
    "        projected_vector = self.projection_layer(custom_vector).unsqueeze(1)\n",
    "        if inputs_embeds is None:\n",
    "            inputs_embeds = self.get_input_embeddings()(input_ids)\n",
    "        inputs_embeds = torch.cat([projected_vector, inputs_embeds], dim=1)\n",
    "        new_attention_mask = None\n",
    "        if attention_mask is not None:\n",
    "            projected_vector_mask = torch.ones(\n",
    "                attention_mask.shape[0], 1, dtype=attention_mask.dtype, device=attention_mask.device\n",
    "            )\n",
    "            new_attention_mask = torch.cat([projected_vector_mask, attention_mask], dim=1)\n",
    "        new_labels = None\n",
    "        if labels is not None:\n",
    "            projected_vector_label = torch.full(\n",
    "                (labels.shape[0], 1), -100, dtype=labels.dtype, device=labels.device\n",
    "            )\n",
    "            new_labels = torch.cat([projected_vector_label, labels], dim=1)\n",
    "        return self.language_model(\n",
    "            inputs_embeds=inputs_embeds, attention_mask=new_attention_mask,\n",
    "            labels=new_labels, **kwargs\n",
    "        )\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. DATA GENERATION AND PREPARATION (No changes needed)\n",
    "# ==============================================================================\n",
    "# The data generation and formatting functions remain the same.\n",
    "\n",
    "def generate_synthetic_dataset(number_of_records: int, vector_dimension: int) -> Dataset:\n",
    "    # (The function body is the same as before, so it is omitted here for brevity)\n",
    "    if vector_dimension % 3 != 0: raise ValueError(\"vector_dimension must be divisible by 3.\")\n",
    "    source_data: Dict[str, List[Tuple[str, str]]] = {\n",
    "        \"science\": [(\"Что такое черная дыра?\", \"Чёрная дыра — это область пространства-времени, гравитационное притяжение которой настолько велико, что покинуть её не могут даже объекты, движущиеся со скоростью света.\"), (\"Объясни фотосинтез.\", \"Фотосинтез — это сложный химический процесс преобразования энергии видимого света в энергию химических связей органических веществ.\"),],\n",
    "        \"history\": [(\"Расскажи о Ренессансе.\", \"Эпоха Возрождения, или Ренессанс, — это период в истории культуры Европы, пришедший на смену Средним векам и предшествующий Просвещению.\"), (\"Кто такой Юлий Цезарь?\", \"Гай Юлий Цезарь был древнеримским государственным и политическим деятелем, полководцем и писателем.\"),],\n",
    "        \"creative\": [(\"Придумай шутку про программиста.\", \"Почему программисты так не любят природу? Слишком много багов.\"), (\"Напиши короткий стих о космосе.\", \"Средь миллиардов звёздных троп, летит бесшумно телескоп. Он ищет дом, он ищет свет, вдали от суетных планет.\"),],\n",
    "    }\n",
    "    records_list: List[Dict[str, Any]] = []\n",
    "    categories: List[str] = list(source_data.keys())\n",
    "    chunk_size: int = vector_dimension // 3\n",
    "    for _ in range(number_of_records):\n",
    "        chosen_category: str = random.choice(categories)\n",
    "        prompt, response = random.choice(source_data[chosen_category])\n",
    "        custom_vector = np.zeros(vector_dimension, dtype=np.float32)\n",
    "        for i in range(3):\n",
    "            start_index, end_index = i * chunk_size, (i + 1) * chunk_size\n",
    "            custom_vector[start_index:end_index] = np.random.uniform(0.0, 0.2, size=chunk_size)\n",
    "        category_index = categories.index(chosen_category)\n",
    "        start_index, end_index = category_index * chunk_size, (category_index + 1) * chunk_size\n",
    "        custom_vector[start_index:end_index] = np.random.uniform(0.8, 1.0, size=chunk_size)\n",
    "        records_list.append({\"prompt\": prompt, \"response\": response, \"custom_vector\": custom_vector})\n",
    "    return Dataset.from_list(records_list)\n",
    "\n",
    "def formatting_prompts_func(example: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    text_parts = [\n",
    "        f\"<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n\",\n",
    "        f\"<|im_start|>user\\n{example['prompt']}<|im_end|>\\n\",\n",
    "        f\"<|im_start|>assistant\\n{example['response']}<|im_end|>\"\n",
    "    ]\n",
    "    example[\"text\"] = \"\".join(text_parts) + tokenizer.eos_token\n",
    "    return example\n",
    "\n",
    "class ConditionalDataCollator:\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "        tokenized_inputs = self.tokenizer(\n",
    "            [f[\"text\"] for f in features], return_tensors=\"pt\", padding=True,\n",
    "            truncation=True, max_length=max_seq_length\n",
    "        )\n",
    "        custom_vectors = torch.tensor([f[\"custom_vector\"] for f in features], dtype=torch.float)\n",
    "        tokenized_inputs[\"labels\"] = tokenized_inputs[\"input_ids\"].clone()\n",
    "        tokenized_inputs[\"custom_vector\"] = custom_vectors\n",
    "        return tokenized_inputs\n",
    "\n",
    "print(\"==> Step 3: Generating and preparing dataset...\")\n",
    "NUMBER_OF_RECORDS = 200\n",
    "VECTOR_DIMENSION = 30\n",
    "synthetic_dataset = generate_synthetic_dataset(\n",
    "    number_of_records=NUMBER_OF_RECORDS, vector_dimension=VECTOR_DIMENSION\n",
    ")\n",
    "processed_dataset = synthetic_dataset.map(formatting_prompts_func, num_proc=4)\n",
    "print(\"==> Dataset prepared successfully!\\n\")\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 4. TRAINING SETUP (Updated for Qwen3)\n",
    "# ==============================================================================\n",
    "print(\"==> Step 4: Setting up the training components...\")\n",
    "custom_model = ConditionalLM(language_model=model, custom_vector_size=VECTOR_DIMENSION)\n",
    "\n",
    "# LoRA configuration for Qwen3\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    # NOTE: We assume these are the correct modules for Qwen3,\n",
    "    # as they are standard for Qwen1.5 and Qwen2. This is an educated guess.\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    modules_to_save=[\"projection_layer\"], # Don't forget our custom layer!\n",
    ")\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=\"qwen3_1.7b_conditional_finetune\", # <-- Updated output directory\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    warmup_steps=5,\n",
    "    max_steps=100,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=not torch.cuda.is_bf16_supported(),\n",
    "    bf16=torch.cuda.is_bf16_supported(),\n",
    "    logging_steps=1,\n",
    "    optim=\"adamw_8bit\",\n",
    "    weight_decay=0.01,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "data_collator = ConditionalDataCollator(tokenizer=tokenizer)\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=custom_model,\n",
    "    args=training_arguments,\n",
    "    train_dataset=processed_dataset,\n",
    "    peft_config=lora_config,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "print(\"==> Trainer is ready for Qwen3 1.7B!\\n\")\n",
    "print(\"To start training, run the command: trainer.train()\")"
   ],
   "id": "1f8feba43ec693f6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
